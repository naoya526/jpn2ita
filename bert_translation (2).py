# -*- coding: utf-8 -*-
"""Bert_Translation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/naoya526/jpn2ita/blob/main/Bert_Translation.ipynb

## 1 Import Module

I used materials below:
[1]`06_Attention_and_Transformers_in_BERT.ipynb`
[2]`English_to_italian_automatic_translation.ipynb`
"""

import os
import re
import random
import itertools
import math
from pathlib import Path
import tqdm
import numpy as np
print(np.__version__)
import torch
print(torch.__version__)
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from torch.utils.data import Dataset, DataLoader
import transformers
from tokenizers import BertWordPieceTokenizer
from transformers import BertTokenizer, BertConfig
### Suppress useless warnings
import warnings
warnings.filterwarnings("ignore", message="The secret `HF_TOKEN` does not exist")
from collections import defaultdict
from transformers import AutoTokenizer

"""## Define Model
---

## 2 Define Model
### 2.1 Encoder (Bert) part
Here, There's the function for implementing Encoder(Bert). I implemented with refering to [1]`06_Attention_and_Transformers_in_BERT.ipynb` and the paper.
- `MultiHeadAttention`
- `PositionwiseFeedForward`
- `Encoder Block`
- `BertEmbeddings` (Embedding for words)
- `Bert`
Bert is highly possible to understand meaning, but it is not enough for produce translation.
Hence, In the next part, I implement Decoder. It is quite similar to Bert.
"""

class MultiHeadAttention(nn.Module):
    """
    - Query, Key, Value
    - Scaled Dot Product Attention: softmax(QK^T / sqrt(d_k))V
    """
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"

        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.dropout = torch.nn.Dropout(dropout)

        # Q, K, V linear Conversion
        self.query = torch.nn.Linear(d_model, d_model)
        self.key = torch.nn.Linear(d_model, d_model)
        self.value = torch.nn.Linear(d_model, d_model)

        self.out_proj = torch.nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape

        # step1: Q, K, V
        query = self.query(x)  # (batch, seq_len, d_model)
        key = self.key(x)      # (batch, seq_len, d_model)
        value = self.value(x)  # (batch, seq_len, d_model)

        # step2: Multi-Head
        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim)
        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim)  # 修正: query.shape → batch_size
        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim)

        # step3: Change Dimention for Calclate Efficiently
        query = query.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, head_dim)
        key = key.permute(0, 2, 1, 3)
        value = value.permute(0, 2, 1, 3)

        # ステップ4: Scaled Dot-Product Attention
        # scores = Q @ K^T / sqrt(d_k)
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # ステップ5: マスク処理（オプション）
        if mask is not None:
            scores = scores + mask # 加算によるマスキングに変更

        # ステップ6: Softmax + Dropout
        weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)
        weights = self.dropout(weights)
        # ステップ7: Value との積
        context = torch.matmul(weights, value)
        # ステップ8: ヘッドを結合して元の形状に戻す
        context = context.permute(0, 2, 1, 3)
        # → (batch, seq_len, d_model)
        context = context.contiguous().view(batch_size, seq_len, self.num_heads * self.head_dim)

        # ステップ9: 最終的な線形変換
        return self.out_proj(context)  # 修正: output_linear → out_proj

class PositionwiseFeedForward(nn.Module):
    """
    ヒント:
    - 2層のフィードフォワードネットワーク
    - 中間層では次元を拡張（通常4倍）
    - GELU活性化関数を使用
    - ドロップアウトも忘れずに
    """
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)  # 入力次元 → 中間次元
        self.linear2 = nn.Linear(d_ff, d_model)  # 中間次元
        self.dropout = nn.Dropout(dropout)
        self.gelu = nn.GELU()

    def forward(self, x):
        x = self.linear1(x)
        x = self.gelu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        x = self.dropout(x)
        return x

class EncoderBlock(nn.Module):
    """
    ヒント:
    - Multi-Head Attention + Residual Connection + Layer Norm
    - Feed Forward + Residual Connection + Layer Norm
    - Which is better??: Pre-LN vs Post-LN
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model,num_heads)
        self.ffn = PositionwiseFeedForward(d_model,d_ff)

        self.layer_norm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)
        self.layer_norm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        #Attention block
        #TODO implement transformer block
        residual = x
        #print("Took Residual...",x.shape)
        x = self.layer_norm1(x)
        #print("calculating layer norm...",x.shape)
        x = self.dropout(self.attention(x,mask))
        #print("calculating Attention...",x.shape)
        x = x + residual
        #print("calculating Residual Connection...",x.shape)
        #ffnn
        residual = x
        x = self.layer_norm2(x)
        #print("calculating layer norm...",x.shape)
        x = self.dropout(self.ffn(x))
        #print("calculating ffn...",x.shape)
        x = x + residual
        return x


class BertEmbeddings(nn.Module):
    """
    - Token Embeddings (語彙サイズ × d_model)
    - Position Embeddings (最大系列長 × d_model)
    - Segment Embeddings (2 × d_model, NSPタスク用)
    - 3つを足し合わせてLayerNormとDropout
    """
    def __init__(self, vocab_size, d_model, max_seq_len=512, dropout=0.1):
        super().__init__()
        # TODO: 3種類の埋め込みを実装
        self.d_model = d_model
        self.token = torch.nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.position = torch.nn.Embedding(max_seq_len, d_model)
        self.segment = torch.nn.Embedding(2, d_model)  # 2つのセグメント（0と1）
        self.layer_norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        #Embedding: Lookup table that keep meaning vector of words
    def forward(self, input_ids, token_type_ids=None):
        # TODO: 埋め込みの計算を実装
        batch_size, seq_len = input_ids.shape
        # Step 1: Token Embeddings
        token_embeddings = self.token(input_ids) * (self.d_model ** 0.5) # multiply sqrt d scaling
        # Step 2: Position Embeddings
        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)
        position_ids = position_ids.expand(batch_size, -1)  # 🔧 バッチ次元を拡張
        position_embeddings = self.position(position_ids)
        # Step 3: Segment Embeddings
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)  # 全て0（単一文）
        segment_embeddings = self.segment(token_type_ids)  # (batch, seq_len, d_model)
        embeddings = token_embeddings + position_embeddings + segment_embeddings
        embeddings = self.dropout(self.layer_norm(embeddings))

        return embeddings

class Bert(nn.Module):
    def __init__(self, vocab_size, d_model=768, num_layers=12, num_heads=12, d_ff=3072, max_seq_len=512, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        self.heads = num_heads
        # paper noted 4*d_model size for ff
        self.feed_forward_hidden = d_model * 4
        # embedding for BERT, sum of positional, segment, token embeddings
        self.embedding = BertEmbeddings(vocab_size, d_model, max_seq_len, dropout)

        self.encoder_blocks = torch.nn.ModuleList(
            [EncoderBlock(d_model, num_heads, d_model * 4, dropout) for _ in range(num_layers)])

    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        # TODO: BERT全体のforward passを実装
        if attention_mask is None:
            attention_mask = (input_ids != 0).float()
        if attention_mask.dim() == 2:
            # (batch, seq_len) → (batch, 1, 1, seq_len)
            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            # print("squeeze is required") # デバッグプリントを削除
        elif attention_mask.dim() == 4:
            # 既に正しい形状の場合はそのまま使用
            extended_attention_mask = attention_mask
            # print("squeeze is not required") # デバッグプリントを削除
        else:
             raise ValueError(f"Attention mask should be 2D or 4D, but got {attention_mask.dim()}D")

        # 0を-1e9に変換（Softmaxで0になるように） - 加算によるマスキングのために値を調整
        extended_attention_mask = (1.0 - extended_attention_mask) * -1e9


        # embedding the indexed sequence to sequence of vectors
        x = self.embedding(input_ids, token_type_ids)
        # running over multiple transformer blocks
        for encoder in self.encoder_blocks:
            x = encoder.forward(x, extended_attention_mask) # 修正後のMultiHeadAttentionは加算マスクを期待
        return x

"""### 2.2 Decoder part
This part, I implemented these functions:
- `CrossAttention`(English Queue, Italian Key, Italian Value)
- `DecoderBlock`
- `BertTranslationModel`(Bert + Decoder Embedding + DecoderBlock*`num_layers`)
"""

class CrossAttention(nn.Module):
    """
    this module is implemented with modifying MultiHeadAttention.
    Query: English
    Key, Value: Italian
    You can see the difference in forward input
    """
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__() # initialization
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"

        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads  # dimention of each head
        self.dropout = torch.nn.Dropout(dropout)

        # Q, K, V の線形変換（修正：torch.nn.linear → torch.nn.Linear）
        self.query = torch.nn.Linear(d_model, d_model)
        self.key = torch.nn.Linear(d_model, d_model)
        self.value = torch.nn.Linear(d_model, d_model)

        # 最終的な出力変換
        self.out_proj = torch.nn.Linear(d_model, d_model)

    def forward(self, query_input, key_value_input, mask=None): # here is the difference
        batch_size, q_len, _ = query_input.shape
        _, kv_len, _ = key_value_input.shape
        # ステップ1: Q, K, V を線形変換で生成
        query = self.query(query_input)  # (batch, seq_len, d_model)
        key = self.key(key_value_input)      # (batch, seq_len, d_model)
        value = self.value(key_value_input)  # (batch, seq_len, d_model)

        # ステップ2: Multi-Head用に次元を変形
        query = query.view(batch_size, q_len, self.num_heads, self.head_dim)
        key = key.view(batch_size, kv_len, self.num_heads, self.head_dim)  # 修正: query.shape → batch_size
        value = value.view(batch_size, kv_len, self.num_heads, self.head_dim)

        query = query.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, head_dim)
        key = key.permute(0, 2, 1, 3)
        value = value.permute(0, 2, 1, 3)

        # ステップ4: Scaled Dot-Product Attention
        # scores = Q @ K^T / sqrt(d_k)
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # ステップ5: マスク処理（オプション）
        if mask is not None:
            # mask形状: (batch, 1, 1, seq_len) → scores形状: (batch, num_heads, seq_len, seq_len)
            scores = scores + mask  # ブロードキャストで加算

        # ステップ6: Softmax + Dropout
        weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)
        weights = self.dropout(weights)
        # ステップ7: Value との積
        context = torch.matmul(weights, value)
        # ステップ8: ヘッドを結合して元の形状に戻す
        context = context.permute(0, 2, 1, 3)
        # → (batch, seq_len, d_model)
        context = context.contiguous().view(batch_size, q_len, self.num_heads * self.head_dim)

        # ステップ9: 最終的な線形変換
        return self.out_proj(context)  # 修正: output_linear → out_proj

class DecoderBlock(nn.Module):
    """
    Basically similar to EncoderBlock, but refer to the infomation of Input(English context)
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        #First, implement Self Attention
        self.self_attention = MultiHeadAttention(d_model,num_heads)
        #Second, implement Cross Attention
        self.cross_attention = CrossAttention(d_model, num_heads)
        #Third, FFNN
        self.ffn = PositionwiseFeedForward(d_model,d_ff)

        self.layer_norm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)
        self.layer_norm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)
        self.layer_norm3 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, encoder_output, self_mask=None, cross_mask=None):
        #Self Attention
        residual = x
        x = self.layer_norm1(x)
        x = self.self_attention(x,mask=self_mask)
        x = self.dropout(x) + residual

        #Cross Attention
        residual = x
        x = self.layer_norm2(x)
        """
        x = self.cross_attention(
            query_input=x,
            key_value_input=encoder_output,
            mask=cross_mask
        )
        """
        x = self.dropout(x) + residual

        residual = x
        x = self.layer_norm3(x)
        x = self.ffn(x)
        x = self.dropout(x) + residual
        return x

class BertTranslationModel(nn.Module):
    """
    Ita2Eng Translation Model
    Encoder: Bert
    Decoder: BertEmbedding, DecoderBlock*N, FFN
    """
    def __init__(self,
                 ita_vocab_size,  # イタリア語語彙サイズ
                 eng_vocab_size,  # 英語語彙サイズ
                 max_seq_len,
                 d_model=512,
                 num_layers=6,
                 num_heads=8,
                 dropout=0.1):
        super().__init__()

        self.encoder = Bert(
            vocab_size=eng_vocab_size,
            d_model=d_model,
            num_layers=num_layers,
            num_heads=num_heads,
            max_seq_len=max_seq_len,
            dropout=dropout
        )


        self.decoder_embeddings = BertEmbeddings(
            vocab_size=ita_vocab_size,
            d_model=d_model,
            max_seq_len=max_seq_len,
            dropout=dropout
        )

        self.decoder_blocks = nn.ModuleList([
            DecoderBlock(
                d_model=d_model,
                num_heads=num_heads,
                d_ff=d_model * 4, #based on the paper of Bert
                dropout=dropout)
            for _ in range(num_layers)
        ])

        self.output_proj = nn.Linear(d_model, ita_vocab_size)

    def forward(self,
                eng_ids,
                ita_ids,
                eng_mask=None,
                ita_mask=None,
                eng_token_type_ids=None,
                ita_token_type_ids=None):
        # understand english
        encoder_output = self.encoder(input_ids=eng_ids, attention_mask=eng_mask, token_type_ids=eng_token_type_ids)
        # produce Italian
        decoder_input = self.decoder_embeddings(input_ids=ita_ids, token_type_ids=ita_token_type_ids)

        for decoder_block in self.decoder_blocks:
            decoder_input = decoder_block(
                x=decoder_input,
                encoder_output=encoder_output,
                self_mask=ita_mask,               # 英語のCausal mask
                cross_mask=eng_mask
                )
        logits = self.output_proj(decoder_input)
        return logits

"""## 3 Dataset
In this part, I followed the configuration of [2]`English_to_italian_automatic_translation.ipynb`.
for Bert, `<sos>`and `<eos>` are not required. Hence, ignore these token.
"""

# Download the files, Execute Once
URL = "https://drive.google.com/file/d/1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT/view?usp=sharing"
!gdown --fuzzy $URL -O- | tar -xz

# for Bert, <sos> and <eos> are not required
#SPECIAL = ["<sos>", "<eos>", "<pad>"]
SPECIAL = ["[PAD]"]
MAXLEN = 20

f = open("text-eng.txt")
# Define the list of all tokens in the English set ...
ENG_VOCABULARY = []
for line in f:
    line = line.strip()
    # Remove <sos> and <eos>
    line = line.replace('<sos>', '').replace('<eos>', '').strip()
    if line == "":
        continue

    ENG_VOCABULARY.append(line)
f.close()
print(ENG_VOCABULARY[:50])

f = open("text-ita.txt")
# Define the list of all tokens in the Italian set ...
ITA_VOCABULARY = []
for line in f:
    line = line.strip()
    # Remove <sos> and <eos>
    line = line.replace('<sos>', '').replace('<eos>', '').strip()
    if line == "":
        continue
    ITA_VOCABULARY.append(line)
f.close()
print(ITA_VOCABULARY[:50])
# Make sure that the three special tokens have the same indices in the two vocabularies.
# Assign here the three indices...

PAD = SPECIAL[0]

# Inverse mappings.
ENG_INVERSE = {w: n for n, w in enumerate(ENG_VOCABULARY)}
ITA_INVERSE = {w: n for n, w in enumerate(ITA_VOCABULARY)}
#print(ENG_INVERSE)
print(len(ENG_VOCABULARY), len(ITA_VOCABULARY))

"""### 3.1 Incremental approach to token vocabulary building
In the lesson of Deep Learning, I learned the sophisticated way of tokenizing words called WordPiece tokenization.

The WordPiece tokenization algorithm builds its vocabulary incrementally, starting from a basic alphabet and iteratively merging subword units based on their frequency and co-occurrence patterns. (cited from [1]`06_Attention_and_Transformers_in_Bert.ipynb`)

---

#### (Appendix)The demonstration of pretrained tokenizer

In [1], Tokenizer `bert-base-cased` was used for English(For tokenization of English, it's used in this project as well). In this project, Tokenizier [3]`dbmdz/bert-base-italian-cased` for italian is used.
[3]https://huggingface.co/dbmdz/bert-base-italian-cased  
\
In this section, With using small scentence, The procedure will be explained.

These procedure will be iterated:
- Compute word frequencies
- Split Words into Alphabet
- Compute score of each pair
- Merge the pair
"""

eng_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")  # English
ita_tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-italian-cased")  # Italian

### Example bilingual corpus
eng_corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]

ita_corpus = [
    "Questo è il corso di Hugging Face.",
    "Questo capitolo riguarda la tokenizzazione.",
    "Questa sezione mostra diversi algoritmi di tokenizzazione.",
    "Speriamo che tu sia in grado di capire come vengono addestrati e generano token.",
]

### Get frequency for English
eng_word_freqs = defaultdict(int)
for text in eng_corpus:
    words_with_offsets = eng_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    print(f"English: {new_words}")
    for word in new_words:
        eng_word_freqs[word] += 1

### Get frequency for Italian
ita_word_freqs = defaultdict(int)
for text in ita_corpus:
    words_with_offsets = ita_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    print(f"Italian: {new_words}")
    for word in new_words:
        ita_word_freqs[word] += 1

print(f"\nEnglish Word Frequency: {eng_word_freqs}")
print(f"Italian Word Frequency: {ita_word_freqs}")

# Get vocabulary sizes for model initialization
eng_vocab_size = eng_tokenizer.vocab_size
ita_vocab_size = ita_tokenizer.vocab_size
print(f"\nEnglish vocab size: {eng_vocab_size}")
print(f"Italian vocab size: {ita_vocab_size}")

### split all word into alphabet
alphabet = []
for word in ita_word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
print(f'All alphabets: {alphabet}')

### insert special token and subword
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
splits = {word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)] for word in ita_word_freqs.keys()}
print(f'\nSplitted Words: {splits}')

### compute score for merging

def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)

    for word, freq in ita_word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores

pair_scores = compute_pair_scores(splits)
print(f'Scores for each Pair: {pair_scores}')
### finding pair with best score

best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
vocab.append("ab")

### merge pair ###
def merge_pair(a, b, splits):
    for word in ita_word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits

splits = merge_pair("Q", "##u", splits)
print(splits["Questo"])
### keep looping to merge more pair

vocab_size = 150
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)

print(f'Final Vocab: {vocab}')
### encode a word ###
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens

print(encode_word("Questo"))
# This one should be unknown (within this corpus)
print(encode_word("Qaesto"))

"""### 3.2 Dataset"""

eng_sentences = []
with open("text-eng.txt", "r") as f:
    for line in f:
        line = line.strip().replace('<sos>', '').replace('<eos>', '').strip()
        if line:
            eng_sentences.append(line)

ita_sentences = []
with open("text-ita.txt", "r") as f:
    for line in f:
        line = line.strip().replace('<sos>', '').replace('<eos>', '').strip()
        if line:
            ita_sentences.append(line)

data_pair = list(zip(eng_sentences, ita_sentences))

# Split data into training and validation sets
# Using a simple split for now, can use train_test_split later if needed
train_size = int(0.8 * len(data_pair))
train_data = data_pair[:train_size]
val_data = data_pair[train_size:]

print(f"Total pairs: {len(data_pair)}")
print(f"Training pairs: {len(train_data)}")
print(f"Validation pairs: {len(val_data)}")

class TranslationDataset(Dataset):
    def __init__(self, data_pair, eng_tokenizer, ita_tokenizer, seq_len=64):
        self.data_pair = data_pair
        self.eng_tokenizer = eng_tokenizer
        self.ita_tokenizer = ita_tokenizer
        self.seq_len = seq_len

    def __len__(self):
        return len(self.data_pair)

    def __getitem__(self, item):
        eng_sentence, ita_sentence = self.data_pair[item]

        # --- Tokenize English sentence ---
        eng_tokens = self.eng_tokenizer(
            eng_sentence,
            padding='max_length',
            truncation=True,
            max_length=self.seq_len,
            return_attention_mask=True,
            return_token_type_ids=True,
            return_tensors='pt'
        )

        # --- Tokenize Italian sentence ---
        ita_tokens = self.ita_tokenizer(
            ita_sentence,
            padding='max_length',
            truncation=True,
            max_length=self.seq_len,
            return_attention_mask=True,
            return_token_type_ids=True,
            return_tensors='pt'
        )

        ita_input_ids = ita_tokens['input_ids'].squeeze(0)  # [CLS] ciao! [SEP] [PAD] ...

        # --- Full alignment: shift right for decoder input ---
        decoder_input_ids = ita_input_ids[:-1]  # 先頭〜最後の1つ前まで（[CLS] 〜 [SEP]含む）
        target_ids = ita_input_ids[1:]          # 2つ目〜最後まで（ciao! 〜 [PAD]）

        # パディングの整合性を保つため、両方 seq_len - 1 にパディング
        decoder_input_ids = F.pad(decoder_input_ids, (0, 1), value=self.ita_tokenizer.pad_token_id)
        target_ids = F.pad(target_ids, (0, 1), value=self.ita_tokenizer.pad_token_id)

        # --- Causal mask for decoder self-attention ---
        seq_len = decoder_input_ids.size(0)
        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        causal_mask = causal_mask.masked_fill(causal_mask, float('-inf'))
        causal_mask = causal_mask.masked_fill(~causal_mask, 0.0)

        return {
            "eng_ids": eng_tokens['input_ids'].squeeze(0),
            "eng_mask": eng_tokens['attention_mask'].squeeze(0),
            "eng_token_type_ids": eng_tokens['token_type_ids'].squeeze(0),
            "ita_ids": decoder_input_ids,
            "ita_mask": ita_tokens['attention_mask'].squeeze(0),  # attention_mask はデコーダでも使える
            "ita_token_type_ids": ita_tokens['token_type_ids'].squeeze(0),
            "causal_mask": causal_mask,
            "ita_target_ids": target_ids
        }

"""## 4 Learning

### 4.1 Model
Create a translation model using `BertTranslationModel` based on loaded text data. The model should translate from English to Italian. Outline the steps for Tokenization, Word Embedding, and training.
"""

model = BertTranslationModel(
    ita_vocab_size=ita_tokenizer.vocab_size,
    eng_vocab_size=eng_tokenizer.vocab_size,
    max_seq_len=MAXLEN,
    d_model=256, # 使用するBERTモデルの一般的な次元
    num_layers=3, # レイヤー数は適宜設定
    num_heads=4, # ヘッド数は適宜設定 (d_modelで割り切れるように)
    dropout=0.1
)

print(model)

"""### 4.2 Lossfunction and Optimizer
 `CrossEntropyLoss`and Optimizer `Adam` is defined.

"""

criterion = nn.CrossEntropyLoss(ignore_index=ita_tokenizer.pad_token_id)
assert ita_tokenizer.pad_token_id is not None
optimizer = Adam(model.parameters(), lr=1e-4)

print("Loss function (criterion):", criterion)
print("Optimizer:", optimizer)

"""### 4.3 Training

### Subtask:
モデルを訓練するためのループを作成します。これには、データのバッチ処理、モデルのフォワードパス、損失の計算、バックプロパゲーション、パラメータの更新などが含まれます。

"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
model.to(device)

# Create DataLoader instances for training and validation sets
BATCH_SIZE = 32 # Define batch size
train_dataset = TranslationDataset(train_data, eng_tokenizer, ita_tokenizer, seq_len=MAXLEN)
val_dataset = TranslationDataset(val_data, eng_tokenizer, ita_tokenizer, seq_len=MAXLEN)

train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)


EPOCHS = 20 # Define the number of training epochs
def translate_sentence(sentence, model, eng_tokenizer, ita_tokenizer, max_len, device):
    with torch.no_grad():
        # Encode English
        enc = eng_tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=max_len)
        eng_ids = enc['input_ids'].to(device)
        eng_mask = enc['attention_mask'].to(device)
        eng_token_type_ids = enc['token_type_ids'].to(device)

        # Prepare encoder attention mask (for encoder self-attention)
        # Shape needed for MultiHeadAttention in EncoderBlock: (batch, 1, 1, seq_len)
        eng_encoder_mask = eng_mask.unsqueeze(1).unsqueeze(2)
        eng_encoder_mask = (1.0 - eng_encoder_mask.float()) * -1e9


        encoder_output = model.encoder(eng_ids, eng_encoder_mask, eng_token_type_ids)

        # Start with [CLS]
        ita_input_ids = torch.tensor([[ita_tokenizer.cls_token_id]], device=device)
        ita_token_type_ids = torch.zeros_like(ita_input_ids).to(device)
        translated = []

        for _ in range(max_len):
            current_len = ita_input_ids.size(1)

            # causal mask for decoder self-attention
            # Shape needed for MultiHeadAttention in DecoderBlock: (batch, num_heads, seq_len, seq_len)
            causal_mask = torch.triu(torch.ones(current_len, current_len), diagonal=1).bool().to(device)
            causal_mask = causal_mask.masked_fill(causal_mask, float('-inf')).masked_fill(~causal_mask, 0.0)
            num_heads = model.decoder_blocks[0].self_attention.num_heads
            ita_causal_mask_expanded = causal_mask.unsqueeze(0).unsqueeze(0).expand(1, num_heads, current_len, current_len)

            # cross attention mask for decoder cross-attention
            # Shape needed for CrossAttention in DecoderBlock: (batch, num_heads, target_seq_len, source_seq_len)
            eng_cross_mask = eng_mask.unsqueeze(1).unsqueeze(2).expand(1, num_heads, current_len, eng_ids.size(1))
            eng_cross_mask = (1.0 - eng_cross_mask.float()) * -1e9

            # Forward
            decoder_input = model.decoder_embeddings(ita_input_ids, ita_token_type_ids)
            for block in model.decoder_blocks:
                # Note: The forward method of BertTranslationModel expects eng_mask for cross-attention
                # and ita_mask for self-attention. In the translate function, we need to pass
                # the appropriate masks derived from eng_mask and the generated causal mask.
                # However, during inference, the forward pass of BertTranslationModel is NOT used directly
                # for iterative generation. Instead, we call the encoder and decoder blocks directly.
                # The decoder block needs the encoder_output and the self/cross attention masks.
                decoder_input = block(decoder_input, encoder_output, self_mask=ita_causal_mask_expanded, cross_mask=eng_cross_mask)
            logits = model.output_proj(decoder_input)


            # predict next token
            next_token_logits = logits[:, -1, :]
            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)
            next_token = next_token_id.item()

            if next_token == ita_tokenizer.sep_token_id:
                break
            # Removed check for PAD and CLS tokens in translated list based on typical translation output
            translated.append(next_token)

            ita_input_ids = torch.cat([ita_input_ids, next_token_id], dim=1)
            # Assuming token_type_ids remain 0 for the translated sequence
            ita_token_type_ids = torch.cat([ita_token_type_ids, torch.tensor([[0]], device=device)], dim=1)

            # Stop if MAXLEN is reached during generation
            if ita_input_ids.size(1) >= max_len:
                break

        return ita_tokenizer.decode(translated, skip_special_tokens=True)

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    train_iterator = tqdm.tqdm(train_dataloader, desc=f"Epoch {epoch+1}")

    for i, batch in enumerate(train_iterator):
        # Move batch to device
        eng_ids = batch['eng_ids'].to(device)
        # eng_mask from dataset is for encoder attention, shape (batch_size, seq_len)
        eng_mask = batch['eng_mask'].to(device)
        eng_token_type_ids = batch['eng_token_type_ids'].to(device)

        ita_ids = batch['ita_ids'].to(device)
        # ita_mask from dataset is for decoder attention (padding), shape (batch_size, seq_len)
        ita_mask = batch['ita_mask'].to(device)
        ita_target_ids = batch['ita_target_ids'].to(device)
        ita_token_type_ids = batch['ita_token_type_ids'].to(device)
        # ita_causal_mask from dataset is for decoder self-attention, shape (seq_len, seq_len) initially, then (batch_size, seq_len, seq_len) after DataLoader
        ita_causal_mask = batch['causal_mask'].to(device)

        batch_size, ita_seq_len = ita_ids.shape
        _, eng_seq_len = eng_ids.shape
        # Assuming the number of heads is the same for encoder and decoder attention
        num_heads = model.encoder.encoder_blocks[0].attention.num_heads


        # Prepare masks for the model's forward pass
        # The BertTranslationModel forward method expects:
        # - eng_mask: for cross-attention in the decoder (attending from Italian to English)
        # - ita_mask: for self-attention in the decoder (causal mask for Italian)
        # - eng_mask for the encoder is handled internally within model.encoder call


        # Cross-attention mask (attending from Italian to English)
        # Use eng_mask (padding mask) and expand to (batch_size, num_heads, ita_seq_len, eng_seq_len)
        eng_cross_mask = eng_mask.unsqueeze(1).unsqueeze(2).expand(batch_size, num_heads, ita_seq_len, eng_seq_len)
        eng_cross_mask = (1.0 - eng_cross_mask.float()) * -1e9 # Convert to additive mask format


        # Decoder self-attention mask (causal mask for Italian)
        # Use ita_causal_mask and expand to (batch_size, num_heads, ita_seq_len, ita_seq_len)
        # The ita_causal_mask from the dataset is already (batch_size, seq_len, seq_len) and in additive format.
        # We just need to add the heads dimension and expand.
        ita_causal_mask_expanded = ita_causal_mask.unsqueeze(1).expand(batch_size, num_heads, ita_seq_len, ita_seq_len)


        logits = model(
            eng_ids=eng_ids,
            ita_ids=ita_ids,
            eng_mask=eng_cross_mask, # Passed as cross_mask in DecoderBlock
            ita_mask=ita_causal_mask_expanded, # Passed as self_mask in DecoderBlock
            eng_token_type_ids=eng_token_type_ids, # Passed to encoder embeddings
            ita_token_type_ids=ita_token_type_ids # Passed to decoder embeddings
            )

        # Calculate loss, ignoring padding tokens
        # Reshape logits and target for CrossEntropyLoss
        # logits shape: (batch_size, ita_seq_len, vocab_size)
        # ita_target_ids shape: (batch_size, ita_seq_len)
        loss = criterion(logits.view(-1, logits.size(-1)), ita_target_ids.view(-1))

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        train_iterator.set_postfix({'loss': loss.item()})
        if i % 100 == 0:
          print(f"Step {i}: loss = {loss.item():.4f}")
          test_sentence = "hello"
          # Translate a test sentence during training to monitor progress
          translated_text = translate_sentence(test_sentence, model, eng_tokenizer, ita_tokenizer, max_len=MAXLEN, device=device)
          print("Translation:", translated_text)


    avg_train_loss = total_loss / len(train_dataloader)
    print(f"Epoch {epoch+1} finished. Average training loss: {avg_train_loss:.4f}")
    # print("Logits range:", logits.min().item(), logits.max().item()) # Optional: keep for debugging


    # Validation loop
    model.eval()
    total_val_loss = 0
    val_iterator = tqdm.tqdm(val_dataloader, desc=f"Validation Epoch {epoch+1}")
    with torch.no_grad():
        for batch in val_iterator:
            # Move batch to device
            eng_ids = batch['eng_ids'].to(device)
            eng_mask = batch['eng_mask'].to(device)
            eng_token_type_ids = batch['eng_token_type_ids'].to(device)
            ita_ids = batch['ita_ids'].to(device)
            ita_mask = batch['ita_mask'].to(device) # This mask is used to create the cross-attention mask
            ita_target_ids = batch['ita_target_ids'].to(device)
            ita_token_type_ids = batch['ita_token_type_ids'].to(device)
            ita_causal_mask = batch['causal_mask'].to(device) # Corrected key name

            # Prepare masks for the model forward pass (similar to training)
            batch_size, ita_seq_len = ita_ids.shape
            _, eng_seq_len = eng_ids.shape
            num_heads = model.encoder.encoder_blocks[0].attention.num_heads # Assuming same number of heads across blocks

            # Cross-attention mask
            eng_cross_mask = eng_mask.unsqueeze(1).unsqueeze(2).expand(batch_size, num_heads, ita_seq_len, eng_seq_len)
            eng_cross_mask = (1.0 - eng_cross_mask.float()) * -1e9

            # Decoder self-attention mask (causal mask)
            ita_causal_mask_expanded = ita_causal_mask.unsqueeze(1).expand(batch_size, num_heads, ita_seq_len, ita_seq_len)


            logits = model(
                eng_ids=eng_ids,
                ita_ids=ita_ids,
                eng_mask=eng_cross_mask,
                ita_mask=ita_causal_mask_expanded,
                eng_token_type_ids=eng_token_type_ids,
                ita_token_type_ids=ita_token_type_ids
            )

            loss = criterion(logits.view(-1, logits.size(-1)), ita_target_ids.view(-1))
            total_val_loss += loss.item()
            val_iterator.set_postfix({'loss': loss.item()})


    avg_val_loss = total_val_loss / len(val_dataloader)
    print(f"Epoch {epoch+1} validation loss: {avg_val_loss:.4f}")

"""Save the state dictionary of the trained model to a file so it can be loaded later for inference or further training."""

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# Define the path to save the model in Google Drive
# You might want to create a specific folder in your Drive, e.g., 'my_models'
model_save_dir = "/content/drive/MyDrive/bert_translation_models"
os.makedirs(model_save_dir, exist_ok=True) # Create directory if it doesn't exist

model_save_path = os.path.join(model_save_dir, "bert_translation_model.pth")

# Save the model's state dictionary
torch.save(model.state_dict(), model_save_path)

print(f"Model parameters saved to {model_save_path}")

# Example usage
english_sentence = "Hi!"
italian_translation = translate_sentence(english_sentence, eng_tokenizer, ita_tokenizer, max_len=MAXLEN, device=device)

print(f"English: {english_sentence}")
print(f"Italian (Translated): {italian_translation}")