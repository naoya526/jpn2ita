{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naoya526/jpn2ita/blob/main/Bert_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b42af237",
      "metadata": {
        "id": "b42af237"
      },
      "source": [
        "## Import Module"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f189bde",
      "metadata": {
        "id": "5f189bde"
      },
      "source": [
        "I used materials below:\n",
        "[1]`06_Attention_and_Transformers_in_BERT.ipynb`\n",
        "[2]`English_to_italian_automatic_translation.ipynb`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "61e92cdc",
      "metadata": {
        "id": "61e92cdc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6151fdbd",
      "metadata": {
        "id": "6151fdbd"
      },
      "source": [
        "## Define Model\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Encoder (Bert) part\n",
        "Here, There's the function for implementing Encoder(Bert). I implemented with refering to [1]`06_Attention_and_Transformers_in_BERT.ipynb` and the paper.\n",
        "- `MultiHeadAttention`\n",
        "- `PositionwiseFeedForward`\n",
        "- `Encoder Block`\n",
        "- `BertEmbeddings` (Embedding for words)\n",
        "- `Bert`\n",
        "Bert is highly possible to understand meaning, but it is not enough for produce translation.\n",
        "Hence, In the next part, I implement Decoder. It is quite similar to Bert."
      ],
      "metadata": {
        "id": "C06d3zDIDp9k"
      },
      "id": "C06d3zDIDp9k"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b9f946b7",
      "metadata": {
        "id": "b9f946b7"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    - Query, Key, Value\n",
        "    - Scaled Dot Product Attention: softmax(QK^T / sqrt(d_k))V\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # Q, K, V linear Conversion\n",
        "        self.query = torch.nn.Linear(d_model, d_model)\n",
        "        self.key = torch.nn.Linear(d_model, d_model)\n",
        "        self.value = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.out_proj = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # step1: Q, K, V\n",
        "        query = self.query(x)  # (batch, seq_len, d_model)\n",
        "        key = self.key(x)      # (batch, seq_len, d_model)\n",
        "        value = self.value(x)  # (batch, seq_len, d_model)\n",
        "\n",
        "        # step2: Multi-Head\n",
        "        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim)  # ‰øÆÊ≠£: query.shape ‚Üí batch_size\n",
        "        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # step3: Change Dimention for Calclate Efficiently\n",
        "        query = query.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, head_dim)\n",
        "        key = key.permute(0, 2, 1, 3)\n",
        "        value = value.permute(0, 2, 1, 3)\n",
        "\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó4: Scaled Dot-Product Attention\n",
        "        # scores = Q @ K^T / sqrt(d_k)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó5: „Éû„Çπ„ÇØÂá¶ÁêÜÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ\n",
        "        if mask is not None:\n",
        "            # maskÂΩ¢Áä∂: (batch, 1, 1, seq_len) ‚Üí scoresÂΩ¢Áä∂: (batch, num_heads, seq_len, seq_len)\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó6: Softmax + Dropout\n",
        "        weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó7: Value „Å®„ÅÆÁ©ç\n",
        "        context = torch.matmul(weights, value)\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó8: „Éò„ÉÉ„Éâ„ÇíÁµêÂêà„Åó„Å¶ÂÖÉ„ÅÆÂΩ¢Áä∂„Å´Êàª„Åô\n",
        "        context = context.permute(0, 2, 1, 3)\n",
        "        # ‚Üí (batch, seq_len, d_model)\n",
        "        context = context.contiguous().view(batch_size, seq_len, self.num_heads * self.head_dim)\n",
        "\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó9: ÊúÄÁµÇÁöÑ„Å™Á∑öÂΩ¢Â§âÊèõ\n",
        "        return self.out_proj(context)  # ‰øÆÊ≠£: output_linear ‚Üí out_proj\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    „Éí„É≥„Éà:\n",
        "    - 2Â±§„ÅÆ„Éï„Ç£„Éº„Éâ„Éï„Ç©„ÉØ„Éº„Éâ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ\n",
        "    - ‰∏≠ÈñìÂ±§„Åß„ÅØÊ¨°ÂÖÉ„ÇíÊã°ÂºµÔºàÈÄöÂ∏∏4ÂÄçÔºâ\n",
        "    - GELUÊ¥ªÊÄßÂåñÈñ¢Êï∞„Çí‰ΩøÁî®\n",
        "    - „Éâ„É≠„ÉÉ„Éó„Ç¢„Ç¶„Éà„ÇÇÂøò„Çå„Åö„Å´\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)  # ÂÖ•ÂäõÊ¨°ÂÖÉ ‚Üí ‰∏≠ÈñìÊ¨°ÂÖÉ\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)  # ‰∏≠ÈñìÊ¨°ÂÖÉ\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    „Éí„É≥„Éà:\n",
        "    - Multi-Head Attention + Residual Connection + Layer Norm\n",
        "    - Feed Forward + Residual Connection + Layer Norm\n",
        "    - Which is better??: Pre-LN vs Post-LN\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model,num_heads)\n",
        "        self.ffn = PositionwiseFeedForward(d_model,d_ff)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layer_norm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        #Attention block\n",
        "        #TODO implement transformer block\n",
        "        residual = x\n",
        "        #print(\"Took Residual...\",x.shape)\n",
        "        x = self.layer_norm1(x)\n",
        "        #print(\"calculating layer norm...\",x.shape)\n",
        "        x = self.dropout(self.attention(x,mask))\n",
        "        #print(\"calculating Attention...\",x.shape)\n",
        "        x = x + residual\n",
        "        #print(\"calculating Residual Connection...\",x.shape)\n",
        "        #ffnn\n",
        "        residual = x\n",
        "        x = self.layer_norm2(x)\n",
        "        #print(\"calculating layer norm...\",x.shape)\n",
        "        x = self.dropout(self.ffn(x))\n",
        "        #print(\"calculating ffn...\",x.shape)\n",
        "        x = x + residual\n",
        "        return x\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    - Token Embeddings (Ë™ûÂΩô„Çµ„Ç§„Ç∫ √ó d_model)\n",
        "    - Position Embeddings (ÊúÄÂ§ßÁ≥ªÂàóÈï∑ √ó d_model)\n",
        "    - Segment Embeddings (2 √ó d_model, NSP„Çø„Çπ„ÇØÁî®)\n",
        "    - 3„Å§„ÇíË∂≥„ÅóÂêà„Çè„Åõ„Å¶LayerNorm„Å®Dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_seq_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # TODO: 3Á®ÆÈ°û„ÅÆÂüã„ÇÅËæº„Åø„ÇíÂÆüË£Ö\n",
        "        self.d_model = d_model\n",
        "        self.token = torch.nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.position = torch.nn.Embedding(max_seq_len, d_model)\n",
        "        self.segment = torch.nn.Embedding(2, d_model)  # 2„Å§„ÅÆ„Çª„Ç∞„É°„É≥„ÉàÔºà0„Å®1Ôºâ\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        #Embedding: Lookup table that keep meaning vector of words\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        # TODO: Âüã„ÇÅËæº„Åø„ÅÆË®àÁÆó„ÇíÂÆüË£Ö\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        # Step 1: Token Embeddings\n",
        "        token_embeddings = self.token(input_ids)\n",
        "        # Step 2: Position Embeddings\n",
        "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
        "        position_ids = position_ids.expand(batch_size, -1)  # üîß „Éê„ÉÉ„ÉÅÊ¨°ÂÖÉ„ÇíÊã°Âºµ\n",
        "        position_embeddings = self.position(position_ids)\n",
        "        # Step 3: Segment Embeddings\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)  # ÂÖ®„Å¶0ÔºàÂçò‰∏ÄÊñáÔºâ\n",
        "        segment_embeddings = self.segment(token_type_ids)  # (batch, seq_len, d_model)\n",
        "        embeddings = token_embeddings + position_embeddings + segment_embeddings\n",
        "        embeddings = self.dropout(self.layer_norm(embeddings))\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "class Bert(nn.Module):\n",
        "    \"\"\"\n",
        "    BERTÂÆüË£Ö„ÅÆÊúÄÁµÇÂΩ¢\n",
        "\n",
        "    Â≠¶Áøí„ÅÆ„Éí„É≥„Éà:\n",
        "    1. Ë´ñÊñá„ÇíË™≠„Çì„ÅßÂÖ®‰ΩìÂÉè„ÇíÁêÜËß£\n",
        "    2. Â∞è„Åï„Å™ÈÉ®ÂìÅ„Åã„ÇâÂÆüË£ÖÔºàAttention ‚Üí FFN ‚Üí Block ‚Üí Full ModelÔºâ\n",
        "    3. ÂêÑÂ±§„Åß print(tensor.shape) „Åó„Å¶„Çµ„Ç§„Ç∫„ÇíÁ¢∫Ë™ç\n",
        "    4. Á∞°Âçò„Å™„ÉÄ„Éü„Éº„Éá„Éº„Çø„Åß„ÉÜ„Çπ„Éà\n",
        "    5. ‰∫ãÂâçÂ≠¶Áøí„ÅØË®àÁÆóÈáè„ÅåÂ§ß„Åç„ÅÑ„ÅÆ„Åß„ÄÅÂ∞è„Åï„ÅÑ„É¢„Éá„É´„Åã„ÇâÈñãÂßã\n",
        "\n",
        "    ÈáçË¶Å„Å™Ê¶ÇÂøµ:\n",
        "    - Bidirectional: Â∑¶Âè≥‰∏°ÊñπÂêë„ÅÆÊñáËÑà„ÇíË¶ã„Çã\n",
        "    - Masked Language Model: „É©„É≥„ÉÄ„É†„Å´„Éû„Çπ„ÇØ„Åó„ÅüÂçòË™û„Çí‰∫àÊ∏¨\n",
        "    - Next Sentence Prediction: 2„Å§„ÅÆÊñá„ÅåÈÄ£Á∂ö„Åô„Çã„Åã„Çí‰∫àÊ∏¨\n",
        "    - Attention Weights: „Å©„ÅÆÂçòË™û„Å´Ê≥®ÁõÆ„Åó„Å¶„ÅÑ„Çã„Åã„ÅÆÂèØË¶ñÂåñ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=768, num_layers=12, num_heads=12, d_ff=3072, max_seq_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.heads = num_heads\n",
        "        # paper noted 4*d_model size for ff\n",
        "        self.feed_forward_hidden = d_model * 4\n",
        "        # embedding for BERT, sum of positional, segment, token embeddings\n",
        "        self.embedding = BertEmbeddings(vocab_size, d_model, max_seq_len, dropout)\n",
        "\n",
        "        self.encoder_blocks = torch.nn.ModuleList(\n",
        "            [EncoderBlock(d_model, num_heads, d_model * 4, dropout) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        # TODO: BERTÂÖ®‰Ωì„ÅÆforward pass„ÇíÂÆüË£Ö\n",
        "        if attention_mask is None:\n",
        "            attention_mask = (input_ids != 0).float()\n",
        "        if attention_mask.dim() == 2:\n",
        "            # (batch, seq_len) ‚Üí (batch, 1, 1, seq_len)\n",
        "            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "            print(\"squeeze is required\")\n",
        "        elif attention_mask.dim() == 4:\n",
        "            # Êó¢„Å´Ê≠£„Åó„ÅÑÂΩ¢Áä∂„ÅÆÂ†¥Âêà„ÅØ„Åù„ÅÆ„Åæ„Åæ‰ΩøÁî®\n",
        "            extended_attention_mask = attention_mask\n",
        "            print(\"squeeze is not required\")\n",
        "        # 0„Çí-1e9„Å´Â§âÊèõÔºàSoftmax„Åß0„Å´„Å™„Çã„Çà„ÅÜ„Å´Ôºâ\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -1e9\n",
        "\n",
        "        # embedding the indexed sequence to sequence of vectors\n",
        "        x = self.embedding(input_ids, token_type_ids)\n",
        "        # running over multiple transformer blocks\n",
        "        for encoder in self.encoder_blocks:\n",
        "            x = encoder.forward(x, extended_attention_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfc507b7",
      "metadata": {
        "id": "dfc507b7"
      },
      "source": [
        "### Decoder part\n",
        "This part, I implemented these functions:\n",
        "- `CrossAttention`(English Queue, Italian Key, Italian Value)\n",
        "- `DecoderBlock`\n",
        "- `BertTranslationModel`(Bert + Decoder Embedding + DecoderBlock*`num_layers`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "_pie-kLFt9PO",
      "metadata": {
        "id": "_pie-kLFt9PO"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    this module is implemented with modifying MultiHeadAttention.\n",
        "    Query: English\n",
        "    Key, Value: Italian\n",
        "    You can see the difference in forward input\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__() # initialization\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads  # dimention of each head\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # Q, K, V „ÅÆÁ∑öÂΩ¢Â§âÊèõÔºà‰øÆÊ≠£Ôºötorch.nn.linear ‚Üí torch.nn.LinearÔºâ\n",
        "        self.query = torch.nn.Linear(d_model, d_model)\n",
        "        self.key = torch.nn.Linear(d_model, d_model)\n",
        "        self.value = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        # ÊúÄÁµÇÁöÑ„Å™Âá∫ÂäõÂ§âÊèõ\n",
        "        self.out_proj = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query_input, key_value_input, mask=None): # here is the difference\n",
        "        batch_size, q_len, _ = query_input.shape\n",
        "        _, kv_len, _ = key_value_input.shape\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó1: Q, K, V „ÇíÁ∑öÂΩ¢Â§âÊèõ„ÅßÁîüÊàê\n",
        "        query = self.query(query_input)  # (batch, seq_len, d_model)\n",
        "        key = self.key(key_value_input)      # (batch, seq_len, d_model)\n",
        "        value = self.value(key_value_input)  # (batch, seq_len, d_model)\n",
        "\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó2: Multi-HeadÁî®„Å´Ê¨°ÂÖÉ„ÇíÂ§âÂΩ¢\n",
        "        query = query.view(batch_size, q_len, self.num_heads, self.head_dim)\n",
        "        key = key.view(batch_size, kv_len, self.num_heads, self.head_dim)  # ‰øÆÊ≠£: query.shape ‚Üí batch_size\n",
        "        value = value.view(batch_size, kv_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        query = query.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, head_dim)\n",
        "        key = key.permute(0, 2, 1, 3)\n",
        "        value = value.permute(0, 2, 1, 3)\n",
        "\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó4: Scaled Dot-Product Attention\n",
        "        # scores = Q @ K^T / sqrt(d_k)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó5: „Éû„Çπ„ÇØÂá¶ÁêÜÔºà„Ç™„Éó„Ç∑„Éß„É≥Ôºâ\n",
        "        if mask is not None:\n",
        "            # maskÂΩ¢Áä∂: (batch, 1, 1, seq_len) ‚Üí scoresÂΩ¢Áä∂: (batch, num_heads, seq_len, seq_len)\n",
        "            scores = scores + mask  # „Éñ„É≠„Éº„Éâ„Ç≠„É£„Çπ„Éà„ÅßÂä†ÁÆó\n",
        "\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó6: Softmax + Dropout\n",
        "        weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó7: Value „Å®„ÅÆÁ©ç\n",
        "        context = torch.matmul(weights, value)\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó8: „Éò„ÉÉ„Éâ„ÇíÁµêÂêà„Åó„Å¶ÂÖÉ„ÅÆÂΩ¢Áä∂„Å´Êàª„Åô\n",
        "        context = context.permute(0, 2, 1, 3)\n",
        "        # ‚Üí (batch, seq_len, d_model)\n",
        "        context = context.contiguous().view(batch_size, q_len, self.num_heads * self.head_dim)\n",
        "\n",
        "        # „Çπ„ÉÜ„ÉÉ„Éó9: ÊúÄÁµÇÁöÑ„Å™Á∑öÂΩ¢Â§âÊèõ\n",
        "        return self.out_proj(context)  # ‰øÆÊ≠£: output_linear ‚Üí out_proj\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Basically similar to EncoderBlock, but refer to the infomation of Input(English context)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        #First, implement Self Attention\n",
        "        self.self_attention = MultiHeadAttention(d_model,num_heads)\n",
        "        #Second, implement Cross Attention\n",
        "        self.cross_attention = CrossAttention(d_model, num_heads)\n",
        "        #Third, FFNN\n",
        "        self.ffn = PositionwiseFeedForward(d_model,d_ff)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layer_norm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layer_norm3 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, self_mask=None, cross_mask=None):\n",
        "        #Self Attention\n",
        "        residual = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.self_attention(x,mask=self_mask)\n",
        "        x = self.dropout(x) + residual\n",
        "\n",
        "        #Cross Attention\n",
        "        residual = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.cross_attention(\n",
        "            query_input=x,\n",
        "            key_value_input=encoder_output,\n",
        "            mask=cross_mask\n",
        "        )\n",
        "        x = self.dropout(x) + residual\n",
        "\n",
        "        residual = x\n",
        "        x = self.layer_norm3(x)\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout(x) + residual\n",
        "        return x\n",
        "\n",
        "class BertTranslationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Ita2Eng Translation Model\n",
        "    Encoder: Bert\n",
        "    Decoder: BertEmbedding, DecoderBlock*N, FFN\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 ita_vocab_size,  # „Ç§„Çø„É™„Ç¢Ë™ûË™ûÂΩô„Çµ„Ç§„Ç∫\n",
        "                 eng_vocab_size,  # Ëã±Ë™ûË™ûÂΩô„Çµ„Ç§„Ç∫\n",
        "                 max_seq_len,\n",
        "                 d_model=512,\n",
        "                 num_layers=6,\n",
        "                 num_heads=8,\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Bert(\n",
        "            vocab_size=eng_vocab_size,\n",
        "            d_model=d_model,\n",
        "            num_layers=num_layers,\n",
        "            num_heads=num_heads,\n",
        "            max_seq_len=max_seq_len,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.decoder_embeddings = BertEmbeddings(\n",
        "            vocab_size=ita_vocab_size,\n",
        "            d_model=d_model,\n",
        "            max_seq_len=max_seq_len,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            DecoderBlock(\n",
        "                d_model=d_model,\n",
        "                num_heads=num_heads,\n",
        "                d_ff=d_model * 4, #based on the paper of Bert\n",
        "                dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_proj = nn.Linear(d_model, ita_vocab_size)\n",
        "\n",
        "    def forward(self,\n",
        "                eng_ids,\n",
        "                ita_ids,\n",
        "                eng_mask=None,\n",
        "                ita_mask=None,\n",
        "                eng_token_type_ids=None,\n",
        "                ita_token_type_ids=None):\n",
        "        # understand english\n",
        "        encoder_output = self.encoder(input_ids=eng_ids, attention_mask=eng_mask, token_type_ids=eng_token_type_ids)\n",
        "        # produce Italian\n",
        "        decoder_input = self.decoder_embeddings(input_ids=ita_ids, token_type_ids=ita_token_type_ids)\n",
        "\n",
        "        for decoder_block in self.decoder_blocks:\n",
        "            decoder_input = decoder_block(\n",
        "                x=decoder_input,\n",
        "                encoder_output=encoder_output,\n",
        "                self_mask=ita_mask,               # Ëã±Ë™û„ÅÆCausal mask\n",
        "                cross_mask=eng_mask\n",
        "                )\n",
        "        logits = self.output_proj(decoder_input)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d02d832e",
      "metadata": {
        "id": "d02d832e"
      },
      "source": [
        "## Use model\n",
        "In this part, I followed the configuration of [2]`English_to_italian_automatic_translation.ipynb`.\n",
        "\n",
        "---\n",
        "### Prepare Dataset\n",
        "for Bert, `<sos>`and `<eos>` are not required. Hence, ignore these token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "EZbXW_bCx1Ef",
      "metadata": {
        "id": "EZbXW_bCx1Ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707ccfa5-da8a-4125-b82a-2f459c8f6cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT\n",
            "To: <_io.BufferedWriter name='<stdout>'>\n",
            "100% 3.92M/3.92M [00:00<00:00, 13.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the files\n",
        "URL = \"https://drive.google.com/file/d/1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT/view?usp=sharing\"\n",
        "!gdown --fuzzy $URL -O- | tar -xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7uYUnrAouYf1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uYUnrAouYf1",
        "outputId": "9ea6e1a6-0e18-4848-b0a5-fe155eeead97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi .', 'hi .', 'run !', 'run !', 'run !', 'who ?', 'wow !', 'duck !', 'duck !', 'jump !', 'jump !', 'jump !', 'jump .', 'jump .', 'jump .', 'stay .', 'stay .', 'stay .', 'stay .', 'stay .', 'stay .', 'stay .', 'stay .', 'stay .', 'stop !', 'stop !', 'stop !', 'wait !', 'wait !', 'wait !', 'wait .', 'wait .', 'wait .', 'do it .', 'do it .', 'do it .', 'do it .', 'do it .', 'do it .', 'go on .', 'go on .', 'go on .', 'go on .', 'go on .', 'go on .', 'hello !', 'hello !', 'hello !', 'hello .', 'i hid .']\n",
            "['ciao !', 'ciao .', 'corri !', 'corra !', 'correte !', 'chi ?', 'wow !', 'amore !', 'tesoro !', 'salta !', 'salti !', 'saltate !', 'salta .', 'salti .', 'saltate .', 'resta .', 'stai .', 'stia .', 'state .', 'resti .', 'restate .', 'rimani .', 'rimanga .', 'rimanete .', 'fermati !', 'fermatevi !', 'si fermi !', 'aspetta !', 'aspettate !', 'aspetti !', 'aspetta .', 'aspetti .', 'aspettate .', 'fallo .', 'falla .', 'lo faccia .', 'la faccia .', 'fatelo .', 'fatela .', 'vai avanti .', 'continua .', 'continui .', 'continuate .', 'vada avanti .', 'andate avanti .', 'buongiorno !', 'ciao !', 'salve .', 'ciao .', 'mi sono nascosto .']\n",
            "333112 333112\n"
          ]
        }
      ],
      "source": [
        "# for Bert, <sos> and <eos> are not required\n",
        "#SPECIAL = [\"<sos>\", \"<eos>\", \"<pad>\"]\n",
        "SPECIAL = [\"<pad>\"]\n",
        "MAXLEN = 20\n",
        "\n",
        "f = open(\"text-eng.txt\")\n",
        "# Define the list of all tokens in the English set ...\n",
        "ENG_VOCABULARY = []\n",
        "for line in f:\n",
        "    line = line.strip()\n",
        "    # Remove <sos> and <eos>\n",
        "    line = line.replace('<sos>', '').replace('<eos>', '').strip()\n",
        "    if line == \"\":\n",
        "        continue\n",
        "\n",
        "    ENG_VOCABULARY.append(line)\n",
        "f.close()\n",
        "print(ENG_VOCABULARY[:50])\n",
        "\n",
        "f = open(\"text-ita.txt\")\n",
        "# Define the list of all tokens in the Italian set ...\n",
        "ITA_VOCABULARY = []\n",
        "for line in f:\n",
        "    line = line.strip()\n",
        "    # Remove <sos> and <eos>\n",
        "    line = line.replace('<sos>', '').replace('<eos>', '').strip()\n",
        "    if line == \"\":\n",
        "        continue\n",
        "    ITA_VOCABULARY.append(line)\n",
        "f.close()\n",
        "print(ITA_VOCABULARY[:50])\n",
        "# Make sure that the three special tokens have the same indices in the two vocabularies.\n",
        "# Assign here the three indices...\n",
        "\n",
        "PAD = SPECIAL[0]\n",
        "\n",
        "# Inverse mappings.\n",
        "ENG_INVERSE = {w: n for n, w in enumerate(ENG_VOCABULARY)}\n",
        "ITA_INVERSE = {w: n for n, w in enumerate(ITA_VOCABULARY)}\n",
        "#print(ENG_INVERSE)\n",
        "print(len(ENG_VOCABULARY), len(ITA_VOCABULARY))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5I5NmJq4yeec",
      "metadata": {
        "id": "5I5NmJq4yeec"
      },
      "source": [
        "### Incremental approach to token vocabulary building%\n",
        "In the lesson of Deep Learning, I learned the sophisticated way of tokenizing words called WordPiece tokenization.\n",
        "\n",
        "The WordPiece tokenization algorithm builds its vocabulary incrementally, starting from a basic alphabet and iteratively merging subword units based on their frequency and co-occurrence patterns. (cited from [1]`06_Attention_and_Transformers_in_Bert.ipynb`)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### The demonstration of pretrained tokenizer\n",
        "\n",
        "In [1], Tokenizer `bert-base-cased` was used for English(For tokenization of English, it's used in this project as well). In this project, Tokenizier [3]`dbmdz/bert-base-italian-cased` for italian is used.\n",
        "[3]https://huggingface.co/dbmdz/bert-base-italian-cased  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "TOiqXhYIus4m",
      "metadata": {
        "id": "TOiqXhYIus4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94d7f4b-bf4c-4602-e498-9e7452511689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: ['This', 'is', 'the', 'Hugging', 'Face', 'Course', '.']\n",
            "English: ['This', 'chapter', 'is', 'about', 'tokenization', '.']\n",
            "English: ['This', 'section', 'shows', 'several', 'tokenizer', 'algorithms', '.']\n",
            "English: ['Hopefully', ',', 'you', 'will', 'be', 'able', 'to', 'understand', 'how', 'they', 'are', 'trained', 'and', 'generate', 'tokens', '.']\n",
            "Italian: ['Questo', '√®', 'il', 'corso', 'di', 'Hugging', 'Face', '.']\n",
            "Italian: ['Questo', 'capitolo', 'riguarda', 'la', 'tokenizzazione', '.']\n",
            "Italian: ['Questa', 'sezione', 'mostra', 'diversi', 'algoritmi', 'di', 'tokenizzazione', '.']\n",
            "Italian: ['Speriamo', 'che', 'tu', 'sia', 'in', 'grado', 'di', 'capire', 'come', 'vengono', 'addestrati', 'e', 'generano', 'token', '.']\n",
            "\n",
            "English Word Frequency: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})\n",
            "Italian Word Frequency: defaultdict(<class 'int'>, {'Questo': 2, '√®': 1, 'il': 1, 'corso': 1, 'di': 3, 'Hugging': 1, 'Face': 1, '.': 4, 'capitolo': 1, 'riguarda': 1, 'la': 1, 'tokenizzazione': 2, 'Questa': 1, 'sezione': 1, 'mostra': 1, 'diversi': 1, 'algoritmi': 1, 'Speriamo': 1, 'che': 1, 'tu': 1, 'sia': 1, 'in': 1, 'grado': 1, 'capire': 1, 'come': 1, 'vengono': 1, 'addestrati': 1, 'e': 1, 'generano': 1, 'token': 1})\n",
            "\n",
            "English vocab size: 28996\n",
            "Italian vocab size: 31102\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "### Suppress useless warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"The secret `HF_TOKEN` does not exist\")\n",
        "\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "eng_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")  # English\n",
        "ita_tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\")  # Italian\n",
        "\n",
        "### Example bilingual corpus\n",
        "eng_corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]\n",
        "\n",
        "ita_corpus = [\n",
        "    \"Questo √® il corso di Hugging Face.\",\n",
        "    \"Questo capitolo riguarda la tokenizzazione.\",\n",
        "    \"Questa sezione mostra diversi algoritmi di tokenizzazione.\",\n",
        "    \"Speriamo che tu sia in grado di capire come vengono addestrati e generano token.\",\n",
        "]\n",
        "\n",
        "### Get frequency for English\n",
        "eng_word_freqs = defaultdict(int)\n",
        "for text in eng_corpus:\n",
        "    words_with_offsets = eng_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    print(f\"English: {new_words}\")\n",
        "    for word in new_words:\n",
        "        eng_word_freqs[word] += 1\n",
        "\n",
        "### Get frequency for Italian\n",
        "ita_word_freqs = defaultdict(int)\n",
        "for text in ita_corpus:\n",
        "    words_with_offsets = ita_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    print(f\"Italian: {new_words}\")\n",
        "    for word in new_words:\n",
        "        ita_word_freqs[word] += 1\n",
        "\n",
        "print(f\"\\nEnglish Word Frequency: {eng_word_freqs}\")\n",
        "print(f\"Italian Word Frequency: {ita_word_freqs}\")\n",
        "\n",
        "# Get vocabulary sizes for model initialization\n",
        "eng_vocab_size = eng_tokenizer.vocab_size\n",
        "ita_vocab_size = ita_tokenizer.vocab_size\n",
        "print(f\"\\nEnglish vocab size: {eng_vocab_size}\")\n",
        "print(f\"Italian vocab size: {ita_vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca76e653",
      "metadata": {
        "id": "ca76e653"
      },
      "source": [
        "### Tokenization Helper Functions\n",
        "\n",
        "Define helper functions to tokenize and encode text for both languages consistently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dc36b6ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc36b6ad",
        "outputId": "064e7121-b923-4275-b9a7-d632cfa856c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English encoded: {'input_ids': tensor([[ 101, 8667, 1362,  102,    0,    0],\n",
            "        [ 101, 1731, 1132, 1128,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1]])}\n",
            "Italian encoded: {'input_ids': tensor([[ 102, 2009, 1015,  103,    0],\n",
            "        [ 102,  804, 1628, 3098,  103]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "def tokenize_and_encode(texts, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Tokenize and encode a list of texts using the given tokenizer\n",
        "\n",
        "    Args:\n",
        "        texts: List of strings to tokenize\n",
        "        tokenizer: HuggingFace tokenizer\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with input_ids, attention_mask, etc.\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "def prepare_translation_data(eng_texts, ita_texts, eng_tokenizer, ita_tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Prepare data for translation training\n",
        "\n",
        "    Args:\n",
        "        eng_texts: List of English sentences\n",
        "        ita_texts: List of Italian sentences (parallel corpus)\n",
        "        eng_tokenizer: English tokenizer\n",
        "        ita_tokenizer: Italian tokenizer\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (eng_data, ita_data) dictionaries\n",
        "    \"\"\"\n",
        "    eng_data = tokenize_and_encode(eng_texts, eng_tokenizer, max_length)\n",
        "    ita_data = tokenize_and_encode(ita_texts, ita_tokenizer, max_length)\n",
        "\n",
        "    return eng_data, ita_data\n",
        "\n",
        "# Example usage with sample data\n",
        "sample_eng = [\"Hello world\", \"How are you?\"]\n",
        "sample_ita = [\"Ciao mondo\", \"Come stai?\"]\n",
        "\n",
        "eng_encoded, ita_encoded = prepare_translation_data(\n",
        "    sample_eng, sample_ita, eng_tokenizer, ita_tokenizer\n",
        ")\n",
        "\n",
        "print(\"English encoded:\", eng_encoded)\n",
        "print(\"Italian encoded:\", ita_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b89c0e",
      "metadata": {
        "id": "a1b89c0e"
      },
      "source": [
        "### Update Model Configuration\n",
        "\n",
        "Update the model initialization to use the proper vocabulary sizes from the tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c0ae83b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0ae83b9",
        "outputId": "a07dcaa6-c589-4b36-d145-32f6bef25202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized with:\n",
            "- English vocab size: 28996\n",
            "- Italian vocab size: 31102\n",
            "- Model parameters: 91,392,382\n"
          ]
        }
      ],
      "source": [
        "# Initialize translation model with proper vocabulary sizes\n",
        "translation_model = BertTranslationModel(\n",
        "    ita_vocab_size=ita_vocab_size,  # Italian output vocabulary\n",
        "    eng_vocab_size=eng_vocab_size,  # English input vocabulary\n",
        "    max_seq_len=512,\n",
        "    d_model=512,\n",
        "    num_layers=6,\n",
        "    num_heads=8,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "print(f\"Model initialized with:\")\n",
        "print(f\"- English vocab size: {eng_vocab_size}\")\n",
        "print(f\"- Italian vocab size: {ita_vocab_size}\")\n",
        "print(f\"- Model parameters: {sum(p.numel() for p in translation_model.parameters()):,}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}