{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naoya526/jpn2ita/blob/main/Bert_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b42af237",
      "metadata": {
        "id": "b42af237"
      },
      "source": [
        "## Import Module"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f189bde",
      "metadata": {
        "id": "5f189bde"
      },
      "source": [
        "I used materials below:\n",
        "[1]`06_Attention_and_Transformers_in_BERT.ipynb`\n",
        "[2]`English_to_italian_automatic_translation.ipynb`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "61e92cdc",
      "metadata": {
        "id": "61e92cdc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6151fdbd",
      "metadata": {
        "id": "6151fdbd"
      },
      "source": [
        "## Define Model\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Encoder (Bert) part\n",
        "Here, There's the function for implementing Encoder(Bert). I implemented with refering to [1]`06_Attention_and_Transformers_in_BERT.ipynb` and the paper.\n",
        "- `MultiHeadAttention`\n",
        "- `PositionwiseFeedForward`\n",
        "- `Encoder Block`\n",
        "- `BertEmbeddings` (Embedding for words)\n",
        "- `Bert`\n",
        "Bert is highly possible to understand meaning, but it is not enough for produce translation.\n",
        "Hence, In the next part, I implement Decoder. It is quite similar to Bert."
      ],
      "metadata": {
        "id": "C06d3zDIDp9k"
      },
      "id": "C06d3zDIDp9k"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b9f946b7",
      "metadata": {
        "id": "b9f946b7"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    - Query, Key, Value\n",
        "    - Scaled Dot Product Attention: softmax(QK^T / sqrt(d_k))V\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # Q, K, V linear Conversion\n",
        "        self.query = torch.nn.Linear(d_model, d_model)\n",
        "        self.key = torch.nn.Linear(d_model, d_model)\n",
        "        self.value = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.out_proj = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # step1: Q, K, V\n",
        "        query = self.query(x)  # (batch, seq_len, d_model)\n",
        "        key = self.key(x)      # (batch, seq_len, d_model)\n",
        "        value = self.value(x)  # (batch, seq_len, d_model)\n",
        "\n",
        "        # step2: Multi-Head\n",
        "        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim)  # 修正: query.shape → batch_size\n",
        "        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # step3: Change Dimention for Calclate Efficiently\n",
        "        query = query.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, head_dim)\n",
        "        key = key.permute(0, 2, 1, 3)\n",
        "        value = value.permute(0, 2, 1, 3)\n",
        "\n",
        "        # ステップ4: Scaled Dot-Product Attention\n",
        "        # scores = Q @ K^T / sqrt(d_k)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # ステップ5: マスク処理（オプション）\n",
        "        if mask is not None:\n",
        "            # mask形状: (batch, 1, 1, seq_len) → scores形状: (batch, num_heads, seq_len, seq_len)\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # ステップ6: Softmax + Dropout\n",
        "        weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # ステップ7: Value との積\n",
        "        context = torch.matmul(weights, value)\n",
        "        # ステップ8: ヘッドを結合して元の形状に戻す\n",
        "        context = context.permute(0, 2, 1, 3)\n",
        "        # → (batch, seq_len, d_model)\n",
        "        context = context.contiguous().view(batch_size, seq_len, self.num_heads * self.head_dim)\n",
        "\n",
        "        # ステップ9: 最終的な線形変換\n",
        "        return self.out_proj(context)  # 修正: output_linear → out_proj\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    ヒント:\n",
        "    - 2層のフィードフォワードネットワーク\n",
        "    - 中間層では次元を拡張（通常4倍）\n",
        "    - GELU活性化関数を使用\n",
        "    - ドロップアウトも忘れずに\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)  # 入力次元 → 中間次元\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)  # 中間次元\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ヒント:\n",
        "    - Multi-Head Attention + Residual Connection + Layer Norm\n",
        "    - Feed Forward + Residual Connection + Layer Norm\n",
        "    - Which is better??: Pre-LN vs Post-LN\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model,num_heads)\n",
        "        self.ffn = PositionwiseFeedForward(d_model,d_ff)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layer_norm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        #Attention block\n",
        "        #TODO implement transformer block\n",
        "        residual = x\n",
        "        #print(\"Took Residual...\",x.shape)\n",
        "        x = self.layer_norm1(x)\n",
        "        #print(\"calculating layer norm...\",x.shape)\n",
        "        x = self.dropout(self.attention(x,mask))\n",
        "        #print(\"calculating Attention...\",x.shape)\n",
        "        x = x + residual\n",
        "        #print(\"calculating Residual Connection...\",x.shape)\n",
        "        #ffnn\n",
        "        residual = x\n",
        "        x = self.layer_norm2(x)\n",
        "        #print(\"calculating layer norm...\",x.shape)\n",
        "        x = self.dropout(self.ffn(x))\n",
        "        #print(\"calculating ffn...\",x.shape)\n",
        "        x = x + residual\n",
        "        return x\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    - Token Embeddings (語彙サイズ × d_model)\n",
        "    - Position Embeddings (最大系列長 × d_model)\n",
        "    - Segment Embeddings (2 × d_model, NSPタスク用)\n",
        "    - 3つを足し合わせてLayerNormとDropout\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_seq_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # TODO: 3種類の埋め込みを実装\n",
        "        self.d_model = d_model\n",
        "        self.token = torch.nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.position = torch.nn.Embedding(max_seq_len, d_model)\n",
        "        self.segment = torch.nn.Embedding(2, d_model)  # 2つのセグメント（0と1）\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        #Embedding: Lookup table that keep meaning vector of words\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        # TODO: 埋め込みの計算を実装\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        # Step 1: Token Embeddings\n",
        "        token_embeddings = self.token(input_ids)\n",
        "        # Step 2: Position Embeddings\n",
        "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
        "        position_ids = position_ids.expand(batch_size, -1)  # 🔧 バッチ次元を拡張\n",
        "        position_embeddings = self.position(position_ids)\n",
        "        # Step 3: Segment Embeddings\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)  # 全て0（単一文）\n",
        "        segment_embeddings = self.segment(token_type_ids)  # (batch, seq_len, d_model)\n",
        "        embeddings = token_embeddings + position_embeddings + segment_embeddings\n",
        "        embeddings = self.dropout(self.layer_norm(embeddings))\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "class Bert(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT実装の最終形\n",
        "\n",
        "    学習のヒント:\n",
        "    1. 論文を読んで全体像を理解\n",
        "    2. 小さな部品から実装（Attention → FFN → Block → Full Model）\n",
        "    3. 各層で print(tensor.shape) してサイズを確認\n",
        "    4. 簡単なダミーデータでテスト\n",
        "    5. 事前学習は計算量が大きいので、小さいモデルから開始\n",
        "\n",
        "    重要な概念:\n",
        "    - Bidirectional: 左右両方向の文脈を見る\n",
        "    - Masked Language Model: ランダムにマスクした単語を予測\n",
        "    - Next Sentence Prediction: 2つの文が連続するかを予測\n",
        "    - Attention Weights: どの単語に注目しているかの可視化\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=768, num_layers=12, num_heads=12, d_ff=3072, max_seq_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.heads = num_heads\n",
        "        # paper noted 4*d_model size for ff\n",
        "        self.feed_forward_hidden = d_model * 4\n",
        "        # embedding for BERT, sum of positional, segment, token embeddings\n",
        "        self.embedding = BertEmbeddings(vocab_size, d_model, max_seq_len, dropout)\n",
        "\n",
        "        self.encoder_blocks = torch.nn.ModuleList(\n",
        "            [EncoderBlock(d_model, num_heads, d_model * 4, dropout) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        # TODO: BERT全体のforward passを実装\n",
        "        if attention_mask is None:\n",
        "            attention_mask = (input_ids != 0).float()\n",
        "        if attention_mask.dim() == 2:\n",
        "            # (batch, seq_len) → (batch, 1, 1, seq_len)\n",
        "            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "            print(\"squeeze is required\")\n",
        "        elif attention_mask.dim() == 4:\n",
        "            # 既に正しい形状の場合はそのまま使用\n",
        "            extended_attention_mask = attention_mask\n",
        "            print(\"squeeze is not required\")\n",
        "        # 0を-1e9に変換（Softmaxで0になるように）\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -1e9\n",
        "\n",
        "        # embedding the indexed sequence to sequence of vectors\n",
        "        x = self.embedding(input_ids, token_type_ids)\n",
        "        # running over multiple transformer blocks\n",
        "        for encoder in self.encoder_blocks:\n",
        "            x = encoder.forward(x, extended_attention_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfc507b7",
      "metadata": {
        "id": "dfc507b7"
      },
      "source": [
        "### Decoder part\n",
        "This part, I implemented these functions:\n",
        "- `CrossAttention`(English Queue, Italian Key, Italian Value)\n",
        "- `DecoderBlock`\n",
        "- `BertTranslationModel`(Bert + Decoder Embedding + DecoderBlock*`num_layers`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "_pie-kLFt9PO",
      "metadata": {
        "id": "_pie-kLFt9PO"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    this module is implemented with modifying MultiHeadAttention.\n",
        "    Query: English\n",
        "    Key, Value: Italian\n",
        "    You can see the difference in forward input\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__() # initialization\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads  # dimention of each head\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # Q, K, V の線形変換（修正：torch.nn.linear → torch.nn.Linear）\n",
        "        self.query = torch.nn.Linear(d_model, d_model)\n",
        "        self.key = torch.nn.Linear(d_model, d_model)\n",
        "        self.value = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        # 最終的な出力変換\n",
        "        self.out_proj = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query_input, key_value_input, mask=None): # here is the difference\n",
        "        batch_size, q_len, _ = query_input.shape\n",
        "        _, kv_len, _ = key_value_input.shape\n",
        "        # ステップ1: Q, K, V を線形変換で生成\n",
        "        query = self.query(query_input)  # (batch, seq_len, d_model)\n",
        "        key = self.key(key_value_input)      # (batch, seq_len, d_model)\n",
        "        value = self.value(key_value_input)  # (batch, seq_len, d_model)\n",
        "\n",
        "        # ステップ2: Multi-Head用に次元を変形\n",
        "        query = query.view(batch_size, q_len, self.num_heads, self.head_dim)\n",
        "        key = key.view(batch_size, kv_len, self.num_heads, self.head_dim)  # 修正: query.shape → batch_size\n",
        "        value = value.view(batch_size, kv_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        query = query.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, head_dim)\n",
        "        key = key.permute(0, 2, 1, 3)\n",
        "        value = value.permute(0, 2, 1, 3)\n",
        "\n",
        "        # ステップ4: Scaled Dot-Product Attention\n",
        "        # scores = Q @ K^T / sqrt(d_k)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # ステップ5: マスク処理（オプション）\n",
        "        if mask is not None:\n",
        "            # mask形状: (batch, 1, 1, seq_len) → scores形状: (batch, num_heads, seq_len, seq_len)\n",
        "            scores = scores + mask  # ブロードキャストで加算\n",
        "\n",
        "        # ステップ6: Softmax + Dropout\n",
        "        weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # ステップ7: Value との積\n",
        "        context = torch.matmul(weights, value)\n",
        "        # ステップ8: ヘッドを結合して元の形状に戻す\n",
        "        context = context.permute(0, 2, 1, 3)\n",
        "        # → (batch, seq_len, d_model)\n",
        "        context = context.contiguous().view(batch_size, q_len, self.num_heads * self.head_dim)\n",
        "\n",
        "        # ステップ9: 最終的な線形変換\n",
        "        return self.out_proj(context)  # 修正: output_linear → out_proj\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Basically similar to EncoderBlock, but refer to the infomation of Input(English context)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        #First, implement Self Attention\n",
        "        self.self_attention = MultiHeadAttention(d_model,num_heads)\n",
        "        #Second, implement Cross Attention\n",
        "        self.cross_attention = CrossAttention(d_model, num_heads)\n",
        "        #Third, FFNN\n",
        "        self.ffn = PositionwiseFeedForward(d_model,d_ff)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layer_norm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layer_norm3 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, self_mask=None, cross_mask=None):\n",
        "        #Self Attention\n",
        "        residual = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.self_attention(x,mask=self_mask)\n",
        "        x = self.dropout(x) + residual\n",
        "\n",
        "        #Cross Attention\n",
        "        residual = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.cross_attention(\n",
        "            query_input=x,\n",
        "            key_value_input=encoder_output,\n",
        "            mask=cross_mask\n",
        "        )\n",
        "        x = self.dropout(x) + residual\n",
        "\n",
        "        residual = x\n",
        "        x = self.layer_norm3(x)\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout(x) + residual\n",
        "        return x\n",
        "\n",
        "class BertTranslationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Ita2Eng Translation Model\n",
        "    Encoder: Bert\n",
        "    Decoder: BertEmbedding, DecoderBlock*N, FFN\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 ita_vocab_size,  # イタリア語語彙サイズ\n",
        "                 eng_vocab_size,  # 英語語彙サイズ\n",
        "                 max_seq_len,\n",
        "                 d_model=512,\n",
        "                 num_layers=6,\n",
        "                 num_heads=8,\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Bert(\n",
        "            vocab_size=eng_vocab_size,\n",
        "            d_model=d_model,\n",
        "            num_layers=num_layers,\n",
        "            num_heads=num_heads,\n",
        "            max_seq_len=max_seq_len,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.decoder_embeddings = BertEmbeddings(\n",
        "            vocab_size=ita_vocab_size,\n",
        "            d_model=d_model,\n",
        "            max_seq_len=max_seq_len,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            DecoderBlock(\n",
        "                d_model=d_model,\n",
        "                num_heads=num_heads,\n",
        "                d_ff=d_model * 4, #based on the paper of Bert\n",
        "                dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_proj = nn.Linear(d_model, ita_vocab_size)\n",
        "\n",
        "    def forward(self,\n",
        "                eng_ids,\n",
        "                ita_ids,\n",
        "                eng_mask=None,\n",
        "                ita_mask=None,\n",
        "                eng_token_type_ids=None,\n",
        "                ita_token_type_ids=None):\n",
        "        # understand english\n",
        "        encoder_output = self.encoder(input_ids=eng_ids, attention_mask=eng_mask, token_type_ids=eng_token_type_ids)\n",
        "        # produce Italian\n",
        "        decoder_input = self.decoder_embeddings(input_ids=ita_ids, token_type_ids=ita_token_type_ids)\n",
        "\n",
        "        for decoder_block in self.decoder_blocks:\n",
        "            decoder_input = decoder_block(\n",
        "                x=decoder_input,\n",
        "                encoder_output=encoder_output,\n",
        "                self_mask=ita_mask,               # 英語のCausal mask\n",
        "                cross_mask=eng_mask\n",
        "                )\n",
        "        logits = self.output_proj(decoder_input)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d02d832e",
      "metadata": {
        "id": "d02d832e"
      },
      "source": [
        "## Use model\n",
        "In this part, I followed the configuration of [2]`English_to_italian_automatic_translation.ipynb`.\n",
        "\n",
        "---\n",
        "### Prepare Dataset\n",
        "for Bert, `<sos>`and `<eos>` are not required. Hence, ignore these token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "EZbXW_bCx1Ef",
      "metadata": {
        "id": "EZbXW_bCx1Ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707ccfa5-da8a-4125-b82a-2f459c8f6cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT\n",
            "To: <_io.BufferedWriter name='<stdout>'>\n",
            "100% 3.92M/3.92M [00:00<00:00, 13.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the files\n",
        "URL = \"https://drive.google.com/file/d/1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT/view?usp=sharing\"\n",
        "!gdown --fuzzy $URL -O- | tar -xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7uYUnrAouYf1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uYUnrAouYf1",
        "outputId": "9ea6e1a6-0e18-4848-b0a5-fe155eeead97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi .', 'hi .', 'run !', 'run !', 'run !', 'who ?', 'wow !', 'duck !', 'duck !', 'jump !', 'jump !', 'jump !', 'jump .', 'jump .', 'jump .', 'stay .', 'stay .', 'stay .', 'stay .', 'stay .', 'stay .', 'stay .', 'stay .', 'stay .', 'stop !', 'stop !', 'stop !', 'wait !', 'wait !', 'wait !', 'wait .', 'wait .', 'wait .', 'do it .', 'do it .', 'do it .', 'do it .', 'do it .', 'do it .', 'go on .', 'go on .', 'go on .', 'go on .', 'go on .', 'go on .', 'hello !', 'hello !', 'hello !', 'hello .', 'i hid .']\n",
            "['ciao !', 'ciao .', 'corri !', 'corra !', 'correte !', 'chi ?', 'wow !', 'amore !', 'tesoro !', 'salta !', 'salti !', 'saltate !', 'salta .', 'salti .', 'saltate .', 'resta .', 'stai .', 'stia .', 'state .', 'resti .', 'restate .', 'rimani .', 'rimanga .', 'rimanete .', 'fermati !', 'fermatevi !', 'si fermi !', 'aspetta !', 'aspettate !', 'aspetti !', 'aspetta .', 'aspetti .', 'aspettate .', 'fallo .', 'falla .', 'lo faccia .', 'la faccia .', 'fatelo .', 'fatela .', 'vai avanti .', 'continua .', 'continui .', 'continuate .', 'vada avanti .', 'andate avanti .', 'buongiorno !', 'ciao !', 'salve .', 'ciao .', 'mi sono nascosto .']\n",
            "333112 333112\n"
          ]
        }
      ],
      "source": [
        "# for Bert, <sos> and <eos> are not required\n",
        "#SPECIAL = [\"<sos>\", \"<eos>\", \"<pad>\"]\n",
        "SPECIAL = [\"<pad>\"]\n",
        "MAXLEN = 20\n",
        "\n",
        "f = open(\"text-eng.txt\")\n",
        "# Define the list of all tokens in the English set ...\n",
        "ENG_VOCABULARY = []\n",
        "for line in f:\n",
        "    line = line.strip()\n",
        "    # Remove <sos> and <eos>\n",
        "    line = line.replace('<sos>', '').replace('<eos>', '').strip()\n",
        "    if line == \"\":\n",
        "        continue\n",
        "\n",
        "    ENG_VOCABULARY.append(line)\n",
        "f.close()\n",
        "print(ENG_VOCABULARY[:50])\n",
        "\n",
        "f = open(\"text-ita.txt\")\n",
        "# Define the list of all tokens in the Italian set ...\n",
        "ITA_VOCABULARY = []\n",
        "for line in f:\n",
        "    line = line.strip()\n",
        "    # Remove <sos> and <eos>\n",
        "    line = line.replace('<sos>', '').replace('<eos>', '').strip()\n",
        "    if line == \"\":\n",
        "        continue\n",
        "    ITA_VOCABULARY.append(line)\n",
        "f.close()\n",
        "print(ITA_VOCABULARY[:50])\n",
        "# Make sure that the three special tokens have the same indices in the two vocabularies.\n",
        "# Assign here the three indices...\n",
        "\n",
        "PAD = SPECIAL[0]\n",
        "\n",
        "# Inverse mappings.\n",
        "ENG_INVERSE = {w: n for n, w in enumerate(ENG_VOCABULARY)}\n",
        "ITA_INVERSE = {w: n for n, w in enumerate(ITA_VOCABULARY)}\n",
        "#print(ENG_INVERSE)\n",
        "print(len(ENG_VOCABULARY), len(ITA_VOCABULARY))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5I5NmJq4yeec",
      "metadata": {
        "id": "5I5NmJq4yeec"
      },
      "source": [
        "### Incremental approach to token vocabulary building%\n",
        "In the lesson of Deep Learning, I learned the sophisticated way of tokenizing words called WordPiece tokenization.\n",
        "\n",
        "The WordPiece tokenization algorithm builds its vocabulary incrementally, starting from a basic alphabet and iteratively merging subword units based on their frequency and co-occurrence patterns. (cited from [1]`06_Attention_and_Transformers_in_Bert.ipynb`)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### The demonstration of pretrained tokenizer\n",
        "\n",
        "In [1], Tokenizer `bert-base-cased` was used for English(For tokenization of English, it's used in this project as well). In this project, Tokenizier [3]`dbmdz/bert-base-italian-cased` for italian is used.\n",
        "[3]https://huggingface.co/dbmdz/bert-base-italian-cased  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "TOiqXhYIus4m",
      "metadata": {
        "id": "TOiqXhYIus4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94d7f4b-bf4c-4602-e498-9e7452511689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: ['This', 'is', 'the', 'Hugging', 'Face', 'Course', '.']\n",
            "English: ['This', 'chapter', 'is', 'about', 'tokenization', '.']\n",
            "English: ['This', 'section', 'shows', 'several', 'tokenizer', 'algorithms', '.']\n",
            "English: ['Hopefully', ',', 'you', 'will', 'be', 'able', 'to', 'understand', 'how', 'they', 'are', 'trained', 'and', 'generate', 'tokens', '.']\n",
            "Italian: ['Questo', 'è', 'il', 'corso', 'di', 'Hugging', 'Face', '.']\n",
            "Italian: ['Questo', 'capitolo', 'riguarda', 'la', 'tokenizzazione', '.']\n",
            "Italian: ['Questa', 'sezione', 'mostra', 'diversi', 'algoritmi', 'di', 'tokenizzazione', '.']\n",
            "Italian: ['Speriamo', 'che', 'tu', 'sia', 'in', 'grado', 'di', 'capire', 'come', 'vengono', 'addestrati', 'e', 'generano', 'token', '.']\n",
            "\n",
            "English Word Frequency: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})\n",
            "Italian Word Frequency: defaultdict(<class 'int'>, {'Questo': 2, 'è': 1, 'il': 1, 'corso': 1, 'di': 3, 'Hugging': 1, 'Face': 1, '.': 4, 'capitolo': 1, 'riguarda': 1, 'la': 1, 'tokenizzazione': 2, 'Questa': 1, 'sezione': 1, 'mostra': 1, 'diversi': 1, 'algoritmi': 1, 'Speriamo': 1, 'che': 1, 'tu': 1, 'sia': 1, 'in': 1, 'grado': 1, 'capire': 1, 'come': 1, 'vengono': 1, 'addestrati': 1, 'e': 1, 'generano': 1, 'token': 1})\n",
            "\n",
            "English vocab size: 28996\n",
            "Italian vocab size: 31102\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "### Suppress useless warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"The secret `HF_TOKEN` does not exist\")\n",
        "\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "eng_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")  # English\n",
        "ita_tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\")  # Italian\n",
        "\n",
        "### Example bilingual corpus\n",
        "eng_corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]\n",
        "\n",
        "ita_corpus = [\n",
        "    \"Questo è il corso di Hugging Face.\",\n",
        "    \"Questo capitolo riguarda la tokenizzazione.\",\n",
        "    \"Questa sezione mostra diversi algoritmi di tokenizzazione.\",\n",
        "    \"Speriamo che tu sia in grado di capire come vengono addestrati e generano token.\",\n",
        "]\n",
        "\n",
        "### Get frequency for English\n",
        "eng_word_freqs = defaultdict(int)\n",
        "for text in eng_corpus:\n",
        "    words_with_offsets = eng_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    print(f\"English: {new_words}\")\n",
        "    for word in new_words:\n",
        "        eng_word_freqs[word] += 1\n",
        "\n",
        "### Get frequency for Italian\n",
        "ita_word_freqs = defaultdict(int)\n",
        "for text in ita_corpus:\n",
        "    words_with_offsets = ita_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    print(f\"Italian: {new_words}\")\n",
        "    for word in new_words:\n",
        "        ita_word_freqs[word] += 1\n",
        "\n",
        "print(f\"\\nEnglish Word Frequency: {eng_word_freqs}\")\n",
        "print(f\"Italian Word Frequency: {ita_word_freqs}\")\n",
        "\n",
        "# Get vocabulary sizes for model initialization\n",
        "eng_vocab_size = eng_tokenizer.vocab_size\n",
        "ita_vocab_size = ita_tokenizer.vocab_size\n",
        "print(f\"\\nEnglish vocab size: {eng_vocab_size}\")\n",
        "print(f\"Italian vocab size: {ita_vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca76e653",
      "metadata": {
        "id": "ca76e653"
      },
      "source": [
        "### Tokenization Helper Functions\n",
        "\n",
        "Define helper functions to tokenize and encode text for both languages consistently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dc36b6ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc36b6ad",
        "outputId": "064e7121-b923-4275-b9a7-d632cfa856c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English encoded: {'input_ids': tensor([[ 101, 8667, 1362,  102,    0,    0],\n",
            "        [ 101, 1731, 1132, 1128,  136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1]])}\n",
            "Italian encoded: {'input_ids': tensor([[ 102, 2009, 1015,  103,    0],\n",
            "        [ 102,  804, 1628, 3098,  103]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "def tokenize_and_encode(texts, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Tokenize and encode a list of texts using the given tokenizer\n",
        "\n",
        "    Args:\n",
        "        texts: List of strings to tokenize\n",
        "        tokenizer: HuggingFace tokenizer\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with input_ids, attention_mask, etc.\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "def prepare_translation_data(eng_texts, ita_texts, eng_tokenizer, ita_tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Prepare data for translation training\n",
        "\n",
        "    Args:\n",
        "        eng_texts: List of English sentences\n",
        "        ita_texts: List of Italian sentences (parallel corpus)\n",
        "        eng_tokenizer: English tokenizer\n",
        "        ita_tokenizer: Italian tokenizer\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (eng_data, ita_data) dictionaries\n",
        "    \"\"\"\n",
        "    eng_data = tokenize_and_encode(eng_texts, eng_tokenizer, max_length)\n",
        "    ita_data = tokenize_and_encode(ita_texts, ita_tokenizer, max_length)\n",
        "\n",
        "    return eng_data, ita_data\n",
        "\n",
        "# Example usage with sample data\n",
        "sample_eng = [\"Hello world\", \"How are you?\"]\n",
        "sample_ita = [\"Ciao mondo\", \"Come stai?\"]\n",
        "\n",
        "eng_encoded, ita_encoded = prepare_translation_data(\n",
        "    sample_eng, sample_ita, eng_tokenizer, ita_tokenizer\n",
        ")\n",
        "\n",
        "print(\"English encoded:\", eng_encoded)\n",
        "print(\"Italian encoded:\", ita_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b89c0e",
      "metadata": {
        "id": "a1b89c0e"
      },
      "source": [
        "### Update Model Configuration\n",
        "\n",
        "Update the model initialization to use the proper vocabulary sizes from the tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c0ae83b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0ae83b9",
        "outputId": "a07dcaa6-c589-4b36-d145-32f6bef25202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized with:\n",
            "- English vocab size: 28996\n",
            "- Italian vocab size: 31102\n",
            "- Model parameters: 91,392,382\n"
          ]
        }
      ],
      "source": [
        "# Initialize translation model with proper vocabulary sizes\n",
        "translation_model = BertTranslationModel(\n",
        "    ita_vocab_size=ita_vocab_size,  # Italian output vocabulary\n",
        "    eng_vocab_size=eng_vocab_size,  # English input vocabulary\n",
        "    max_seq_len=512,\n",
        "    d_model=512,\n",
        "    num_layers=6,\n",
        "    num_heads=8,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "print(f\"Model initialized with:\")\n",
        "print(f\"- English vocab size: {eng_vocab_size}\")\n",
        "print(f\"- Italian vocab size: {ita_vocab_size}\")\n",
        "print(f\"- Model parameters: {sum(p.numel() for p in translation_model.parameters()):,}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}