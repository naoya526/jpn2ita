import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.utils.data import DataLoader
from Encoder import EncoderBlock, BertEmbeddings, Bert
from Decoder import CrossAttention, DecoderBlock


# main

def exp_Transformer():
    d_model = 512
    seq = 10
    batch_size = 2
    num_heads=2
    d_ff=10

    x = torch.randn(batch_size, seq, d_model)  # (batch, seq, d_model)
    print("start")
    func = EncoderBlock(d_model, num_heads, d_ff)
    #func = ShowMultiHeadAttention(d_model, num_heads)
    out = func(x)
    assert out.shape == x.shape
    return 0

def test_bert_embeddings():
    """
    BertEmbeddingsã®è»½é‡ãƒ†ã‚¹ãƒˆ
    """
    print("ğŸ§ª BertEmbeddings ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    vocab_size = 1000
    d_model = 128  # è»½é‡åŒ–ï¼ˆé€šå¸¸ã¯768ï¼‰
    max_seq_len = 64
    batch_size = 2
    seq_len = 8
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    embeddings = BertEmbeddings(
        vocab_size=vocab_size,
        d_model=d_model,
        max_seq_len=max_seq_len,
        dropout=0.1
    )
    
    print(f"ğŸ“‹ è¨­å®š:")
    print(f"  - vocab_size: {vocab_size}")
    print(f"  - d_model: {d_model}")
    print(f"  - max_seq_len: {max_seq_len}")
    print(f"  - batch_size: {batch_size}, seq_len: {seq_len}")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    token_type_ids = torch.zeros(batch_size, seq_len, dtype=torch.long)
    token_type_ids[:, seq_len//2:] = 1  # å¾ŒåŠã‚’æ–‡Bï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆ1ï¼‰ã«
    
    print(f"\nğŸ“¥ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿:")
    print(f"  input_ids: {input_ids.shape}")
    print(f"  token_type_ids: {token_type_ids.shape}")
    print(f"  input_ids[0]: {input_ids[0].tolist()}")
    print(f"  token_type_ids[0]: {token_type_ids[0].tolist()}")
    
    # Forward pass
    with torch.no_grad():
        output = embeddings(input_ids, token_type_ids)
    
    print(f"\nğŸ“¤ å‡ºåŠ›:")
    print(f"  output shape: {output.shape}")
    print(f"  output range: [{output.min():.3f}, {output.max():.3f}]")
    print(f"  output mean: {output.mean():.3f}")
    print(f"  output std: {output.std():.3f}")
    
    # å„åŸ‹ã‚è¾¼ã¿ã®å€‹åˆ¥ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ”¬ å€‹åˆ¥åŸ‹ã‚è¾¼ã¿ãƒ†ã‚¹ãƒˆ:")
    
    # Token Embeddings
    token_emb = embeddings.token(input_ids)
    print(f"  Token embeddings: {token_emb.shape}")
    
    # Position Embeddings
    position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)
    pos_emb = embeddings.position(position_ids)
    print(f"  Position embeddings: {pos_emb.shape}")
    
    # Segment Embeddings
    seg_emb = embeddings.segment(token_type_ids)
    print(f"  Segment embeddings: {seg_emb.shape}")
    
    # æ‰‹å‹•è¨ˆç®—ã¨ã®æ¯”è¼ƒ
    manual_sum = token_emb + pos_emb + seg_emb
    manual_output = embeddings.layer_norm(manual_sum)
    
    print(f"\nâœ… æ¤œè¨¼:")
    print(f"  æ‰‹å‹•è¨ˆç®—ã¨ã®å·®: {torch.abs(output - manual_output).max():.6f}")
    
    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ã®åŠ¹æœç¢ºèª
    print(f"\nğŸ¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ã®åŠ¹æœ:")
    seg_0_emb = embeddings.segment.weight[0]  # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ0ã®åŸ‹ã‚è¾¼ã¿
    seg_1_emb = embeddings.segment.weight[1]  # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ1ã®åŸ‹ã‚è¾¼ã¿
    similarity = F.cosine_similarity(seg_0_emb.unsqueeze(0), seg_1_emb.unsqueeze(0))
    print(f"  ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ0 vs 1 é¡ä¼¼åº¦: {similarity.item():.3f}")
    print(f"  (å­¦ç¿’å‰ãªã®ã§ãƒ©ãƒ³ãƒ€ãƒ å€¤)")
    
    print(f"\nğŸ‰ BertEmbeddings ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    
    return output

def test_edge_cases():
    """
    ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆ
    """
    print("\nğŸš¨ ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ")
    print("=" * 40)
    
    vocab_size = 100
    d_model = 64
    embeddings = BertEmbeddings(vocab_size, d_model, max_seq_len=32, dropout=0.0)
    
    # ãƒ†ã‚¹ãƒˆ1: æœ€å°ã‚µã‚¤ã‚º
    print("1ï¸âƒ£ æœ€å°ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆ (1x1)")
    input_ids = torch.tensor([[5]])
    output = embeddings(input_ids)
    print(f"   å‡ºåŠ›å½¢çŠ¶: {output.shape}")
    assert output.shape == (1, 1, d_model), "å½¢çŠ¶ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"
    
    # ãƒ†ã‚¹ãƒˆ2: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³
    print("2ï¸âƒ£ ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ã‚¹ãƒˆ")
    input_ids = torch.tensor([[0, 1, 2, 0, 0]])  # 0ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    output = embeddings(input_ids)
    print(f"   ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ä½ç½®ã®å‡ºåŠ›: {output[0, 0].sum():.3f}")
    
    # ãƒ†ã‚¹ãƒˆ3: token_type_idsæœªæŒ‡å®š
    print("3ï¸âƒ£ token_type_idsæœªæŒ‡å®šãƒ†ã‚¹ãƒˆ")
    input_ids = torch.tensor([[1, 2, 3, 4]])
    output1 = embeddings(input_ids)  # token_type_ids=None
    output2 = embeddings(input_ids, torch.zeros_like(input_ids))
    diff = torch.abs(output1 - output2).max()
    print(f"   å·®åˆ†: {diff:.6f}")
    assert diff < 1e-6, "token_type_ids=Noneã®å‡¦ç†ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"
    
    print("âœ… å…¨ã¦ã®ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆé€šéï¼")
def test_bert_masking():
    """
    BERTã®ãƒã‚¹ã‚­ãƒ³ã‚°å‹•ä½œã‚’ãƒ†ã‚¹ãƒˆ
    """
    print("ğŸ§ª BERT Masking ãƒ†ã‚¹ãƒˆ")
    print("=" * 40)
    
    # å°ã•ãªBERTãƒ¢ãƒ‡ãƒ«
    bert = Bert(vocab_size=1000, d_model=128, num_layers=2, num_heads=4)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: ç•°ãªã‚‹é•·ã•ã®æ–‡
    input_ids = torch.tensor([
        [101, 123, 578, 102,    0,    0],  # çŸ­ã„æ–‡
        [101, 111, 222, 333, 444, 102]   # é•·ã„æ–‡
    ])
    
    # æ‰‹å‹•ã§attention_maskã‚’ä½œæˆ
    attention_mask = torch.tensor([
        [1, 1, 1, 1, 0, 0],  # æœ€åˆã®4èªã®ã¿
        [1, 1, 1, 1, 1, 1]   # å…¨èªæœ‰åŠ¹
    ])
    
    print(f"ğŸ“¥ å…¥åŠ›:")
    print(f"  input_ids: {input_ids.shape}")
    print(f"  attention_mask: {attention_mask.shape}")
    
    # Forward pass
    with torch.no_grad():
        output = bert(input_ids, attention_mask)
    
    print(f"ğŸ“¤ å‡ºåŠ›:")
    print(f"  output: {output.shape}")
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ä½ç½®ã®å‡ºåŠ›ç¢ºèª
    padding_output = output[0, 4:, :]  # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†
    valid_output = output[0, :4, :]    # æœ‰åŠ¹éƒ¨åˆ†
    
    print(f"ğŸ“Š ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ä½ç½®ã®çµ±è¨ˆ:")
    print(f"  ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã®å¹³å‡: {padding_output.mean():.6f}")
    print(f"  æœ‰åŠ¹éƒ¨åˆ†ã®å¹³å‡: {valid_output.mean():.6f}")
    print(f"  (ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã‚‚Attentionã®å½±éŸ¿ã‚’å—ã‘ã‚‹)")

def test_cross_attention():
    """
    CrossAttentionã®å‹•ä½œã‚’ãƒ†ã‚¹ãƒˆ
    Query: English hidden states
    Key, Value: Italian hidden states
    """
    print("ğŸ§ª CrossAttention ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    batch_size = 2
    eng_len = 6  # è‹±èªã®ç³»åˆ—é•·
    ita_len = 8  # ã‚¤ã‚¿ãƒªã‚¢èªã®ç³»åˆ—é•·
    d_model = 128
    num_heads = 8
    
    print(f"ğŸ“‹ è¨­å®š:")
    print(f"  batch_size: {batch_size}")
    print(f"  english_len: {eng_len}, italian_len: {ita_len}")
    print(f"  d_model: {d_model}, num_heads: {num_heads}")
    
    # CrossAttentionãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    cross_attention = CrossAttention(d_model, num_heads, dropout=0.1)
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    # è‹±èªã®éš ã‚ŒçŠ¶æ…‹ï¼ˆQueryç”¨ï¼‰
    english_hidden = torch.randn(batch_size, eng_len, d_model)
    # ã‚¤ã‚¿ãƒªã‚¢èªã®éš ã‚ŒçŠ¶æ…‹ï¼ˆKey, Valueç”¨ï¼‰
    italian_hidden = torch.randn(batch_size, ita_len, d_model)
    
    print(f"\nğŸ“¥ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿:")
    print(f"  english_hidden: {english_hidden.shape}")
    print(f"  italian_hidden: {italian_hidden.shape}")
    print(f"  english_mean: {english_hidden.mean():.4f}")
    print(f"  italian_mean: {italian_hidden.mean():.4f}")
    
    # ãƒã‚¹ã‚¯ä½œæˆï¼ˆã‚¤ã‚¿ãƒªã‚¢èªã®ä¸€éƒ¨ã‚’ãƒã‚¹ã‚¯ï¼‰
    italian_mask = torch.ones(batch_size, ita_len)
    italian_mask[0, -2:] = 0  # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã®æœ€å¾Œ2èªã‚’ãƒã‚¹ã‚¯
    italian_mask[1, -3:] = 0  # 2ç•ªç›®ã®ã‚µãƒ³ãƒ—ãƒ«ã®æœ€å¾Œ3èªã‚’ãƒã‚¹ã‚¯
    
    # ãƒã‚¹ã‚¯å½¢çŠ¶å¤‰æ›: (batch, ita_len) â†’ (batch, 1, 1, ita_len)
    extended_mask = italian_mask.unsqueeze(1).unsqueeze(2)
    extended_mask = (1.0 - extended_mask) * -1e9
    
    print(f"\nğŸ­ ãƒã‚¹ã‚¯æƒ…å ±:")
    print(f"  italian_mask[0]: {italian_mask[0].tolist()}")
    print(f"  italian_mask[1]: {italian_mask[1].tolist()}")
    print(f"  extended_mask.shape: {extended_mask.shape}")
    
    # Forward pass
    print(f"\nğŸš€ CrossAttentionå®Ÿè¡Œ...")
    with torch.no_grad():
        output = cross_attention(english_hidden, italian_hidden, extended_mask)
    
    print(f"\nğŸ“¤ å‡ºåŠ›çµæœ:")
    print(f"  output.shape: {output.shape}")
    print(f"  output.mean: {output.mean():.4f}")
    print(f"  output.std: {output.std():.4f}")
    print(f"  output.min: {output.min():.4f}")
    print(f"  output.max: {output.max():.4f}")
    
    # å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
    expected_shape = (batch_size, eng_len, d_model)
    assert output.shape == expected_shape, f"å½¢çŠ¶ã‚¨ãƒ©ãƒ¼: {output.shape} != {expected_shape}"
    
    # NaN/Inf ãƒã‚§ãƒƒã‚¯
    assert not torch.isnan(output).any(), "NaNãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ"
    assert not torch.isinf(output).any(), "InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ"
    
    print(f"\nâœ… åŸºæœ¬ãƒã‚§ãƒƒã‚¯é€šé!")
    
    # è©³ç´°æ¤œè¨¼: ãƒã‚¹ã‚¯ã®åŠ¹æœç¢ºèª
    print(f"\nğŸ” è©³ç´°æ¤œè¨¼:")
    
    # ãƒã‚¹ã‚¯ãªã—ã§ã®å®Ÿè¡Œ
    output_no_mask = cross_attention(english_hidden, italian_hidden, mask=None)
    
    # å·®åˆ†ç¢ºèª
    diff = torch.abs(output - output_no_mask).mean()
    print(f"  ãƒã‚¹ã‚¯ã‚ã‚Š vs ãªã— ã®å·®åˆ†: {diff:.6f}")
    
    if diff > 1e-6:
        print(f"  âœ… ãƒã‚¹ã‚¯ãŒæ­£ã—ãé©ç”¨ã•ã‚Œã¦ã„ã¾ã™")
    else:
        print(f"  âš ï¸  ãƒã‚¹ã‚¯ã®åŠ¹æœãŒå°ã•ã„ã§ã™")
    
    # Attentioné‡ã¿ã®ç¢ºèªï¼ˆå†…éƒ¨å®Ÿè£…ã‚’ãƒ†ã‚¹ãƒˆï¼‰
    print(f"\nğŸ¯ å†…éƒ¨å‹•ä½œç¢ºèª:")
    
    # Query, Key, Value ã®ç”Ÿæˆã‚’ãƒ†ã‚¹ãƒˆ
    query = cross_attention.query(english_hidden)
    key = cross_attention.key(italian_hidden)
    value = cross_attention.value(italian_hidden)
    
    print(f"  query.shape: {query.shape}")
    print(f"  key.shape: {key.shape}")
    print(f"  value.shape: {value.shape}")
    
    # å„ãƒ˜ãƒƒãƒ‰ã§ã®å‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆ
    query_heads = query.view(batch_size, eng_len, num_heads, d_model // num_heads)
    key_heads = key.view(batch_size, ita_len, num_heads, d_model // num_heads)
    
    print(f"  query_heads.shape: {query_heads.shape}")
    print(f"  key_heads.shape: {key_heads.shape}")
    
    # Attention scoresã®è¨ˆç®—ãƒ†ã‚¹ãƒˆ
    query_heads = query_heads.permute(0, 2, 1, 3)  # (batch, heads, eng_len, head_dim)
    key_heads = key_heads.permute(0, 2, 1, 3)      # (batch, heads, ita_len, head_dim)
    
    scores = torch.matmul(query_heads, key_heads.transpose(-2, -1))
    print(f"  attention_scores.shape: {scores.shape}")
    print(f"  expected: ({batch_size}, {num_heads}, {eng_len}, {ita_len})")
    
    # Softmaxå¾Œã®é‡ã¿ã®ç¢ºèª
    weights = F.softmax(scores, dim=-1)
    print(f"  attention_weights sum: {weights.sum(dim=-1).mean():.4f} (should be ~1.0)")
    
    print(f"\nğŸ‰ CrossAttention ãƒ†ã‚¹ãƒˆå®Œäº†!")
    return output

def test_cross_attention_edge_cases():
    """
    CrossAttentionã®ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
    """
    print("\nğŸš¨ CrossAttention ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ")
    print("=" * 40)
    
    d_model = 64
    num_heads = 4
    cross_attention = CrossAttention(d_model, num_heads, dropout=0.0)
    
    # ãƒ†ã‚¹ãƒˆ1: æœ€å°ã‚µã‚¤ã‚º
    print("1ï¸âƒ£ æœ€å°ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆ (1x1)")
    english = torch.randn(1, 1, d_model)
    italian = torch.randn(1, 1, d_model)
    output = cross_attention(english, italian)
    assert output.shape == (1, 1, d_model), "æœ€å°ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆå¤±æ•—"
    print("   âœ… é€šé")
    
    # ãƒ†ã‚¹ãƒˆ2: ç•°ãªã‚‹ç³»åˆ—é•·
    print("2ï¸âƒ£ ç•°ãªã‚‹ç³»åˆ—é•·ãƒ†ã‚¹ãƒˆ")
    english = torch.randn(2, 3, d_model)  # è‹±èª: 3èª
    italian = torch.randn(2, 7, d_model)  # ã‚¤ã‚¿ãƒªã‚¢èª: 7èª
    output = cross_attention(english, italian)
    assert output.shape == (2, 3, d_model), "ç•°ãªã‚‹ç³»åˆ—é•·ãƒ†ã‚¹ãƒˆå¤±æ•—"
    print("   âœ… é€šé")
    
    # ãƒ†ã‚¹ãƒˆ3: å®Œå…¨ãƒã‚¹ã‚¯
    print("3ï¸âƒ£ å®Œå…¨ãƒã‚¹ã‚¯ãƒ†ã‚¹ãƒˆ")
    english = torch.randn(1, 4, d_model)
    italian = torch.randn(1, 4, d_model)
    full_mask = torch.full((1, 1, 1, 4), -1e9)  # å…¨ã¦ãƒã‚¹ã‚¯
    output = cross_attention(english, italian, full_mask)
    assert not torch.isnan(output).any(), "å®Œå…¨ãƒã‚¹ã‚¯ã§NaNç™ºç”Ÿ"
    print("   âœ… é€šé")
    
    # ãƒ†ã‚¹ãƒˆ4: å‹¾é…ãƒ•ãƒ­ãƒ¼ç¢ºèª
    print("4ï¸âƒ£ å‹¾é…ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ")
    english = torch.randn(1, 2, d_model, requires_grad=True)
    italian = torch.randn(1, 3, d_model, requires_grad=True)
    output = cross_attention(english, italian)
    loss = output.sum()
    loss.backward()
    
    assert english.grad is not None, "è‹±èªã®å‹¾é…ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã›ã‚“"
    assert italian.grad is not None, "ã‚¤ã‚¿ãƒªã‚¢èªã®å‹¾é…ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã›ã‚“"
    print("   âœ… é€šé")
    
    print("âœ… å…¨ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆé€šé!")

def test_decoder_block():
    """
    DecoderBlockã®å‹•ä½œã‚’ãƒ†ã‚¹ãƒˆ
    Self-Attention + Cross-Attention + Feed Forward ã®çµ±åˆãƒ†ã‚¹ãƒˆ
    """
    print("ğŸ§ª DecoderBlock ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    batch_size = 2
    eng_len = 6    # è‹±èªï¼ˆDecoderå…¥åŠ›ï¼‰ã®ç³»åˆ—é•·
    ita_len = 8    # ã‚¤ã‚¿ãƒªã‚¢èªï¼ˆEncoderå‡ºåŠ›ï¼‰ã®ç³»åˆ—é•·
    d_model = 128
    num_heads = 8
    d_ff = 512
    
    print(f"ğŸ“‹ è¨­å®š:")
    print(f"  batch_size: {batch_size}")
    print(f"  english_len: {eng_len}, italian_len: {ita_len}")
    print(f"  d_model: {d_model}, num_heads: {num_heads}, d_ff: {d_ff}")
    
    # DecoderBlockåˆæœŸåŒ–
    decoder_block = DecoderBlock(d_model, num_heads, d_ff, dropout=0.1)
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    english_hidden = torch.randn(batch_size, eng_len, d_model)    # Decoderå…¥åŠ›
    italian_encoder = torch.randn(batch_size, ita_len, d_model)   # Encoderå‡ºåŠ›
    
    print(f"\nğŸ“¥ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿:")
    print(f"  english_hidden: {english_hidden.shape}")
    print(f"  italian_encoder: {italian_encoder.shape}")
    print(f"  english_mean: {english_hidden.mean():.4f}")
    print(f"  italian_mean: {italian_encoder.mean():.4f}")
    
    # ãƒã‚¹ã‚¯ä½œæˆ
    # Self-attentionç”¨Causal Maskï¼ˆæœªæ¥ã‚’è¦‹ãªã„ã‚ˆã†ã«ï¼‰
    causal_mask = torch.triu(torch.ones(eng_len, eng_len), diagonal=1)
    causal_mask = causal_mask.masked_fill(causal_mask == 1, -1e9)
    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, eng_len, eng_len)
    
    # Cross-attentionç”¨ãƒã‚¹ã‚¯ï¼ˆã‚¤ã‚¿ãƒªã‚¢èªã®ä¸€éƒ¨ã‚’ãƒã‚¹ã‚¯ï¼‰
    cross_mask = torch.ones(batch_size, ita_len)
    cross_mask[0, -2:] = 0  # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã®æœ€å¾Œ2èªã‚’ãƒã‚¹ã‚¯
    cross_mask[1, -3:] = 0  # 2ç•ªç›®ã®ã‚µãƒ³ãƒ—ãƒ«ã®æœ€å¾Œ3èªã‚’ãƒã‚¹ã‚¯
    cross_mask = cross_mask.unsqueeze(1).unsqueeze(2)
    cross_mask = (1.0 - cross_mask) * -1e9
    
    print(f"\nğŸ­ ãƒã‚¹ã‚¯æƒ…å ±:")
    print(f"  causal_mask.shape: {causal_mask.shape}")
    print(f"  cross_mask.shape: {cross_mask.shape}")
    print(f"  cross_mask[0,0,0]: {cross_mask[0,0,0].tolist()}")
    
    # Forward pass
    print(f"\nğŸš€ DecoderBlockå®Ÿè¡Œ...")
    with torch.no_grad():
        output = decoder_block(
            x=english_hidden,
            encoder_output=italian_encoder,
            self_mask=causal_mask,
            cross_mask=cross_mask
        )
    
    print(f"\nğŸ“¤ å‡ºåŠ›çµæœ:")
    print(f"  output.shape: {output.shape}")
    print(f"  output.mean: {output.mean():.4f}")
    print(f"  output.std: {output.std():.4f}")
    print(f"  output.min: {output.min():.4f}")
    print(f"  output.max: {output.max():.4f}")
    
    # å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
    expected_shape = (batch_size, eng_len, d_model)
    assert output.shape == expected_shape, f"å½¢çŠ¶ã‚¨ãƒ©ãƒ¼: {output.shape} != {expected_shape}"
    
    # NaN/Inf ãƒã‚§ãƒƒã‚¯
    assert not torch.isnan(output).any(), "NaNãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ"
    assert not torch.isinf(output).any(), "InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ"
    
    print(f"\nâœ… åŸºæœ¬ãƒã‚§ãƒƒã‚¯é€šé!")
    
    # æ®µéšåˆ¥å‹•ä½œç¢ºèª
    test_decoder_step_by_step(decoder_block, english_hidden, italian_encoder, causal_mask, cross_mask)
    
    print(f"\nğŸ‰ DecoderBlock ãƒ†ã‚¹ãƒˆå®Œäº†!")
    return output

def test_decoder_step_by_step(decoder_block, english_hidden, italian_encoder, causal_mask, cross_mask):
    """
    DecoderBlockã®å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’è©³ç´°ç¢ºèª
    """
    print(f"\nğŸ” æ®µéšåˆ¥å‹•ä½œç¢ºèª:")
    
    with torch.no_grad():
        x = english_hidden.clone()
        
        # Step 1: Self-Attention
        residual = x
        x_norm = decoder_block.layer_norm1(x)
        x_self_attn = decoder_block.self_attention(x_norm, mask=causal_mask)
        x = decoder_block.dropout(x_self_attn) + residual
        
        print(f"  Step 1 (Self-Attention):")
        print(f"    input_mean: {english_hidden.mean():.4f}")
        print(f"    output_mean: {x.mean():.4f}")
        print(f"    change: {torch.abs(x - english_hidden).mean():.4f}")
        
        # Step 2: Cross-Attention
        residual = x
        x_norm = decoder_block.layer_norm2(x)
        x_cross_attn = decoder_block.cross_attention(
            query_input=x_norm,
            key_value_input=italian_encoder,
            mask=cross_mask
        )
        x = decoder_block.dropout(x_cross_attn) + residual
        
        print(f"  Step 2 (Cross-Attention):")
        print(f"    output_mean: {x.mean():.4f}")
        print(f"    italian_influence: {torch.abs(x_cross_attn).mean():.4f}")
        
        # Step 3: Feed Forward
        residual = x
        x_norm = decoder_block.layer_norm3(x)
        x_ffn = decoder_block.ffn(x_norm)
        x = decoder_block.dropout(x_ffn) + residual
        
        print(f"  Step 3 (Feed Forward):")
        print(f"    output_mean: {x.mean():.4f}")
        print(f"    ffn_change: {torch.abs(x_ffn).mean():.4f}")
        
        # æœ€çµ‚çµæœã¨ã®æ¯”è¼ƒ
        full_output = decoder_block(english_hidden, italian_encoder, causal_mask, cross_mask)
        diff = torch.abs(x - full_output).max()
        print(f"  âœ… æ®µéšåˆ¥ vs ä¸€æ‹¬å®Ÿè¡Œã®å·®åˆ†: {diff:.8f}")

def test_decoder_masking_effects():
    """
    DecoderBlockã§ã®ãƒã‚¹ã‚¯ã®åŠ¹æœã‚’ç¢ºèª
    """
    print(f"\nğŸ­ ãƒã‚¹ã‚¯åŠ¹æœãƒ†ã‚¹ãƒˆ:")
    
    batch_size, eng_len, ita_len, d_model = 1, 4, 6, 64
    decoder_block = DecoderBlock(d_model, 4, 256, dropout=0.0)
    
    english_hidden = torch.randn(batch_size, eng_len, d_model)
    italian_encoder = torch.randn(batch_size, ita_len, d_model)
    
    # ãƒ†ã‚¹ãƒˆ1: ãƒã‚¹ã‚¯ãªã—
    output_no_mask = decoder_block(english_hidden, italian_encoder)
    
    # ãƒ†ã‚¹ãƒˆ2: Causal Maskã®ã¿
    causal_mask = torch.triu(torch.ones(eng_len, eng_len), diagonal=1)
    causal_mask = causal_mask.masked_fill(causal_mask == 1, -1e9)
    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)
    
    output_causal = decoder_block(english_hidden, italian_encoder, self_mask=causal_mask)
    
    # ãƒ†ã‚¹ãƒˆ3: Cross Maskã®ã¿
    cross_mask = torch.zeros(batch_size, 1, 1, ita_len)
    cross_mask[0, 0, 0, -2:] = -1e9  # æœ€å¾Œ2èªã‚’ãƒã‚¹ã‚¯
    
    output_cross = decoder_block(english_hidden, italian_encoder, cross_mask=cross_mask)
    
    # ãƒ†ã‚¹ãƒˆ4: ä¸¡æ–¹ã®ãƒã‚¹ã‚¯
    output_both = decoder_block(english_hidden, italian_encoder, causal_mask, cross_mask)
    
    print(f"  ãƒã‚¹ã‚¯ãªã— vs Causal: {torch.abs(output_no_mask - output_causal).mean():.6f}")
    print(f"  ãƒã‚¹ã‚¯ãªã— vs Cross: {torch.abs(output_no_mask - output_cross).mean():.6f}")
    print(f"  ãƒã‚¹ã‚¯ãªã— vs ä¸¡æ–¹: {torch.abs(output_no_mask - output_both).mean():.6f}")

def test_decoder_edge_cases():
    """
    DecoderBlockã®ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
    """
    print(f"\nğŸš¨ DecoderBlock ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ:")
    
    d_model = 64
    decoder_block = DecoderBlock(d_model, 4, 256, dropout=0.0)
    
    # ãƒ†ã‚¹ãƒˆ1: æœ€å°ã‚µã‚¤ã‚º
    print("1ï¸âƒ£ æœ€å°ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆ")
    english = torch.randn(1, 1, d_model)
    italian = torch.randn(1, 1, d_model)
    output = decoder_block(english, italian)
    assert output.shape == (1, 1, d_model), "æœ€å°ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆå¤±æ•—"
    print("   âœ… é€šé")
    
    # ãƒ†ã‚¹ãƒˆ2: å‹¾é…ãƒ•ãƒ­ãƒ¼
    print("2ï¸âƒ£ å‹¾é…ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ")
    english = torch.randn(1, 3, d_model, requires_grad=True)
    italian = torch.randn(1, 4, d_model, requires_grad=True)
    output = decoder_block(english, italian)
    loss = output.sum()
    loss.backward()
    assert english.grad is not None and italian.grad is not None, "å‹¾é…è¨ˆç®—å¤±æ•—"
    print("   âœ… é€šé")
    
    print("âœ… å…¨ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆé€šé!")

def main():
    """
    DecoderBlockç·åˆãƒ†ã‚¹ãƒˆ
    """
    print("ğŸš€ DecoderBlock ç·åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    try:
        # åŸºæœ¬ãƒ†ã‚¹ãƒˆ
        output = test_decoder_block()
        
        # ãƒã‚¹ã‚¯åŠ¹æœãƒ†ã‚¹ãƒˆ
        test_decoder_masking_effects()
        
        # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
        test_decoder_edge_cases()
        
        print(f"\nğŸ† å…¨ãƒ†ã‚¹ãƒˆå®Œäº†! DecoderBlockã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
        
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

"""
def main():
    #test_bert_embeddings()
    #test_edge_cases()
    #test_bert_masking()
    print("ğŸš€ CrossAttention ç·åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    try:
        # åŸºæœ¬ãƒ†ã‚¹ãƒˆ
        output = test_cross_attention()
        
        # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
        test_cross_attention_edge_cases()
        
        print(f"\nğŸ† å…¨ãƒ†ã‚¹ãƒˆå®Œäº†! CrossAttentionã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
        
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

"""



def visualize_tensor_transformation():
    """
    ãƒ†ãƒ³ã‚½ãƒ«å¤‰å½¢ã®å¯è¦–åŒ–
    """
    print("ğŸ” Multi-Head Attention ã®ãƒ†ãƒ³ã‚½ãƒ«å¤‰å½¢ã‚’å¯è¦–åŒ–")
    print("=" * 60)
    
    # ä¾‹: å°ã•ãªã‚µã‚¤ã‚ºã§å®Ÿé¨“
    batch_size, seq_len, d_model = 2, 4, 8
    num_heads, head_dim = 2, 4  # d_model = num_heads * head_dim
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"å…¥åŠ› x: {x.shape}")
    
    # ç·šå½¢å¤‰æ›ï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚ã€æ’ç­‰å¤‰æ›ï¼‰
    query = x
    print(f"Q (ç·šå½¢å¤‰æ›å¾Œ): {query.shape}")
    
    # Step 1: viewæ“ä½œ
    query_reshaped = query.view(batch_size, seq_len, num_heads, head_dim)
    print(f"Q (viewå¾Œ): {query_reshaped.shape}")
    print("â†’ d_modelã‚’num_headså€‹ã®head_dimã«åˆ†å‰²")
    
    # Step 2: permuteæ“ä½œ  
    query_permuted = query_reshaped.permute(0, 2, 1, 3)
    print(f"Q (permuteå¾Œ): {query_permuted.shape}")
    print("â†’ ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒã‚’å‰ã«ç§»å‹•ï¼ˆä¸¦åˆ—å‡¦ç†ã®ãŸã‚ï¼‰")
    
    # å®Ÿéš›ã®è¨ˆç®—ä¾‹
    key_permuted = query_permuted  # ç°¡ç•¥åŒ–
    
    # è¡Œåˆ—ç©ã®å®Ÿè¡Œ
    scores = torch.matmul(query_permuted, key_permuted.transpose(-2, -1))
    print(f"scores: {scores.shape}")
    print("â†’ (batch, num_heads, seq_len, seq_len) ã®attention matrix")
    
    print("\nğŸ’¡ é‡è¦ãƒã‚¤ãƒ³ãƒˆ:")
    print("- permuteå¾Œã¯ (batch, num_heads, seq_len, head_dim)")
    print("- æœ€å¾Œã®2æ¬¡å…ƒ (seq_len, head_dim) ã§è¡Œåˆ—ç©")
    print("- num_headsåˆ†ã‚’ä¸¦åˆ—ã§è¨ˆç®—ï¼")

# å®Ÿè¡Œä¾‹ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„ï¼‰
# visualize_tensor_transformation()

def compare_ffn_activations():
    """
    æœ€å¾Œã®æ´»æ€§åŒ–ã‚ã‚Šãªã—ã®æ¯”è¼ƒå®Ÿé¨“
    """
    print("ğŸ§ª FFNæœ€çµ‚å±¤ã®æ´»æ€§åŒ–é–¢æ•°æ¯”è¼ƒå®Ÿé¨“")
    print("=" * 50)
    
    d_model, d_ff, batch_size, seq_len = 512, 2048, 2, 10
    x = torch.randn(batch_size, seq_len, d_model)
    
    # 1. æ¨™æº–çš„ãªFFNï¼ˆæœ€å¾Œã«æ´»æ€§åŒ–ãªã—ï¼‰
    class StandardFFN(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear1 = nn.Linear(d_model, d_ff)
            self.linear2 = nn.Linear(d_ff, d_model)
            self.gelu = nn.GELU()
            
        def forward(self, x):
            x = self.linear1(x)
            x = self.gelu(x)        # ä¸­é–“å±¤ã®ã¿æ´»æ€§åŒ–
            x = self.linear2(x)     # æœ€å¾Œã¯ç·šå½¢
            return x
    
    # 2. æœ€å¾Œã«ã‚‚æ´»æ€§åŒ–ã‚’å…¥ã‚ŒãŸFFNï¼ˆå®Ÿé¨“ç”¨ï¼‰
    class ActivatedFFN(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear1 = nn.Linear(d_model, d_ff)
            self.linear2 = nn.Linear(d_ff, d_model)
            self.gelu = nn.GELU()
            
        def forward(self, x):
            x = self.linear1(x)
            x = self.gelu(x)        # ä¸­é–“å±¤æ´»æ€§åŒ–
            x = self.linear2(x)
            x = self.gelu(x)        # æœ€å¾Œã«ã‚‚æ´»æ€§åŒ–ï¼ˆå®Ÿé¨“ï¼‰
            return x
    
    standard_ffn = StandardFFN()
    activated_ffn = ActivatedFFN()
    
    # å‡ºåŠ›ã®æ¯”è¼ƒ
    with torch.no_grad():
        standard_out = standard_ffn(x)
        activated_out = activated_ffn(x)
        
        print(f"å…¥åŠ›ã®ç¯„å›²: [{x.min():.3f}, {x.max():.3f}]")
        print(f"æ¨™æº–FFNå‡ºåŠ›: [{standard_out.min():.3f}, {standard_out.max():.3f}]")
        print(f"æ´»æ€§åŒ–FFNå‡ºåŠ›: [{activated_out.min():.3f}, {activated_out.max():.3f}]")
        
        # è² ã®å€¤ã®å‰²åˆ
        standard_neg_ratio = (standard_out < 0).float().mean()
        activated_neg_ratio = (activated_out < 0).float().mean()
        
        print(f"\nğŸ“Š è² ã®å€¤ã®å‰²åˆ:")
        print(f"æ¨™æº–FFN: {standard_neg_ratio:.1%}")
        print(f"æ´»æ€§åŒ–FFN: {activated_neg_ratio:.1%}")
        
        print(f"\nğŸ’¡ çµè«–:")
        print(f"- æ¨™æº–FFNã¯æ­£è² ä¸¡æ–¹ã®å€¤ã‚’å‡ºåŠ›ï¼ˆè¡¨ç¾åŠ›ãŒé«˜ã„ï¼‰")
        print(f"- æ´»æ€§åŒ–FFNã¯æ­£ã®å€¤ã®ã¿ï¼ˆè¡¨ç¾åŠ›ãŒåˆ¶é™ã•ã‚Œã‚‹ï¼‰")

# å®Ÿè¡Œä¾‹ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„ï¼‰
# compare_ffn_activations()

# ğŸ“š ä»–ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã®æ¯”è¼ƒ
"""
ğŸ” ä»–ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã®é•ã„:

1. **CNN (ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ)**
   - å„å±¤ã§ReLUæ´»æ€§åŒ–ãŒä¸€èˆ¬çš„
   - ç‰¹å¾´æŠ½å‡ºãŒç›®çš„

2. **RNN/LSTM**
   - éš ã‚ŒçŠ¶æ…‹ã§tanh/sigmoidã‚’ä½¿ç”¨
   - ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ã§åˆ¶å¾¡

3. **Transformer/BERT**
   - ä¸­é–“å±¤ã®ã¿GELUæ´»æ€§åŒ–
   - æœ€çµ‚å±¤ã¯ç·šå½¢ï¼ˆResidual + LayerNormï¼‰

4. **MLP (å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³)**
   - é€šå¸¸ã¯å„å±¤ã§æ´»æ€§åŒ–
   - åˆ†é¡å•é¡Œã§ã¯æœ€å¾Œã«Softmax

ğŸ¯ TransformerãŒç‰¹æ®Šãªç†ç”±:
- Residual Connection ã®å­˜åœ¨
- LayerNormalization ã®å¾Œå‡¦ç†
- Attentionæ©Ÿæ§‹ã¨ã®å”èª¿
"""
