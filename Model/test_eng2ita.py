import torch
import torch.nn.functional as F
from Decoder import BertTranslationModel

def create_causal_mask(seq_len, batch_size=1):
    """
    âœ… ä¿®æ­£: Causal maskã‚’æ­£ã—ã„å½¢çŠ¶ã§ä½œæˆ
    """
    # (seq_len, seq_len) ã®ãƒã‚¹ã‚¯ã‚’ä½œæˆ
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    mask = mask.masked_fill(mask == 1, -1e9)
    
    # âœ… ä¿®æ­£: repeatã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ 
    mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)
    mask = mask.repeat(batch_size, 1, 1, 1)  # expand â†’ repeat
    
    print(f"  ğŸ” Causal maskå½¢çŠ¶: {mask.shape}")
    return mask

def create_padding_mask(ids):
    """
    âœ… ä¿®æ­£: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯ã‚’æ­£ã—ã„å½¢çŠ¶ã§ä½œæˆ
    """
    batch_size, seq_len = ids.shape
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ä½ç½®ã‚’ç‰¹å®š (0 = ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°)
    mask = (ids != 0).float()  # (batch, seq_len)
    
    # âœ… ä¿®æ­£: æ­£ç¢ºãªå½¢çŠ¶å¤‰æ›
    mask = mask.unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)
    mask = (1.0 - mask) * -1e9
    
    print(f"  ğŸ” Padding maskå½¢çŠ¶: {mask.shape}")
    print(f"  ğŸ” ãƒã‚¹ã‚¯ç¯„å›²: [{mask.min():.1f}, {mask.max():.1f}]")
    return mask

def create_simple_masks(batch_size, seq_len):
    """
    âœ… æ–°è¦: ç°¡å˜ãªãƒã‚¹ã‚¯ä½œæˆï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    """
    # å…¨ã¦æœ‰åŠ¹ãªãƒã‚¹ã‚¯ï¼ˆãƒã‚¹ã‚¯ãªã—çŠ¶æ…‹ï¼‰
    mask = torch.zeros(batch_size, 1, 1, seq_len)
    return mask

def test_translation_model():
    """
    ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰
    """
    print("ğŸ§ª ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 40)
    
    # âœ… ä¿®æ­£: ã‚ˆã‚Šå°ã•ãªãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
    model = BertTranslationModel(
        ita_vocab_size=1000,   # å°ã•ãã—ã¦å®‰å…¨ã«
        eng_vocab_size=1200,   # å°ã•ãã—ã¦å®‰å…¨ã«
        max_seq_len=64,        # å°ã•ãã—ã¦å®‰å…¨ã«
        d_model=128,           # å°ã•ãã—ã¦é«˜é€ŸåŒ–
        num_layers=2,          # å°ã•ãã—ã¦é«˜é€ŸåŒ–
        num_heads=4            # å°ã•ãã—ã¦é«˜é€ŸåŒ–
    )
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    batch_size = 2
    eng_len, ita_len = 6, 4  # âœ… ä¿®æ­£: ã‚ˆã‚Šå°ã•ãªã‚µã‚¤ã‚º
    
    print(f"ğŸ“‹ è¨­å®š:")
    print(f"  batch_size: {batch_size}")
    print(f"  english_len: {eng_len}, italian_len: {ita_len}")
    print(f"  model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # è‹±èªå…¥åŠ›ï¼ˆEncoderç”¨ï¼‰
    eng_ids = torch.randint(1, 999, (batch_size, eng_len))  # âœ… å®‰å…¨ãªç¯„å›²
    eng_ids[:, 0] = 101   # [CLS]
    eng_ids[:, -1] = 102  # [SEP]
    
    # ã‚¤ã‚¿ãƒªã‚¢èªå…¥åŠ›ï¼ˆDecoderç”¨ - Teacher Forcingï¼‰
    ita_ids = torch.randint(1, 999, (batch_size, ita_len))  # âœ… å®‰å…¨ãªç¯„å›²
    ita_ids[:, 0] = 101   # [CLS]
    ita_ids[:, -1] = 102  # [SEP]
    
    print(f"ğŸ“¥ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿:")
    print(f"  eng_ids: {eng_ids.shape}, range: [{eng_ids.min()}, {eng_ids.max()}]")
    print(f"  ita_ids: {ita_ids.shape}, range: [{ita_ids.min()}, {ita_ids.max()}]")
    print(f"  eng_ids[0]: {eng_ids[0].tolist()}")
    print(f"  ita_ids[0]: {ita_ids[0].tolist()}")
    
    # âœ… ãƒã‚¹ã‚¯ä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰
    print(f"\nğŸ­ ãƒã‚¹ã‚¯ä½œæˆ:")
    try:
        eng_mask = create_padding_mask(eng_ids)        # è‹±èªã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
        ita_mask = create_causal_mask(ita_len, batch_size)  # ã‚¤ã‚¿ãƒªã‚¢èªã®Causalãƒã‚¹ã‚¯
        
        print(f"  eng_mask: {eng_mask.shape}")
        print(f"  ita_mask: {ita_mask.shape}")
        
        # âœ… å½¢çŠ¶ãŒæ­£ã—ã„ã‹ãƒã‚§ãƒƒã‚¯
        assert eng_mask.dim() == 4, f"è‹±èªãƒã‚¹ã‚¯ã®æ¬¡å…ƒãŒé–“é•ã„: {eng_mask.dim()} != 4"
        assert ita_mask.dim() == 4, f"ã‚¤ã‚¿ãƒªã‚¢èªãƒã‚¹ã‚¯ã®æ¬¡å…ƒãŒé–“é•ã„: {ita_mask.dim()} != 4"
        
    except Exception as e:
        print(f"âš ï¸  ãƒã‚¹ã‚¯ä½œæˆã§ã‚¨ãƒ©ãƒ¼: {e}")
        print("  â†’ ç°¡å˜ãªãƒã‚¹ã‚¯ã‚’ä½¿ç”¨ã—ã¾ã™")
        eng_mask = create_simple_masks(batch_size, eng_len)
        ita_mask = create_simple_masks(batch_size, ita_len)
    
    # Forward pass
    print(f"\nğŸš€ Forward passå®Ÿè¡Œ...")
    try:
        with torch.no_grad():
            # âœ… token_type_idsã‚’å‰Šé™¤ã—ã¦ã‚·ãƒ³ãƒ—ãƒ«ã«
            logits = model(
                eng_ids=eng_ids,
                ita_ids=ita_ids,
                eng_mask=eng_mask,
                ita_mask=ita_mask,
                eng_token_type_ids=None,  # âœ… Noneã§ç°¡å˜ã«
                ita_token_type_ids=None,  # âœ… Noneã§ç°¡å˜ã«
            )
        
        print(f"ğŸ“¤ å‡ºåŠ›çµæœ:")
        print(f"  logits.shape: {logits.shape}")
        print(f"  æœŸå¾…ã™ã‚‹å½¢çŠ¶: ({batch_size}, {ita_len}, 1000)")
        print(f"  logitsçµ±è¨ˆ: mean={logits.mean():.4f}, std={logits.std():.4f}")
        
        # å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
        expected_shape = (batch_size, ita_len, 1000)
        assert logits.shape == expected_shape, f"å½¢çŠ¶ã‚¨ãƒ©ãƒ¼: {logits.shape} != {expected_shape}"
        
        # NaN/Infãƒã‚§ãƒƒã‚¯
        assert not torch.isnan(logits).any(), "NaNãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ"
        assert not torch.isinf(logits).any(), "InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ"
        
        print(f"âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ! ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
        
        # âœ… äºˆæ¸¬çµæœã®ç¢ºèª
        predicted_tokens = torch.argmax(logits, dim=-1)
        print(f"\nğŸ¯ äºˆæ¸¬çµæœ:")
        print(f"  predicted_tokens[0]: {predicted_tokens[0].tolist()}")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

def test_step_by_step():
    """
    æ®µéšåˆ¥ãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰
    """
    print(f"\nğŸ” æ®µéšåˆ¥ãƒ†ã‚¹ãƒˆ:")
    
    model = BertTranslationModel(
        ita_vocab_size=5000,
        eng_vocab_size=6000,
        max_seq_len=32,
        d_model=64,
        num_layers=1,
        num_heads=2
    )
    
    # âœ… ä¿®æ­£: éå¸¸ã«å°ã•ãªãƒ‡ãƒ¼ã‚¿
    eng_ids = torch.tensor([[101, 123, 456, 102]])  # (1, 4)
    ita_ids = torch.tensor([[101, 789, 102, 0]])    # (1, 4)
    
    print(f"  Encoderå…¥åŠ›: {eng_ids.shape} = {eng_ids[0].tolist()}")
    print(f"  Decoderå…¥åŠ›: {ita_ids.shape} = {ita_ids[0].tolist()}")
    
    # âœ… ãƒã‚¹ã‚¯ä½œæˆï¼ˆç°¡å˜ç‰ˆï¼‰
    eng_mask = create_simple_masks(1, 4)
    ita_mask = create_simple_masks(1, 4)
    
    print(f"  eng_mask: {eng_mask.shape}")
    print(f"  ita_mask: {ita_mask.shape}")
    
    # æ®µéšåˆ¥å®Ÿè¡Œ
    with torch.no_grad():
        try:
            # Encoderã®ã¿ãƒ†ã‚¹ãƒˆ
            print(f"\n  ğŸ“ Encoderãƒ†ã‚¹ãƒˆ:")
            encoder_output = model.encoder(eng_ids, eng_mask)
            print(f"    Encoderå‡ºåŠ›: {encoder_output.shape}")
            print(f"    çµ±è¨ˆ: mean={encoder_output.mean():.4f}")
            
            # Decoder embeddingã®ã¿ãƒ†ã‚¹ãƒˆ  
            print(f"\n  ğŸ“ Decoder Embeddingãƒ†ã‚¹ãƒˆ:")
            decoder_emb = model.decoder_embeddings(ita_ids)
            print(f"    Decoder embedding: {decoder_emb.shape}")
            print(f"    çµ±è¨ˆ: mean={decoder_emb.mean():.4f}")
            
            # å…¨ä½“ãƒ†ã‚¹ãƒˆ
            print(f"\n  ğŸ“ å…¨ä½“ãƒ†ã‚¹ãƒˆ:")
            logits = model(eng_ids, ita_ids, eng_mask, ita_mask)
            print(f"    æœ€çµ‚å‡ºåŠ›: {logits.shape}")
            print(f"    çµ±è¨ˆ: mean={logits.mean():.4f}")
            
        except Exception as e:
            print(f"    âŒ æ®µéšåˆ¥ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()

def test_mask_shapes():
    """
    âœ… æ–°è¦: ãƒã‚¹ã‚¯ã®å½¢çŠ¶ã‚’è©³ç´°ãƒ†ã‚¹ãƒˆ
    """
    print(f"\nğŸ”¬ ãƒã‚¹ã‚¯å½¢çŠ¶ãƒ†ã‚¹ãƒˆ:")
    
    batch_size, seq_len = 2, 4
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    ids = torch.tensor([
        [101, 123, 456, 102],
        [101, 789, 102, 0]    # æœ€å¾ŒãŒãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    ])
    
    print(f"  å…¥åŠ›IDs: {ids.shape}")
    print(f"  ids[0]: {ids[0].tolist()}")
    print(f"  ids[1]: {ids[1].tolist()}")
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¹ã‚¯
    padding_mask = create_padding_mask(ids)
    print(f"  padding_mask: {padding_mask.shape}")
    print(f"  padding_mask[0,0,0]: {padding_mask[0,0,0].tolist()}")
    print(f"  padding_mask[1,0,0]: {padding_mask[1,0,0].tolist()}")
    
    # Causalãƒã‚¹ã‚¯
    causal_mask = create_causal_mask(seq_len, batch_size)
    print(f"  causal_mask: {causal_mask.shape}")
    print(f"  causal_mask[0,0]: \n{causal_mask[0,0]}")

def main():
    test_mask_shapes()      # âœ… ã¾ãšãƒã‚¹ã‚¯ã‚’ãƒ†ã‚¹ãƒˆ
    test_step_by_step()     # âœ… æ®µéšåˆ¥ãƒ†ã‚¹ãƒˆ
    test_translation_model() # âœ… å…¨ä½“ãƒ†ã‚¹ãƒˆ

if __name__ == "__main__":
    main()