{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4576bb2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99bf338d",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import math\n",
    "\n",
    "class ImprovedTranslationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder-Decoder Translation Model with proper architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 d_model=512,\n",
    "                 num_layers=6,\n",
    "                 num_heads=8,\n",
    "                 d_ff=2048,\n",
    "                 max_seq_len=128,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        \n",
    "        # Encoder layers (unidirectional for translation)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def encode(self, src_ids, src_mask=None):\n",
    "        # Source embedding + positional encoding\n",
    "        src_emb = self.src_embedding(src_ids) * math.sqrt(self.d_model)\n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "        \n",
    "        encoder_output = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output, src_mask)\n",
    "            \n",
    "        return encoder_output\n",
    "    \n",
    "    def decode(self, tgt_ids, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        # Target embedding + positional encoding\n",
    "        tgt_emb = self.tgt_embedding(tgt_ids) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoding(tgt_emb)\n",
    "        \n",
    "        decoder_output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_output, encoder_output, src_mask, tgt_mask)\n",
    "            \n",
    "        return self.output_projection(decoder_output)\n",
    "    \n",
    "    def forward(self, src_ids, tgt_ids, src_mask=None, tgt_mask=None):\n",
    "        encoder_output = self.encode(src_ids, src_mask)\n",
    "        decoder_output = self.decode(tgt_ids, encoder_output, src_mask, tgt_mask)\n",
    "        return decoder_output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding\"\"\"\n",
    "    def __init__(self, d_model, max_seq_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Standard Transformer Encoder Layer\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.self_attention(x, x, x, key_padding_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \"\"\"Standard Transformer Decoder Layer\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.cross_attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        # Self-attention with causal mask\n",
    "        attn_output, _ = self.self_attention(x, x, x, attn_mask=tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Cross-attention\n",
    "        cross_attn_output, _ = self.cross_attention(x, encoder_output, encoder_output, key_padding_mask=src_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ImprovedTranslationDataset(Dataset):\n",
    "    \"\"\"Simplified dataset for translation\"\"\"\n",
    "    def __init__(self, data_pairs, src_tokenizer, tgt_tokenizer, max_len=128):\n",
    "        self.data_pairs = data_pairs\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_text, tgt_text = self.data_pairs[idx]\n",
    "        \n",
    "        # Tokenize source\n",
    "        src_tokens = self.src_tokenizer(\n",
    "            src_text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize target with BOS/EOS\n",
    "        tgt_tokens = self.tgt_tokenizer(\n",
    "            tgt_text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        src_ids = src_tokens['input_ids'].squeeze()\n",
    "        src_mask = src_tokens['attention_mask'].squeeze() == 0  # True for padding\n",
    "        \n",
    "        tgt_ids = tgt_tokens['input_ids'].squeeze()\n",
    "        tgt_input = tgt_ids[:-1]  # Remove last token for input\n",
    "        tgt_target = tgt_ids[1:]  # Remove first token for target\n",
    "        \n",
    "        # Create causal mask for target\n",
    "        tgt_len = tgt_input.size(0)\n",
    "        tgt_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1).bool()\n",
    "        \n",
    "        return {\n",
    "            'src_ids': src_ids,\n",
    "            'src_mask': src_mask,\n",
    "            'tgt_input': tgt_input,\n",
    "            'tgt_target': tgt_target,\n",
    "            'tgt_mask': tgt_mask\n",
    "        }\n",
    "\n",
    "def create_improved_model():\n",
    "    \"\"\"Create an improved translation model\"\"\"\n",
    "    \n",
    "    # Use consistent tokenizers\n",
    "    src_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    tgt_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    \n",
    "    model = ImprovedTranslationModel(\n",
    "        src_vocab_size=src_tokenizer.vocab_size,\n",
    "        tgt_vocab_size=tgt_tokenizer.vocab_size,\n",
    "        d_model=512,\n",
    "        num_layers=6,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        max_seq_len=128,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    return model, src_tokenizer, tgt_tokenizer\n",
    "\n",
    "def train_improved_model(model, train_dataloader, val_dataloader, epochs=10):\n",
    "    \"\"\"Simplified training loop\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is padding\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            src_ids = batch['src_ids'].to(device)\n",
    "            src_mask = batch['src_mask'].to(device)\n",
    "            tgt_input = batch['tgt_input'].to(device)\n",
    "            tgt_target = batch['tgt_target'].to(device)\n",
    "            tgt_mask = batch['tgt_mask'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(src_ids, tgt_input, src_mask, tgt_mask)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), tgt_target.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                src_ids = batch['src_ids'].to(device)\n",
    "                src_mask = batch['src_mask'].to(device)\n",
    "                tgt_input = batch['tgt_input'].to(device)\n",
    "                tgt_target = batch['tgt_target'].to(device)\n",
    "                tgt_mask = batch['tgt_mask'].to(device)\n",
    "                \n",
    "                logits = model(src_ids, tgt_input, src_mask, tgt_mask)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), tgt_target.view(-1))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Usage example\n",
    "# model, src_tokenizer, tgt_tokenizer = create_improved_model()\n",
    "# train_dataset = ImprovedTranslationDataset(train_data, src_tokenizer, tgt_tokenizer)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# train_improved_model(model, train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
