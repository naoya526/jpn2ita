{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIGY--_sZ0jb"
      },
      "source": [
        "## English to Italian automatic translation\n",
        "\n",
        "Automatic language translation is often regarded to as the most typical sequence-to-sequence problem. Traditional approaches based on explicitly modeling languages have been proven difficult. In the last decade, deep learning demonstrated to be a more than viable solution to this problem.\n",
        "\n",
        "Deep learning solutions only requires a large bilingual corpus, and computational resources. Encoder-decoder architectures are the most widely used. They can be implemented with recurrent networks (LSTM and the like) or transformers (which are the state-of-the-art for this problem).\n",
        "\n",
        "In this lab activity we will build a simple English-to-Italian translation system based on a pair of LSTM networks working together in a encoder-decoder architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_KoPm3bz9A9"
      },
      "source": [
        "## Data\n",
        "\n",
        "We will use a subset of the English-Italian bilingual dataset from the [Tatoeba Project](https://www.manythings.org/anki/).\n",
        "\n",
        "There are two files, `text-eng.txt` and `text-ita.txt`, containing 333112 lines, each one reporting one sentenced in English or Italian. Sentences are paired so that the i-th sentence in the English file has a corresponding translation in the i-th sentence in the Italian file.\n",
        "\n",
        "Each sentence has been already converted to lowercase, rewritten as space-separated tokens (words and punctuation symbols). Each sentence starts with the special `<sos>` token and is terminated by the `<eos>` token. The longest sequences are 20 tokens long.\n",
        "\n",
        "For instance, this is one example from the English file:\n",
        "\n",
        "`<sos> do you want me to make coffee ? <eos>`\n",
        "\n",
        "and this is the corresponding translation in the Italian file:\n",
        "\n",
        "`<sos> vuoi che prepari del caffè ? <eos>`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YxQ9TgUuafMn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT\n",
            "To: /home/naoya/pv2/deeplearning/jpn2ita/eng-ita.tar.gz\n",
            "100%|██████████| 3.92M/3.92M [00:05<00:00, 748kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files downloaded and extracted successfully!\n",
            "text-eng.txt: 333,112 lines\n",
            "text-ita.txt: 333,112 lines\n"
          ]
        }
      ],
      "source": [
        "# Download the files using gdown\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Check if files already exist\n",
        "if not os.path.exists(\"text-eng.txt\") or not os.path.exists(\"text-ita.txt\"):\n",
        "    print(\"Downloading files...\")\n",
        "    \n",
        "    # Option 1: Use gdown directly in Python (recommended for venv)\n",
        "    try:\n",
        "        import gdown\n",
        "        url = \"https://drive.google.com/file/d/1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT/view?usp=sharing\"\n",
        "        output = gdown.download(url, fuzzy=True)\n",
        "        \n",
        "        # Extract the downloaded tar.gz file\n",
        "        import tarfile\n",
        "        with tarfile.open(output, 'r:gz') as tar:\n",
        "            tar.extractall()\n",
        "        \n",
        "        # Remove the tar file\n",
        "        os.remove(output)\n",
        "        print(\"Files downloaded and extracted successfully!\")\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"gdown not installed. Installing...\")\n",
        "        !pip install gdown\n",
        "        import gdown\n",
        "        url = \"https://drive.google.com/file/d/1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT/view?usp=sharing\"\n",
        "        output = gdown.download(url, fuzzy=True)\n",
        "        \n",
        "        import tarfile\n",
        "        with tarfile.open(output, 'r:gz') as tar:\n",
        "            tar.extractall()\n",
        "        os.remove(output)\n",
        "        print(\"Files downloaded and extracted successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading with gdown: {e}\")\n",
        "        print(\"Trying alternative method...\")\n",
        "        \n",
        "        # Option 2: Use subprocess with explicit Python path (fallback)\n",
        "        try:\n",
        "            result = subprocess.run([\n",
        "                \"/home/naoya/pv2/deeplearning/jpn2ita/.venv/bin/python\", \n",
        "                \"-m\", \"gdown\", \n",
        "                \"--fuzzy\", \n",
        "                \"https://drive.google.com/file/d/1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT/view?usp=sharing\"\n",
        "            ], capture_output=True, text=True, check=True)\n",
        "            \n",
        "            # Extract using tar command\n",
        "            subprocess.run([\"tar\", \"-xzf\", \"*.tar.gz\"], shell=True, check=True)\n",
        "            print(\"Files downloaded and extracted successfully with subprocess!\")\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Subprocess error: {e}\")\n",
        "            print(\"Please download files manually from the provided URL\")\n",
        "            \n",
        "else:\n",
        "    print(\"Files already exist!\")\n",
        "\n",
        "# Verify files exist\n",
        "if os.path.exists(\"text-eng.txt\") and os.path.exists(\"text-ita.txt\"):\n",
        "    with open(\"text-eng.txt\", 'r') as f:\n",
        "        eng_lines = len(f.readlines())\n",
        "    with open(\"text-ita.txt\", 'r') as f:\n",
        "        ita_lines = len(f.readlines())\n",
        "    \n",
        "    print(f\"text-eng.txt: {eng_lines:,} lines\")\n",
        "    print(f\"text-ita.txt: {ita_lines:,} lines\")\n",
        "else:\n",
        "    print(\"Files not found. Please check the download process.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eSTJ6qCG3pS"
      },
      "source": [
        "### Vocabularies\n",
        "\n",
        "First, we need to build separate vocabularies for English and Italian.\n",
        "For each language we need to find the list of unique tokens, and an inverse  mapping between tokens and their index in the list.\n",
        "\n",
        "We need to include in the vocabularies also the special tokens `<sos>`, `<eos>` and `<pad>` (that we will need later, and is not in the dataset). It's better if we can manage to have these three tokens in the same position (index) of both vocabularies.\n",
        "\n",
        "For making the list of VOCABULARY, I used `set`, which enables to add unique tokens. \n",
        "\n",
        "In this dataset, there's no UPPERCASE. Hence, we don't need to have the process of convert them into lower case in this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_6-FlVRhNFh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English vocabulary size: 7583\n",
            "Italian vocabulary size: 9963\n",
            "Special token indices - SOS: 0, EOS: 1, PAD: 2\n",
            "First 10 English tokens: ['<sos>', '<eos>', '<pad>', '!', '\"', '$', '%', ',', '.', '00', '000', '1', '10', '100', '11', '110', '119', '12', '13', '13-year-old', '15', '18', '1939', '1941', '1945', '1950s', '1960', '1969', '1980', '2', '20', '200', '2003', '2013', '20th', '22', '24', '25', '3', '30', '300', '4', '40', '5', '50', '500', '5th', '6', '60', '7', '70', '8', '80', '9', '90', '911', ':', '?', 'a', \"a's\", 'abacus', 'abandon', 'abandoned', 'abducted', 'abilities', 'ability', 'able', 'aboard', 'about', 'above', 'abroad', 'abrupt', 'absence', 'absent', 'absent-minded', 'absolute', 'absolutely', 'absorb', 'absorbed', 'absorbs', 'absurd', 'abundant', 'abused', 'abusive', 'accent', 'accept', 'acceptable', 'accepted', 'accepting', 'accepts', 'access', 'accident', 'accidentally', 'accidents', 'accompanied', 'accomplice', 'accomplish', 'accomplished', 'accomplishment', 'according']\n",
            "First 10 Italian tokens: ['<sos>', '<eos>', '<pad>', '!', '\"', '$', '%', \"'\", ',', '.', '/', '00', '000', '1', '10', '100', '11', '110', '119', '12', '15', '18', '1939', '1941', '1945', '1960', '1969', '1980', '2', '20', '2003', '2013', '22', '24', '25', '3', '30', '300', '4', '40', '5', '50', '500', '6', '60', '7', '8', '9', '90', '911', ':', '?', 'a', 'abaco', 'abbaia', 'abbaiando', 'abbaiare', 'abbaiato', 'abbaiò', 'abbandonare', 'abbandonata', 'abbandonate', 'abbandonato', 'abbandonerò', 'abbandonò', 'abbassa', 'abbassare', 'abbassate', 'abbassato', 'abbassi', 'abbastanza', 'abbattere', 'abbattuto', 'abbi', 'abbia', 'abbiamo', 'abbiano', 'abbiate', 'abbigliamento', 'abbina', 'abbraccia', 'abbracciare', 'abbracciarono', 'abbracciate', 'abbracciati', 'abbracciato', 'abbraccio', 'abbracciò', 'abile', 'abilità', 'abita', 'abitando', 'abitano', 'abitanti', 'abitare', 'abitasse', 'abitate', 'abitato', 'abitava', 'abitavo']\n",
            "ENG_INVERSE: [('<sos>', 0), ('<eos>', 1), ('<pad>', 2), ('!', 3), ('\"', 4), ('$', 5), ('%', 6), (',', 7), ('.', 8), ('00', 9), ('000', 10), ('1', 11), ('10', 12), ('100', 13), ('11', 14), ('110', 15), ('119', 16), ('12', 17), ('13', 18), ('13-year-old', 19), ('15', 20), ('18', 21), ('1939', 22), ('1941', 23), ('1945', 24), ('1950s', 25), ('1960', 26), ('1969', 27), ('1980', 28), ('2', 29), ('20', 30), ('200', 31), ('2003', 32), ('2013', 33), ('20th', 34), ('22', 35), ('24', 36), ('25', 37), ('3', 38), ('30', 39), ('300', 40), ('4', 41), ('40', 42), ('5', 43), ('50', 44), ('500', 45), ('5th', 46), ('6', 47), ('60', 48), ('7', 49), ('70', 50), ('8', 51), ('80', 52), ('9', 53), ('90', 54), ('911', 55), (':', 56), ('?', 57), ('a', 58), (\"a's\", 59), ('abacus', 60), ('abandon', 61), ('abandoned', 62), ('abducted', 63), ('abilities', 64), ('ability', 65), ('able', 66), ('aboard', 67), ('about', 68), ('above', 69), ('abroad', 70), ('abrupt', 71), ('absence', 72), ('absent', 73), ('absent-minded', 74), ('absolute', 75), ('absolutely', 76), ('absorb', 77), ('absorbed', 78), ('absorbs', 79), ('absurd', 80), ('abundant', 81), ('abused', 82), ('abusive', 83), ('accent', 84), ('accept', 85), ('acceptable', 86), ('accepted', 87), ('accepting', 88), ('accepts', 89), ('access', 90), ('accident', 91), ('accidentally', 92), ('accidents', 93), ('accompanied', 94), ('accomplice', 95), ('accomplish', 96), ('accomplished', 97), ('accomplishment', 98), ('according', 99)]\n",
            "ITA_INVERSE: [('<sos>', 0), ('<eos>', 1), ('<pad>', 2), ('!', 3), ('\"', 4), ('$', 5), ('%', 6), (\"'\", 7), (',', 8), ('.', 9), ('/', 10), ('00', 11), ('000', 12), ('1', 13), ('10', 14), ('100', 15), ('11', 16), ('110', 17), ('119', 18), ('12', 19), ('15', 20), ('18', 21), ('1939', 22), ('1941', 23), ('1945', 24), ('1960', 25), ('1969', 26), ('1980', 27), ('2', 28), ('20', 29), ('2003', 30), ('2013', 31), ('22', 32), ('24', 33), ('25', 34), ('3', 35), ('30', 36), ('300', 37), ('4', 38), ('40', 39), ('5', 40), ('50', 41), ('500', 42), ('6', 43), ('60', 44), ('7', 45), ('8', 46), ('9', 47), ('90', 48), ('911', 49), (':', 50), ('?', 51), ('a', 52), ('abaco', 53), ('abbaia', 54), ('abbaiando', 55), ('abbaiare', 56), ('abbaiato', 57), ('abbaiò', 58), ('abbandonare', 59), ('abbandonata', 60), ('abbandonate', 61), ('abbandonato', 62), ('abbandonerò', 63), ('abbandonò', 64), ('abbassa', 65), ('abbassare', 66), ('abbassate', 67), ('abbassato', 68), ('abbassi', 69), ('abbastanza', 70), ('abbattere', 71), ('abbattuto', 72), ('abbi', 73), ('abbia', 74), ('abbiamo', 75), ('abbiano', 76), ('abbiate', 77), ('abbigliamento', 78), ('abbina', 79), ('abbraccia', 80), ('abbracciare', 81), ('abbracciarono', 82), ('abbracciate', 83), ('abbracciati', 84), ('abbracciato', 85), ('abbraccio', 86), ('abbracciò', 87), ('abile', 88), ('abilità', 89), ('abita', 90), ('abitando', 91), ('abitano', 92), ('abitanti', 93), ('abitare', 94), ('abitasse', 95), ('abitate', 96), ('abitato', 97), ('abitava', 98), ('abitavo', 99)]\n"
          ]
        }
      ],
      "source": [
        "SPECIAL = [\"<sos>\", \"<eos>\", \"<pad>\",\"<unk>\"]  # Added \"<unk>\" for unknown tokens\n",
        "MAXLEN = 50\n",
        "\n",
        "# English vocabulary creation with unique tokens\n",
        "f = open(\"text-eng.txt\")\n",
        "eng_tokens_set = set(SPECIAL)  # Start with special tokens\n",
        "for line in f:\n",
        "    line = line.strip() # Remove leading/trailing whitespace\n",
        "    if line and len(line.split()) <= MAXLEN: # Check if the line is not empty and does not exceed MAXLEN\n",
        "        tokens = line.split() # Split the line into tokens\n",
        "        eng_tokens_set.update(tokens) # Add tokens to the set (automatically handles duplicates)\n",
        "\n",
        "f.close()\n",
        "\n",
        "# Convert set to list for vocabulary\n",
        "ENG_VOCABULARY = list(eng_tokens_set)\n",
        "\n",
        "# Italian vocabulary creation with unique tokens\n",
        "f = open(\"text-ita.txt\")\n",
        "ita_tokens_set = set(SPECIAL)  # Start with special tokens\n",
        "for line in f:\n",
        "    line = line.strip()  # Remove leading/trailing whitespace\n",
        "    if line and len(line.split()) <= MAXLEN:\n",
        "        tokens = line.split()\n",
        "        ita_tokens_set.update(tokens) # Add tokens to the set (automatically handles duplicates)\n",
        "\n",
        "f.close()\n",
        "\n",
        "# Convert set to list for vocabulary\n",
        "ITA_VOCABULARY = list(ita_tokens_set)\n",
        "\n",
        "# Make sure that the three special tokens have the same indices in the two vocabularies.\n",
        "# Sort vocabularies to ensure consistent ordering, with special tokens first\n",
        "ENG_VOCABULARY = SPECIAL + sorted([token for token in ENG_VOCABULARY if token not in SPECIAL])\n",
        "ITA_VOCABULARY = SPECIAL + sorted([token for token in ITA_VOCABULARY if token not in SPECIAL])\n",
        "\n",
        "# Assign the three indices for special tokens\n",
        "SOS = 0  # Index of \"<sos>\"\n",
        "EOS = 1  # Index of \"<eos>\"  \n",
        "PAD = 2  # Index of \"<pad>\"\n",
        "UNK = 3  # Index of \"<unk>\" (unknown token)\n",
        "\n",
        "# Inverse mappings.\n",
        "ENG_INVERSE = {w: n for n, w in enumerate(ENG_VOCABULARY)}\n",
        "ITA_INVERSE = {w: n for n, w in enumerate(ITA_VOCABULARY)}\n",
        "\n",
        "print(f\"English vocabulary size: {len(ENG_VOCABULARY)}\")\n",
        "print(f\"Italian vocabulary size: {len(ITA_VOCABULARY)}\")\n",
        "print(f\"Special token indices - SOS: {SOS}, EOS: {EOS}, PAD: {PAD}, UNK: {UNK}\")\n",
        "print(f\"First 10 English tokens: {ENG_VOCABULARY[:100]}\")\n",
        "print(f\"First 10 Italian tokens: {ITA_VOCABULARY[:100]}\")\n",
        "print(f\"ENG_INVERSE: {list(ENG_INVERSE.items())[:100]}\")\n",
        "print(f\"ITA_INVERSE: {list(ITA_INVERSE.items())[:100]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "account\n",
            "3571\n"
          ]
        }
      ],
      "source": [
        "# usage example\n",
        "print(ENG_VOCABULARY[100]) # Should print \"<sos>\"\n",
        "print(ENG_INVERSE[\"italy\"])  # Should print 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPz15mo4Ibj1"
      },
      "source": [
        "### Encoding/decoding functions\n",
        "\n",
        "We need now functions to map strings with sentences into lists of numerical indices, and vice-versa. Thse functions will take as arguments also the vocabularies, or thweir inverses, so that we can use them for both English and Italian.\n",
        "\n",
        "Having all sequences of the same length simplify training.\n",
        "For this reason, the `encode_sentence` should add padding to make sure that the list of codes include exactly `MAXLEN` elements.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GCElBQ1VIpKV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded length: 50\n",
            "Codes: [0, 2004, 7560, 7271, 4082, 6805, 4016, 1322, 57, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "Decoded: <sos> do you want me to make coffee ? <eos>\n",
            "Encoded length: 50\n",
            "Codes: [0, 9917, 1648, 6772, 2579, 1345, 51, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "Decoded: <sos> vuoi che prepari del caffè ? <eos>\n"
          ]
        }
      ],
      "source": [
        "def encode_sentence(sentence, inverse):\n",
        "    \"\"\"Translate the sentence as a list of numerical codes, given the inverse mapping.\"\"\"\n",
        "    inverse_sentence = []  # goal: append numerical codes for each word\n",
        "    words = sentence.strip().split()  # Split the sentence into words and remove whitespace\n",
        "    \n",
        "    # Convert words to indices\n",
        "    for word in words:\n",
        "        if word in inverse:\n",
        "            inverse_sentence.append(inverse[word])\n",
        "        else:\n",
        "            inverse_sentence.append(UNK)\n",
        "            print(f\"Warning: Unknown word '{word}' skipped\")  # Handle unknown words\n",
        "            continue\n",
        "    \n",
        "    # Add padding to make all sequences the same length (MAXLEN)\n",
        "    while len(inverse_sentence) < MAXLEN:\n",
        "        inverse_sentence.append(PAD)  # PAD = 2\n",
        "    \n",
        "    # Truncate if too long\n",
        "    if len(inverse_sentence) > MAXLEN:\n",
        "        inverse_sentence = inverse_sentence[:MAXLEN]\n",
        "    \n",
        "    return inverse_sentence\n",
        "\n",
        "\n",
        "\n",
        "def decode_sentence(codes, voc):\n",
        "    \"\"\"Translate a list of numerical codes into a sentence, given the mapping.\"\"\"\n",
        "    sentence = []\n",
        "    for code in codes:\n",
        "        if code == PAD:  # Stop at padding\n",
        "            break\n",
        "        sentence.append(voc[code])\n",
        "    return \" \".join(sentence)\n",
        "\n",
        "\n",
        "# Test the functions\n",
        "eng = \"<sos> do you want me to make coffee ? <eos>\"\n",
        "codes = encode_sentence(eng, ENG_INVERSE)\n",
        "print(f\"Encoded length: {len(codes)}\")\n",
        "print(f\"Codes: {codes}\")\n",
        "print(f\"Decoded: {decode_sentence(codes, ENG_VOCABULARY)}\")\n",
        "\n",
        "ita = \"<sos> vuoi che prepari del caffè ? <eos>\"\n",
        "codes = encode_sentence(ita, ITA_INVERSE)\n",
        "print(f\"Encoded length: {len(codes)}\")\n",
        "print(f\"Codes: {codes}\")\n",
        "print(f\"Decoded: {decode_sentence(codes, ITA_VOCABULARY)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8dGgIr2L85f"
      },
      "source": [
        "### Dataset and data loader\n",
        "\n",
        "All the data will be loaded into memory. The `torch.utils.data.TensorDataset` will make the data accessible to the data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6F0oHau0XzvB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 50]) torch.int64 torch.Size([64, 50]) torch.int64\n",
            "<sos> i thought that tom would say that . <eos>\n",
            "<sos> pensavo che tom l'avrebbe detto . <eos>\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "with open(\"text-eng.txt\") as f:\n",
        "    eng_sentences = [encode_sentence(line, ENG_INVERSE) for line in f]\n",
        "\n",
        "with open(\"text-ita.txt\") as f:\n",
        "    ita_sentences = [encode_sentence(line, ITA_INVERSE) for line in f]\n",
        "\n",
        "train_set = torch.utils.data.TensorDataset(torch.tensor(eng_sentences), torch.tensor(ita_sentences))\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "eng, ita = next(iter(train_loader))\n",
        "print(eng.shape, eng.dtype, ita.shape, ita.dtype)\n",
        "\n",
        "print(decode_sentence(eng[0], ENG_VOCABULARY))\n",
        "print(decode_sentence(ita[0], ITA_VOCABULARY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_RimpD7Z0Uv"
      },
      "source": [
        "## Model\n",
        "\n",
        "We will use an encoder-decoder architecture (picture from \"Dive into deep learning\").\n",
        "\n",
        "![link text](https://d2l.ai/_images/seq2seq.svg)\n",
        "\n",
        "The encoder will read the English sentence and encode it into a vector of features (we will use both the final hidden state and cell state).\n",
        "\n",
        "The decoder will output Italian tokens, given the previous one.\n",
        "The encoded input is passed to the decoder as initial state and as additional input at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wZdPRFY3bKCJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([7, 22]) -> torch.Size([2, 7, 256]) torch.Size([2, 7, 256])\n",
            "torch.Size([7, 22]) -> torch.Size([7, 22, 9963])\n"
          ]
        }
      ],
      "source": [
        "DIM = 256\n",
        "DROPOUT = 0.2\n",
        "LAYERS = 2\n",
        "\n",
        "encoder = torch.nn.Sequential(\n",
        "    torch.nn.Embedding(len(ENG_VOCABULARY), DIM),\n",
        "    torch.nn.LSTM(DIM, DIM, batch_first=True, dropout=DROPOUT, num_layers=LAYERS)\n",
        ")\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(len(ITA_VOCABULARY), embedding_size)\n",
        "        self.cell_linear = torch.nn.Linear(hidden_size, embedding_size)\n",
        "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, batch_first=True, dropout=DROPOUT, num_layers=LAYERS)\n",
        "        self.linear = torch.nn.Linear(hidden_size, len(ITA_VOCABULARY))\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        cell_state = hidden[1][-1]\n",
        "        output = self.embedding(input)\n",
        "        y = self.cell_linear(cell_state).unsqueeze(1)\n",
        "        output = output + y\n",
        "        output, _ = self.lstm(output, hidden)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "decoder = Decoder(DIM, DIM)\n",
        "\n",
        "input1 = torch.zeros(7, 22, dtype=torch.long)\n",
        "_, hidden = encoder(input1)\n",
        "print(input1.shape, \"->\", hidden[0].shape, hidden[1].shape)\n",
        "\n",
        "input2 = torch.zeros(7, 22, dtype=torch.long)\n",
        "output = decoder(input2, hidden)\n",
        "print(input2.shape, \"->\", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7LzwL-heBOy"
      },
      "source": [
        "## Training\n",
        "\n",
        "During training the cross entropy is minimized.\n",
        "Each output from the decoder is compared to the next token in the output sequence.\n",
        "\n",
        "Padding should be ignored during training. The `torch.nn.CrossEntropyLoss` has an optional argument for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BQIGRquneFIF"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "DEVICE = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder.to(DEVICE)\n",
        "decoder.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=LEARNING_RATE)\n",
        "loss_fun = torch.nn.CrossEntropyLoss(ignore_index=PAD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9Co2s3veSXN"
      },
      "outputs": [],
      "source": [
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "steps = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    for lq, sq in train_loader:\n",
        "        lq = lq.to(DEVICE)\n",
        "        sq = sq.to(DEVICE)\n",
        "        _, hidden = encoder(lq)\n",
        "        output = decoder(sq[:, :-1], hidden)\n",
        "        loss = loss_fun(output.permute(0, 2, 1), sq[:, 1:])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "        if steps % 1000 == 0:\n",
        "            predictions = output.argmax(2)\n",
        "            correct = (predictions == sq[:, 1:]).sum().item()\n",
        "            total = (sq[:, 1:] != PAD).sum().item()\n",
        "            accuracy = 100 * correct / max(total, 1)\n",
        "            print(f\"{steps} [{epoch}]  Loss: {loss.item():.4f}  Acc: {accuracy:.1f}%\")\n",
        "            print(decode_sentence(lq[0], ENG_VOCABULARY))\n",
        "            print(decode_sentence(sq[0], ITA_VOCABULARY))\n",
        "            print(decode_sentence(predictions[0], ITA_VOCABULARY))\n",
        "            print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB4qZO1RWa-s"
      },
      "source": [
        "## Using the model\n",
        "\n",
        "To translate a new sentence, you need to:\n",
        "\n",
        "1. encode the input sentence;\n",
        "2. initialize the output sentence with the `<sos>` token;\n",
        "3. pass the current output into the decoder together with the encoder state;\n",
        "4. take the output token with the highest score, and add it to the current output.\n",
        "5. repeat from step 3 until the `<eos>` token is generated.\n",
        "\n",
        "Implement this algorithm and use it to translate some English sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWn46pASkOOo"
      },
      "outputs": [],
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "eng = \"<sos> how old are you ? <eos>\"\n",
        "# eng = \"<sos> i like to play tennis . <eos>\"\n",
        "# eng = \"<sos> i hope it snows at christmas . <eos>\"\n",
        "# eng = \"<sos> would you like to go to the movie theater . <eos>\"\n",
        "\n",
        "input = torch.tensor([encode_sentence(eng, ENG_INVERSE)], device=DEVICE)\n",
        "_, hidden = encoder(input)\n",
        "\n",
        "output = torch.zeros(1, MAXLEN, dtype=torch.long, device=DEVICE)\n",
        "output[0, 0] = SOS\n",
        "\n",
        "# ...\n",
        "\n",
        "ita = ...\n",
        "\n",
        "print(ita)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
