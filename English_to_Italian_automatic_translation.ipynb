{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## English to Italian automatic translation\n",
        "\n",
        "Automatic language translation is often regarded to as the most typical sequence-to-sequence problem. Traditional approaches based on explicitly modeling languages have been proven difficult. In the last decade, deep learning demonstrated to be a more than viable solution to this problem.\n",
        "\n",
        "Deep learning solutions only requires a large bilingual corpus, and computational resources. Encoder-decoder architectures are the most widely used. They can be implemented with recurrent networks (LSTM and the like) or transformers (which are the state-of-the-art for this problem).\n",
        "\n",
        "In this lab activity we will build a simple English-to-Italian translation system based on a pair of LSTM networks working together in a encoder-decoder architecture."
      ],
      "metadata": {
        "id": "nIGY--_sZ0jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "We will use a subset of the English-Italian bilingual dataset from the [Tatoeba Project](https://www.manythings.org/anki/).\n",
        "\n",
        "There are two files, `text-eng.txt` and `text-ita.txt`, containing 333112 lines, each one reporting one sentenced in English or Italian. Sentences are paired so that the i-th sentence in the English file has a corresponding translation in the i-th sentence in the Italian file.\n",
        "\n",
        "Each sentence has been already converted to lowercase, rewritten as space-separated tokens (words and punctuation symbols). Each sentence starts with the special `<sos>` token and is terminated by the `<eos>` token. The longest sequences are 20 tokens long.\n",
        "\n",
        "For instance, this is one example from the English file:\n",
        "\n",
        "`<sos> do you want me to make coffee ? <eos>`\n",
        "\n",
        "and this is the corresponding translation in the Italian file:\n",
        "\n",
        "`<sos> vuoi che prepari del caffè ? <eos>`\n"
      ],
      "metadata": {
        "id": "S_KoPm3bz9A9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the files\n",
        "URL = \"https://drive.google.com/file/d/1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT/view?usp=sharing\"\n",
        "!gdown --fuzzy $URL -O- | tar -xz"
      ],
      "metadata": {
        "id": "YxQ9TgUuafMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabularies\n",
        "\n",
        "First, we need to build separate vocabularies for English and Italian.\n",
        "For each language we need to find the list of unique tokens, and an inverse  mapping between tokens and their index in the list.\n",
        "\n",
        "We need to include in the vocabularies also the special tokens `<sos>`, `<eos>` and `<pad>` (that we will need later, and is not in the dataset). It's better if we can manage to have these three tokens in the same position (index) of both vocabularies."
      ],
      "metadata": {
        "id": "-eSTJ6qCG3pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SPECIAL = [\"<sos>\", \"<eos>\", \"<pad>\"]\n",
        "MAXLEN = 20\n",
        "\n",
        "f = open(\"text-eng.txt\")\n",
        "# Define the list of all tokens in the English set ...\n",
        "ENG_VOCABULARY = ...\n",
        "\n",
        "f.close()\n",
        "\n",
        "f = open(\"text-ita.txt\")\n",
        "# Define the list of all tokens in the Italian set ...\n",
        "ITA_VOCABULARY = ...\n",
        "\n",
        "f.close()\n",
        "\n",
        "# Make sure that the three special tokens have the same indices in the two vocabularies.\n",
        "# Assign here the three indices...\n",
        "SOS = ...\n",
        "EOS = ...\n",
        "PAD = ...\n",
        "\n",
        "# Inverse mappings.\n",
        "ENG_INVERSE = {w: n for n, w in enumerate(ENG_VOCABULARY)}\n",
        "ITA_INVERSE = {w: n for n, w in enumerate(ITA_VOCABULARY)}\n",
        "\n",
        "print(len(ENG_VOCABULARY), len(ITA_VOCABULARY))"
      ],
      "metadata": {
        "id": "E_6-FlVRhNFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding/decoding functions\n",
        "\n",
        "We need now functions to map strings with sentences into lists of numerical indices, and vice-versa. Thse functions will take as arguments also the vocabularies, or thweir inverses, so that we can use them for both English and Italian.\n",
        "\n",
        "Having all sequences of the same length simplify training.\n",
        "For this reason, the `encode_sentence` should add padding to make sure that the list of codes include exactly `MAXLEN` elements.  "
      ],
      "metadata": {
        "id": "wPz15mo4Ibj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(sentence, inverse):\n",
        "    \"\"\"Translate the sentence as a list of numerical codes, given the inverse mapping.\"\"\"\n",
        "    # ...\n",
        "\n",
        "\n",
        "\n",
        "def decode_sentence(codes, voc):\n",
        "    \"\"\"Translate a list of numerical codes into a sentence, given the mapping.\"\"\"\n",
        "    # ...\n",
        "\n",
        "\n",
        "\n",
        "eng = \"<sos> do you want me to make coffee ? <eos>\"\n",
        "codes = encode_sentence(eng, ENG_INVERSE)\n",
        "print(codes)\n",
        "print(decode_sentence(codes, ENG_VOCABULARY))\n",
        "\n",
        "ita = \"<sos> vuoi che prepari del caffè ? <eos>\"\n",
        "codes = encode_sentence(ita, ITA_INVERSE)\n",
        "print(codes)\n",
        "print(decode_sentence(codes, ITA_VOCABULARY))"
      ],
      "metadata": {
        "id": "GCElBQ1VIpKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset and data loader\n",
        "\n",
        "All the data will be loaded into memory. The `torch.utils.data.TensorDataset` will make the data accessible to the data loader."
      ],
      "metadata": {
        "id": "K8dGgIr2L85f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "with open(\"text-eng.txt\") as f:\n",
        "    eng_sentences = [encode_sentence(line, ENG_INVERSE) for line in f]\n",
        "\n",
        "with open(\"text-ita.txt\") as f:\n",
        "    ita_sentences = [encode_sentence(line, ITA_INVERSE) for line in f]\n",
        "\n",
        "train_set = torch.utils.data.TensorDataset(torch.tensor(eng_sentences), torch.tensor(ita_sentences))\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "eng, ita = next(iter(train_loader))\n",
        "print(eng.shape, eng.dtype, ita.shape, ita.dtype)\n",
        "\n",
        "print(decode_sentence(eng[0], ENG_VOCABULARY))\n",
        "print(decode_sentence(ita[0], ITA_VOCABULARY))"
      ],
      "metadata": {
        "id": "6F0oHau0XzvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "We will use an encoder-decoder architecture (picture from \"Dive into deep learning\").\n",
        "\n",
        "![link text](https://d2l.ai/_images/seq2seq.svg)\n",
        "\n",
        "The encoder will read the English sentence and encode it into a vector of features (we will use both the final hidden state and cell state).\n",
        "\n",
        "The decoder will output Italian tokens, given the previous one.\n",
        "The encoded input is passed to the decoder as initial state and as additional input at each step."
      ],
      "metadata": {
        "id": "S_RimpD7Z0Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = 256\n",
        "DROPOUT = 0.2\n",
        "LAYERS = 2\n",
        "\n",
        "encoder = torch.nn.Sequential(\n",
        "    torch.nn.Embedding(len(ENG_VOCABULARY), DIM),\n",
        "    torch.nn.LSTM(DIM, DIM, batch_first=True, dropout=DROPOUT, num_layers=LAYERS)\n",
        ")\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(len(ITA_VOCABULARY), embedding_size)\n",
        "        self.cell_linear = torch.nn.Linear(hidden_size, embedding_size)\n",
        "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, batch_first=True, dropout=DROPOUT, num_layers=LAYERS)\n",
        "        self.linear = torch.nn.Linear(hidden_size, len(ITA_VOCABULARY))\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        cell_state = hidden[1][-1]\n",
        "        output = self.embedding(input)\n",
        "        y = self.cell_linear(cell_state).unsqueeze(1)\n",
        "        output = output + y\n",
        "        output, _ = self.lstm(output, hidden)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "decoder = Decoder(DIM, DIM)\n",
        "\n",
        "input1 = torch.zeros(7, 22, dtype=torch.long)\n",
        "_, hidden = encoder(input1)\n",
        "print(input1.shape, \"->\", hidden[0].shape, hidden[1].shape)\n",
        "\n",
        "input2 = torch.zeros(7, 22, dtype=torch.long)\n",
        "output = decoder(input2, hidden)\n",
        "print(input2.shape, \"->\", output.shape)"
      ],
      "metadata": {
        "id": "wZdPRFY3bKCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "During training the cross entropy is minimized.\n",
        "Each output from the decoder is compared to the next token in the output sequence.\n",
        "\n",
        "Padding should be ignored during training. The `torch.nn.CrossEntropyLoss` has an optional argument for this."
      ],
      "metadata": {
        "id": "Q7LzwL-heBOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "DEVICE = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder.to(DEVICE)\n",
        "decoder.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=LEARNING_RATE)\n",
        "loss_fun = torch.nn.CrossEntropyLoss(ignore_index=PAD)"
      ],
      "metadata": {
        "id": "BQIGRquneFIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "steps = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    for lq, sq in train_loader:\n",
        "        lq = lq.to(DEVICE)\n",
        "        sq = sq.to(DEVICE)\n",
        "        _, hidden = encoder(lq)\n",
        "        output = decoder(sq[:, :-1], hidden)\n",
        "        loss = loss_fun(output.permute(0, 2, 1), sq[:, 1:])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "        if steps % 1000 == 0:\n",
        "            predictions = output.argmax(2)\n",
        "            correct = (predictions == sq[:, 1:]).sum().item()\n",
        "            total = (sq[:, 1:] != PAD).sum().item()\n",
        "            accuracy = 100 * correct / max(total, 1)\n",
        "            print(f\"{steps} [{epoch}]  Loss: {loss.item():.4f}  Acc: {accuracy:.1f}%\")\n",
        "            print(decode_sentence(lq[0], ENG_VOCABULARY))\n",
        "            print(decode_sentence(sq[0], ITA_VOCABULARY))\n",
        "            print(decode_sentence(predictions[0], ITA_VOCABULARY))\n",
        "            print()"
      ],
      "metadata": {
        "id": "F9Co2s3veSXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the model\n",
        "\n",
        "To translate a new sentence, you need to:\n",
        "\n",
        "1. encode the input sentence;\n",
        "2. initialize the output sentence with the `<sos>` token;\n",
        "3. pass the current output into the decoder together with the encoder state;\n",
        "4. take the output token with the highest score, and add it to the current output.\n",
        "5. repeat from step 3 until the `<eos>` token is generated.\n",
        "\n",
        "Implement this algorithm and use it to translate some English sentence."
      ],
      "metadata": {
        "id": "kB4qZO1RWa-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "eng = \"<sos> how old are you ? <eos>\"\n",
        "# eng = \"<sos> i like to play tennis . <eos>\"\n",
        "# eng = \"<sos> i hope it snows at christmas . <eos>\"\n",
        "# eng = \"<sos> would you like to go to the movie theater . <eos>\"\n",
        "\n",
        "input = torch.tensor([encode_sentence(eng, ENG_INVERSE)], device=DEVICE)\n",
        "_, hidden = encoder(input)\n",
        "\n",
        "output = torch.zeros(1, MAXLEN, dtype=torch.long, device=DEVICE)\n",
        "output[0, 0] = SOS\n",
        "\n",
        "# ...\n",
        "\n",
        "ita = ...\n",
        "\n",
        "print(ita)"
      ],
      "metadata": {
        "id": "gWn46pASkOOo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}