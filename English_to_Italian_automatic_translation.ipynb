{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIGY--_sZ0jb"
      },
      "source": [
        "## English to Italian automatic translation\n",
        "\n",
        "Automatic language translation is often regarded to as the most typical sequence-to-sequence problem. Traditional approaches based on explicitly modeling languages have been proven difficult. In the last decade, deep learning demonstrated to be a more than viable solution to this problem.\n",
        "\n",
        "Deep learning solutions only requires a large bilingual corpus, and computational resources. Encoder-decoder architectures are the most widely used. They can be implemented with recurrent networks (LSTM and the like) or transformers (which are the state-of-the-art for this problem).\n",
        "\n",
        "In this lab activity we will build a simple English-to-Italian translation system based on a pair of LSTM networks working together in a encoder-decoder architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_KoPm3bz9A9"
      },
      "source": [
        "## Data\n",
        "\n",
        "We will use a subset of the English-Italian bilingual dataset from the [Tatoeba Project](https://www.manythings.org/anki/).\n",
        "\n",
        "There are two files, `text-eng.txt` and `text-ita.txt`, containing 333112 lines, each one reporting one sentenced in English or Italian. Sentences are paired so that the i-th sentence in the English file has a corresponding translation in the i-th sentence in the Italian file.\n",
        "\n",
        "Each sentence has been already converted to lowercase, rewritten as space-separated tokens (words and punctuation symbols). Each sentence starts with the special `<sos>` token and is terminated by the `<eos>` token. The longest sequences are 20 tokens long.\n",
        "\n",
        "For instance, this is one example from the English file:\n",
        "\n",
        "`<sos> do you want me to make coffee ? <eos>`\n",
        "\n",
        "and this is the corresponding translation in the Italian file:\n",
        "\n",
        "`<sos> vuoi che prepari del caffè ? <eos>`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YxQ9TgUuafMn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT\n",
            "To: /home/naoya/pv2/deeplearning/jpn2ita/eng-ita.tar.gz\n",
            "100%|██████████| 3.92M/3.92M [00:05<00:00, 748kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files downloaded and extracted successfully!\n",
            "text-eng.txt: 333,112 lines\n",
            "text-ita.txt: 333,112 lines\n"
          ]
        }
      ],
      "source": [
        "# Download the files using gdown\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Check if files already exist\n",
        "if not os.path.exists(\"text-eng.txt\") or not os.path.exists(\"text-ita.txt\"):\n",
        "    print(\"Downloading files...\")\n",
        "    \n",
        "    # Option 1: Use gdown directly in Python (recommended for venv)\n",
        "    try:\n",
        "        import gdown\n",
        "        url = \"https://drive.google.com/file/d/1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT/view?usp=sharing\"\n",
        "        output = gdown.download(url, fuzzy=True)\n",
        "        \n",
        "        # Extract the downloaded tar.gz file\n",
        "        import tarfile\n",
        "        with tarfile.open(output, 'r:gz') as tar:\n",
        "            tar.extractall()\n",
        "        \n",
        "        # Remove the tar file\n",
        "        os.remove(output)\n",
        "        print(\"Files downloaded and extracted successfully!\")\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"gdown not installed. Installing...\")\n",
        "        !pip install gdown\n",
        "        import gdown\n",
        "        url = \"https://drive.google.com/file/d/1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT/view?usp=sharing\"\n",
        "        output = gdown.download(url, fuzzy=True)\n",
        "        \n",
        "        import tarfile\n",
        "        with tarfile.open(output, 'r:gz') as tar:\n",
        "            tar.extractall()\n",
        "        os.remove(output)\n",
        "        print(\"Files downloaded and extracted successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading with gdown: {e}\")\n",
        "        print(\"Trying alternative method...\")\n",
        "        \n",
        "        # Option 2: Use subprocess with explicit Python path (fallback)\n",
        "        try:\n",
        "            result = subprocess.run([\n",
        "                \"/home/naoya/pv2/deeplearning/jpn2ita/.venv/bin/python\", \n",
        "                \"-m\", \"gdown\", \n",
        "                \"--fuzzy\", \n",
        "                \"https://drive.google.com/file/d/1_npGYZk13fs5hE0kAggiSrmKkqW3OrLT/view?usp=sharing\"\n",
        "            ], capture_output=True, text=True, check=True)\n",
        "            \n",
        "            # Extract using tar command\n",
        "            subprocess.run([\"tar\", \"-xzf\", \"*.tar.gz\"], shell=True, check=True)\n",
        "            print(\"Files downloaded and extracted successfully with subprocess!\")\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Subprocess error: {e}\")\n",
        "            print(\"Please download files manually from the provided URL\")\n",
        "            \n",
        "else:\n",
        "    print(\"Files already exist!\")\n",
        "\n",
        "# Verify files exist\n",
        "if os.path.exists(\"text-eng.txt\") and os.path.exists(\"text-ita.txt\"):\n",
        "    with open(\"text-eng.txt\", 'r') as f:\n",
        "        eng_lines = len(f.readlines())\n",
        "    with open(\"text-ita.txt\", 'r') as f:\n",
        "        ita_lines = len(f.readlines())\n",
        "    \n",
        "    print(f\"text-eng.txt: {eng_lines:,} lines\")\n",
        "    print(f\"text-ita.txt: {ita_lines:,} lines\")\n",
        "else:\n",
        "    print(\"Files not found. Please check the download process.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eSTJ6qCG3pS"
      },
      "source": [
        "### Vocabularies\n",
        "\n",
        "First, we need to build separate vocabularies for English and Italian.\n",
        "For each language we need to find the list of unique tokens, and an inverse  mapping between tokens and their index in the list.\n",
        "\n",
        "We need to include in the vocabularies also the special tokens `<sos>`, `<eos>` and `<pad>` (that we will need later, and is not in the dataset). It's better if we can manage to have these three tokens in the same position (index) of both vocabularies.\n",
        "\n",
        "For making the list of VOCABULARY, I used `set`, which enables to add unique tokens. \n",
        "\n",
        "In this dataset, there's no UPPERCASE. Hence, we don't need to have the process of convert them into lower case in this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_6-FlVRhNFh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English vocabulary size: 7583\n",
            "Italian vocabulary size: 9963\n",
            "Special token indices - SOS: 0, EOS: 1, PAD: 2\n",
            "First 10 English tokens: ['<sos>', '<eos>', '<pad>', '!', '\"', '$', '%', ',', '.', '00', '000', '1', '10', '100', '11', '110', '119', '12', '13', '13-year-old', '15', '18', '1939', '1941', '1945', '1950s', '1960', '1969', '1980', '2', '20', '200', '2003', '2013', '20th', '22', '24', '25', '3', '30', '300', '4', '40', '5', '50', '500', '5th', '6', '60', '7', '70', '8', '80', '9', '90', '911', ':', '?', 'a', \"a's\", 'abacus', 'abandon', 'abandoned', 'abducted', 'abilities', 'ability', 'able', 'aboard', 'about', 'above', 'abroad', 'abrupt', 'absence', 'absent', 'absent-minded', 'absolute', 'absolutely', 'absorb', 'absorbed', 'absorbs', 'absurd', 'abundant', 'abused', 'abusive', 'accent', 'accept', 'acceptable', 'accepted', 'accepting', 'accepts', 'access', 'accident', 'accidentally', 'accidents', 'accompanied', 'accomplice', 'accomplish', 'accomplished', 'accomplishment', 'according']\n",
            "First 10 Italian tokens: ['<sos>', '<eos>', '<pad>', '!', '\"', '$', '%', \"'\", ',', '.', '/', '00', '000', '1', '10', '100', '11', '110', '119', '12', '15', '18', '1939', '1941', '1945', '1960', '1969', '1980', '2', '20', '2003', '2013', '22', '24', '25', '3', '30', '300', '4', '40', '5', '50', '500', '6', '60', '7', '8', '9', '90', '911', ':', '?', 'a', 'abaco', 'abbaia', 'abbaiando', 'abbaiare', 'abbaiato', 'abbaiò', 'abbandonare', 'abbandonata', 'abbandonate', 'abbandonato', 'abbandonerò', 'abbandonò', 'abbassa', 'abbassare', 'abbassate', 'abbassato', 'abbassi', 'abbastanza', 'abbattere', 'abbattuto', 'abbi', 'abbia', 'abbiamo', 'abbiano', 'abbiate', 'abbigliamento', 'abbina', 'abbraccia', 'abbracciare', 'abbracciarono', 'abbracciate', 'abbracciati', 'abbracciato', 'abbraccio', 'abbracciò', 'abile', 'abilità', 'abita', 'abitando', 'abitano', 'abitanti', 'abitare', 'abitasse', 'abitate', 'abitato', 'abitava', 'abitavo']\n",
            "ENG_INVERSE: [('<sos>', 0), ('<eos>', 1), ('<pad>', 2), ('!', 3), ('\"', 4), ('$', 5), ('%', 6), (',', 7), ('.', 8), ('00', 9), ('000', 10), ('1', 11), ('10', 12), ('100', 13), ('11', 14), ('110', 15), ('119', 16), ('12', 17), ('13', 18), ('13-year-old', 19), ('15', 20), ('18', 21), ('1939', 22), ('1941', 23), ('1945', 24), ('1950s', 25), ('1960', 26), ('1969', 27), ('1980', 28), ('2', 29), ('20', 30), ('200', 31), ('2003', 32), ('2013', 33), ('20th', 34), ('22', 35), ('24', 36), ('25', 37), ('3', 38), ('30', 39), ('300', 40), ('4', 41), ('40', 42), ('5', 43), ('50', 44), ('500', 45), ('5th', 46), ('6', 47), ('60', 48), ('7', 49), ('70', 50), ('8', 51), ('80', 52), ('9', 53), ('90', 54), ('911', 55), (':', 56), ('?', 57), ('a', 58), (\"a's\", 59), ('abacus', 60), ('abandon', 61), ('abandoned', 62), ('abducted', 63), ('abilities', 64), ('ability', 65), ('able', 66), ('aboard', 67), ('about', 68), ('above', 69), ('abroad', 70), ('abrupt', 71), ('absence', 72), ('absent', 73), ('absent-minded', 74), ('absolute', 75), ('absolutely', 76), ('absorb', 77), ('absorbed', 78), ('absorbs', 79), ('absurd', 80), ('abundant', 81), ('abused', 82), ('abusive', 83), ('accent', 84), ('accept', 85), ('acceptable', 86), ('accepted', 87), ('accepting', 88), ('accepts', 89), ('access', 90), ('accident', 91), ('accidentally', 92), ('accidents', 93), ('accompanied', 94), ('accomplice', 95), ('accomplish', 96), ('accomplished', 97), ('accomplishment', 98), ('according', 99)]\n",
            "ITA_INVERSE: [('<sos>', 0), ('<eos>', 1), ('<pad>', 2), ('!', 3), ('\"', 4), ('$', 5), ('%', 6), (\"'\", 7), (',', 8), ('.', 9), ('/', 10), ('00', 11), ('000', 12), ('1', 13), ('10', 14), ('100', 15), ('11', 16), ('110', 17), ('119', 18), ('12', 19), ('15', 20), ('18', 21), ('1939', 22), ('1941', 23), ('1945', 24), ('1960', 25), ('1969', 26), ('1980', 27), ('2', 28), ('20', 29), ('2003', 30), ('2013', 31), ('22', 32), ('24', 33), ('25', 34), ('3', 35), ('30', 36), ('300', 37), ('4', 38), ('40', 39), ('5', 40), ('50', 41), ('500', 42), ('6', 43), ('60', 44), ('7', 45), ('8', 46), ('9', 47), ('90', 48), ('911', 49), (':', 50), ('?', 51), ('a', 52), ('abaco', 53), ('abbaia', 54), ('abbaiando', 55), ('abbaiare', 56), ('abbaiato', 57), ('abbaiò', 58), ('abbandonare', 59), ('abbandonata', 60), ('abbandonate', 61), ('abbandonato', 62), ('abbandonerò', 63), ('abbandonò', 64), ('abbassa', 65), ('abbassare', 66), ('abbassate', 67), ('abbassato', 68), ('abbassi', 69), ('abbastanza', 70), ('abbattere', 71), ('abbattuto', 72), ('abbi', 73), ('abbia', 74), ('abbiamo', 75), ('abbiano', 76), ('abbiate', 77), ('abbigliamento', 78), ('abbina', 79), ('abbraccia', 80), ('abbracciare', 81), ('abbracciarono', 82), ('abbracciate', 83), ('abbracciati', 84), ('abbracciato', 85), ('abbraccio', 86), ('abbracciò', 87), ('abile', 88), ('abilità', 89), ('abita', 90), ('abitando', 91), ('abitano', 92), ('abitanti', 93), ('abitare', 94), ('abitasse', 95), ('abitate', 96), ('abitato', 97), ('abitava', 98), ('abitavo', 99)]\n"
          ]
        }
      ],
      "source": [
        "SPECIAL = [\"<sos>\", \"<eos>\", \"<pad>\",\"<unk>\"]  # Added \"<unk>\" for unknown tokens\n",
        "MAXLEN = 50\n",
        "\n",
        "# English vocabulary creation with unique tokens\n",
        "f = open(\"text-eng.txt\")\n",
        "eng_tokens_set = set(SPECIAL)  # Start with special tokens\n",
        "for line in f:\n",
        "    line = line.strip() # Remove leading/trailing whitespace\n",
        "    if line and len(line.split()) <= MAXLEN: # Check if the line is not empty and does not exceed MAXLEN\n",
        "        tokens = line.split() # Split the line into tokens\n",
        "        eng_tokens_set.update(tokens) # Add tokens to the set (automatically handles duplicates)\n",
        "\n",
        "f.close()\n",
        "\n",
        "# Convert set to list for vocabulary\n",
        "ENG_VOCABULARY = list(eng_tokens_set)\n",
        "\n",
        "# Italian vocabulary creation with unique tokens\n",
        "f = open(\"text-ita.txt\")\n",
        "ita_tokens_set = set(SPECIAL)  # Start with special tokens\n",
        "for line in f:\n",
        "    line = line.strip()  # Remove leading/trailing whitespace\n",
        "    if line and len(line.split()) <= MAXLEN:\n",
        "        tokens = line.split()\n",
        "        ita_tokens_set.update(tokens) # Add tokens to the set (automatically handles duplicates)\n",
        "\n",
        "f.close()\n",
        "\n",
        "# Convert set to list for vocabulary\n",
        "ITA_VOCABULARY = list(ita_tokens_set)\n",
        "\n",
        "# Make sure that the three special tokens have the same indices in the two vocabularies.\n",
        "# Sort vocabularies to ensure consistent ordering, with special tokens first\n",
        "ENG_VOCABULARY = SPECIAL + sorted([token for token in ENG_VOCABULARY if token not in SPECIAL])\n",
        "ITA_VOCABULARY = SPECIAL + sorted([token for token in ITA_VOCABULARY if token not in SPECIAL])\n",
        "\n",
        "# Assign the three indices for special tokens\n",
        "SOS = 0  # Index of \"<sos>\"\n",
        "EOS = 1  # Index of \"<eos>\"  \n",
        "PAD = 2  # Index of \"<pad>\"\n",
        "UNK = 3  # Index of \"<unk>\" (unknown token)\n",
        "\n",
        "# Inverse mappings.\n",
        "ENG_INVERSE = {w: n for n, w in enumerate(ENG_VOCABULARY)}\n",
        "ITA_INVERSE = {w: n for n, w in enumerate(ITA_VOCABULARY)}\n",
        "\n",
        "print(f\"English vocabulary size: {len(ENG_VOCABULARY)}\")\n",
        "print(f\"Italian vocabulary size: {len(ITA_VOCABULARY)}\")\n",
        "print(f\"Special token indices - SOS: {SOS}, EOS: {EOS}, PAD: {PAD}, UNK: {UNK}\")\n",
        "print(f\"First 10 English tokens: {ENG_VOCABULARY[:100]}\")\n",
        "print(f\"First 10 Italian tokens: {ITA_VOCABULARY[:100]}\")\n",
        "print(f\"ENG_INVERSE: {list(ENG_INVERSE.items())[:100]}\")\n",
        "print(f\"ITA_INVERSE: {list(ITA_INVERSE.items())[:100]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "account\n",
            "3571\n"
          ]
        }
      ],
      "source": [
        "# usage example\n",
        "print(ENG_VOCABULARY[100]) # Should print \"<sos>\"\n",
        "print(ENG_INVERSE[\"italy\"])  # Should print 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPz15mo4Ibj1"
      },
      "source": [
        "### Encoding/decoding functions\n",
        "\n",
        "We need now functions to map strings with sentences into lists of numerical indices, and vice-versa. Thse functions will take as arguments also the vocabularies, or thweir inverses, so that we can use them for both English and Italian.\n",
        "\n",
        "Having all sequences of the same length simplify training.\n",
        "For this reason, the `encode_sentence` should add padding to make sure that the list of codes include exactly `MAXLEN` elements.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GCElBQ1VIpKV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded length: 50\n",
            "Codes: [0, 2004, 7560, 7271, 4082, 6805, 4016, 1322, 57, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "Decoded: <sos> do you want me to make coffee ? <eos>\n",
            "Encoded length: 50\n",
            "Codes: [0, 9917, 1648, 6772, 2579, 1345, 51, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "Decoded: <sos> vuoi che prepari del caffè ? <eos>\n"
          ]
        }
      ],
      "source": [
        "def encode_sentence(sentence, inverse):\n",
        "    \"\"\"Translate the sentence as a list of numerical codes, given the inverse mapping.\"\"\"\n",
        "    inverse_sentence = []  # goal: append numerical codes for each word\n",
        "    words = sentence.strip().split()  # Split the sentence into words and remove whitespace\n",
        "    \n",
        "    # Convert words to indices\n",
        "    for word in words:\n",
        "        if word in inverse:\n",
        "            inverse_sentence.append(inverse[word])\n",
        "        else:\n",
        "            inverse_sentence.append(UNK)\n",
        "            print(f\"Warning: Unknown word '{word}' skipped\")  # Handle unknown words\n",
        "            continue\n",
        "    \n",
        "    # Add padding to make all sequences the same length (MAXLEN)\n",
        "    while len(inverse_sentence) < MAXLEN:\n",
        "        inverse_sentence.append(PAD)  # PAD = 2\n",
        "    \n",
        "    # Truncate if too long\n",
        "    if len(inverse_sentence) > MAXLEN:\n",
        "        inverse_sentence = inverse_sentence[:MAXLEN]\n",
        "    \n",
        "    return inverse_sentence\n",
        "\n",
        "\n",
        "\n",
        "def decode_sentence(codes, voc):\n",
        "    \"\"\"Translate a list of numerical codes into a sentence, given the mapping.\"\"\"\n",
        "    sentence = []\n",
        "    for code in codes:\n",
        "        if code == PAD:  # Stop at padding\n",
        "            break\n",
        "        sentence.append(voc[code])\n",
        "    return \" \".join(sentence)\n",
        "\n",
        "\n",
        "# Test the functions\n",
        "eng = \"<sos> do you want me to make coffee ? <eos>\"\n",
        "codes = encode_sentence(eng, ENG_INVERSE)\n",
        "print(f\"Encoded length: {len(codes)}\")\n",
        "print(f\"Codes: {codes}\")\n",
        "print(f\"Decoded: {decode_sentence(codes, ENG_VOCABULARY)}\")\n",
        "\n",
        "ita = \"<sos> vuoi che prepari del caffè ? <eos>\"\n",
        "codes = encode_sentence(ita, ITA_INVERSE)\n",
        "print(f\"Encoded length: {len(codes)}\")\n",
        "print(f\"Codes: {codes}\")\n",
        "print(f\"Decoded: {decode_sentence(codes, ITA_VOCABULARY)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8dGgIr2L85f"
      },
      "source": [
        "### Dataset and data loader\n",
        "\n",
        "All the data will be loaded into memory. The `torch.utils.data.TensorDataset` will make the data accessible to the data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6F0oHau0XzvB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 50]) torch.int64 torch.Size([64, 50]) torch.int64\n",
            "<sos> i thought that tom would say that . <eos>\n",
            "<sos> pensavo che tom l'avrebbe detto . <eos>\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "with open(\"text-eng.txt\") as f:\n",
        "    eng_sentences = [encode_sentence(line, ENG_INVERSE) for line in f]\n",
        "\n",
        "with open(\"text-ita.txt\") as f:\n",
        "    ita_sentences = [encode_sentence(line, ITA_INVERSE) for line in f]\n",
        "\n",
        "train_set = torch.utils.data.TensorDataset(torch.tensor(eng_sentences), torch.tensor(ita_sentences))\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "eng, ita = next(iter(train_loader))\n",
        "print(eng.shape, eng.dtype, ita.shape, ita.dtype)\n",
        "\n",
        "print(decode_sentence(eng[0], ENG_VOCABULARY))\n",
        "print(decode_sentence(ita[0], ITA_VOCABULARY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_RimpD7Z0Uv"
      },
      "source": [
        "## Model\n",
        "\n",
        "We will use an encoder-decoder architecture (picture from \"Dive into deep learning\").\n",
        "\n",
        "![link text](https://d2l.ai/_images/seq2seq.svg)\n",
        "\n",
        "The encoder will read the English sentence and encode it into a vector of features (we will use both the final hidden state and cell state).\n",
        "\n",
        "The decoder will output Italian tokens, given the previous one.\n",
        "The encoded input is passed to the decoder as initial state and as additional input at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wZdPRFY3bKCJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([7, 22]) -> torch.Size([2, 7, 256]) torch.Size([2, 7, 256])\n",
            "torch.Size([7, 22]) -> torch.Size([7, 22, 9963])\n"
          ]
        }
      ],
      "source": [
        "DIM = 256\n",
        "DROPOUT = 0.2\n",
        "LAYERS = 2\n",
        "\n",
        "encoder = torch.nn.Sequential(\n",
        "    torch.nn.Embedding(len(ENG_VOCABULARY), DIM),\n",
        "    torch.nn.LSTM(DIM, DIM, batch_first=True, dropout=DROPOUT, num_layers=LAYERS)\n",
        ")\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(len(ITA_VOCABULARY), embedding_size)\n",
        "        self.cell_linear = torch.nn.Linear(hidden_size, embedding_size)\n",
        "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, batch_first=True, dropout=DROPOUT, num_layers=LAYERS)\n",
        "        self.linear = torch.nn.Linear(hidden_size, len(ITA_VOCABULARY))\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        cell_state = hidden[1][-1]\n",
        "        output = self.embedding(input)\n",
        "        y = self.cell_linear(cell_state).unsqueeze(1)\n",
        "        output = output + y\n",
        "        output, _ = self.lstm(output, hidden)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "decoder = Decoder(DIM, DIM)\n",
        "\n",
        "input1 = torch.zeros(7, 22, dtype=torch.long)\n",
        "_, hidden = encoder(input1)\n",
        "print(input1.shape, \"->\", hidden[0].shape, hidden[1].shape)\n",
        "\n",
        "input2 = torch.zeros(7, 22, dtype=torch.long)\n",
        "output = decoder(input2, hidden)\n",
        "print(input2.shape, \"->\", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7LzwL-heBOy"
      },
      "source": [
        "## Training\n",
        "\n",
        "During training the cross entropy is minimized.\n",
        "Each output from the decoder is compared to the next token in the output sequence.\n",
        "\n",
        "Padding should be ignored during training. The `torch.nn.CrossEntropyLoss` has an optional argument for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BQIGRquneFIF"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "DEVICE = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder.to(DEVICE)\n",
        "decoder.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=LEARNING_RATE)\n",
        "loss_fun = torch.nn.CrossEntropyLoss(ignore_index=PAD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9Co2s3veSXN"
      },
      "outputs": [],
      "source": [
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "steps = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    for lq, sq in train_loader:\n",
        "        lq = lq.to(DEVICE)\n",
        "        sq = sq.to(DEVICE)\n",
        "        _, hidden = encoder(lq)\n",
        "        output = decoder(sq[:, :-1], hidden)\n",
        "        loss = loss_fun(output.permute(0, 2, 1), sq[:, 1:])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "        if steps % 1000 == 0:\n",
        "            predictions = output.argmax(2)\n",
        "            correct = (predictions == sq[:, 1:]).sum().item()\n",
        "            total = (sq[:, 1:] != PAD).sum().item()\n",
        "            accuracy = 100 * correct / max(total, 1)\n",
        "            print(f\"{steps} [{epoch}]  Loss: {loss.item():.4f}  Acc: {accuracy:.1f}%\")\n",
        "            print(decode_sentence(lq[0], ENG_VOCABULARY))\n",
        "            print(decode_sentence(sq[0], ITA_VOCABULARY))\n",
        "            print(decode_sentence(predictions[0], ITA_VOCABULARY))\n",
        "            print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB4qZO1RWa-s"
      },
      "source": [
        "## Using the model\n",
        "\n",
        "To translate a new sentence, you need to:\n",
        "\n",
        "1. encode the input sentence;\n",
        "2. initialize the output sentence with the `<sos>` token;\n",
        "3. pass the current output into the decoder together with the encoder state;\n",
        "4. take the output token with the highest score, and add it to the current output.\n",
        "5. repeat from step 3 until the `<eos>` token is generated.\n",
        "\n",
        "Implement this algorithm and use it to translate some English sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWn46pASkOOo"
      },
      "outputs": [],
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "def translate_sentence(sentence, max_length=MAXLEN):\n",
        "    \"\"\"LSTM Encoder-Decoderを使って英語の文をイタリア語に翻訳\"\"\"\n",
        "    with torch.no_grad():\n",
        "        # 1. 入力文をエンコード\n",
        "        input_tensor = torch.tensor([encode_sentence(sentence, ENG_INVERSE)], device=DEVICE)\n",
        "        _, hidden = encoder(input_tensor)\n",
        "        \n",
        "        # 2. 出力文を<sos>で初期化\n",
        "        output = torch.zeros(1, max_length, dtype=torch.long, device=DEVICE)\n",
        "        output[0, 0] = SOS\n",
        "        \n",
        "        # 3. 各ステップで次のトークンを予測\n",
        "        for i in range(1, max_length):\n",
        "            decoder_output = decoder(output[:, :i], hidden)\n",
        "            next_token = decoder_output[0, -1].argmax().item()\n",
        "            output[0, i] = next_token\n",
        "            \n",
        "            # <eos>トークンが生成されたら終了\n",
        "            if next_token == EOS:\n",
        "\n",
        "                break\n",
        "        \n",
        "        # 4. 結果をデコード\n",
        "        return decode_sentence(output[0], ITA_VOCABULARY)\n",
        "\n",
        "# テスト用の英語文\n",
        "test_sentences = [\n",
        "    \"<sos> how old are you ? <eos>\",\n",
        "    \"<sos> i like to play tennis . <eos>\",\n",
        "    \"<sos> i hope it snows at christmas . <eos>\",\n",
        "    \"<sos> would you like to go to the movie theater . <eos>\"\n",
        "]\n",
        "\n",
        "print(\"=== LSTM Translation Results ===\")\n",
        "for eng in test_sentences:\n",
        "    ita = translate_sentence(eng)\n",
        "    print(f\"EN: {eng}\")\n",
        "    print(f\"IT: {ita}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM vs BERT Performance Comparison\n",
        "\n",
        "この翻訳タスクにおいて、LSTM Encoder-DecoderとBERTベースのモデルの性能を比較します。\n",
        "\n",
        "## 比較の観点\n",
        "\n",
        "1. **翻訳品質** - BLEU Score, 人間評価\n",
        "2. **学習効率** - 収束速度, 必要エポック数\n",
        "3. **計算効率** - 推論時間, メモリ使用量\n",
        "4. **モデルサイズ** - パラメータ数\n",
        "\n",
        "## 実装方針\n",
        "\n",
        "### LSTM Encoder-Decoder (上記で実装済み)\n",
        "- ✅ カスタム語彙ベース\n",
        "- ✅ Sequence-to-Sequence アーキテクチャ\n",
        "- ✅ 軽量で高速\n",
        "\n",
        "### BERT-based Translation\n",
        "- 🔄 事前学習済みBERTを利用\n",
        "- 🔄 mBERT (Multilingual BERT) または専用モデル\n",
        "- 🔄 Transformerアーキテクチャ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT Implementation Setup\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "import time\n",
        "\n",
        "# BERTベースの翻訳モデル用設定\n",
        "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-it\"  # 英語→イタリア語の事前学習済みモデル\n",
        "\n",
        "print(\"Available BERT-based models for EN-IT translation:\")\n",
        "print(\"1. Helsinki-NLP/opus-mt-en-it (MarianMT)\")\n",
        "print(\"2. facebook/mbart-large-50-many-to-many-mmt\")\n",
        "print(\"3. google/mt5-small\")\n",
        "\n",
        "# GPU使用可能性の確認\n",
        "print(f\"\\nDevice: {DEVICE}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT Model and Tokenizer Setup\n",
        "try:\n",
        "    # MarianMTモデル（英語→イタリア語）を読み込み\n",
        "    bert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    bert_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "    bert_model.to(DEVICE)\n",
        "    \n",
        "    print(f\"✅ Model loaded: {MODEL_NAME}\")\n",
        "    print(f\"Model parameters: {bert_model.num_parameters():,}\")\n",
        "    \n",
        "    # トークナイザーのテスト\n",
        "    test_input = \"How old are you?\"\n",
        "    tokens = bert_tokenizer(test_input, return_tensors=\"pt\")\n",
        "    print(f\"Tokenizer test: '{test_input}' -> {tokens['input_ids'].shape[1]} tokens\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading BERT model: {e}\")\n",
        "    print(\"Installing transformers...\")\n",
        "    # Transformersがインストールされていない場合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT Data Preprocessing\n",
        "def prepare_bert_data():\n",
        "    \"\"\"既存のデータをBERT用の形式に変換\"\"\"\n",
        "    \n",
        "    # ファイルから生データを読み込み\n",
        "    with open(\"text-eng.txt\", 'r') as f:\n",
        "        eng_raw = [line.strip() for line in f if line.strip()]\n",
        "    \n",
        "    with open(\"text-ita.txt\", 'r') as f:\n",
        "        ita_raw = [line.strip() for line in f if line.strip()]\n",
        "    \n",
        "    # <sos>, <eos>タグを除去（BERTは自動で処理）\n",
        "    eng_clean = [sent.replace(\"<sos>\", \"\").replace(\"<eos>\", \"\").strip() for sent in eng_raw]\n",
        "    ita_clean = [sent.replace(\"<sos>\", \"\").replace(\"<eos>\", \"\").strip() for sent in ita_raw]\n",
        "    \n",
        "    # 空の文や極端に長い文を除外\n",
        "    valid_pairs = []\n",
        "    for eng, ita in zip(eng_clean, ita_clean):\n",
        "        if eng and ita and len(eng.split()) <= 30 and len(ita.split()) <= 30:\n",
        "            valid_pairs.append({\"english\": eng, \"italian\": ita})\n",
        "    \n",
        "    print(f\"Valid pairs: {len(valid_pairs):,}\")\n",
        "    print(f\"Sample data:\")\n",
        "    for i in range(3):\n",
        "        print(f\"  EN: {valid_pairs[i]['english']}\")\n",
        "        print(f\"  IT: {valid_pairs[i]['italian']}\")\n",
        "        print()\n",
        "    \n",
        "    return valid_pairs\n",
        "\n",
        "# データの準備\n",
        "bert_data = prepare_bert_data()\n",
        "\n",
        "# Train/Validation split (80/20)\n",
        "split_idx = int(0.8 * len(bert_data))\n",
        "train_data = bert_data[:split_idx]\n",
        "val_data = bert_data[split_idx:]\n",
        "\n",
        "print(f\"Training samples: {len(train_data):,}\")\n",
        "print(f\"Validation samples: {len(val_data):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT Translation Function\n",
        "def translate_with_bert(sentence, model, tokenizer, max_length=50):\n",
        "    \"\"\"BERTを使って英語の文をイタリア語に翻訳\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # トークン化\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "        \n",
        "        # 翻訳生成\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            do_sample=False\n",
        "        )\n",
        "        \n",
        "        # デコード\n",
        "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return translation\n",
        "\n",
        "# Performance Comparison Function\n",
        "def compare_translations(test_sentences):\n",
        "    \"\"\"LSTMとBERTの翻訳を比較\"\"\"\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"TRANSLATION COMPARISON: LSTM vs BERT\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    lstm_times = []\n",
        "    bert_times = []\n",
        "    \n",
        "    for i, eng_sentence in enumerate(test_sentences, 1):\n",
        "        print(f\"\\n--- Test {i} ---\")\n",
        "        \n",
        "        # 入力文（<sos>, <eos>を除去）\n",
        "        clean_eng = eng_sentence.replace(\"<sos>\", \"\").replace(\"<eos>\", \"\").strip()\n",
        "        print(f\"Input (EN): {clean_eng}\")\n",
        "        \n",
        "        # LSTM翻訳\n",
        "        start_time = time.time()\n",
        "        lstm_result = translate_sentence(eng_sentence)\n",
        "        lstm_time = time.time() - start_time\n",
        "        lstm_times.append(lstm_time)\n",
        "        print(f\"LSTM (IT):  {lstm_result} ({lstm_time:.3f}s)\")\n",
        "        \n",
        "        # BERT翻訳\n",
        "        start_time = time.time()\n",
        "        bert_result = translate_with_bert(clean_eng, bert_model, bert_tokenizer)\n",
        "        bert_time = time.time() - start_time\n",
        "        bert_times.append(bert_time)\n",
        "        print(f\"BERT (IT):  {bert_result} ({bert_time:.3f}s)\")\n",
        "    \n",
        "    # 平均処理時間の比較\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PERFORMANCE SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Average LSTM time: {np.mean(lstm_times):.3f}s (±{np.std(lstm_times):.3f}s)\")\n",
        "    print(f\"Average BERT time: {np.mean(bert_times):.3f}s (±{np.std(bert_times):.3f}s)\")\n",
        "    print(f\"Speed ratio (BERT/LSTM): {np.mean(bert_times)/np.mean(lstm_times):.2f}x\")\n",
        "\n",
        "# テスト実行\n",
        "test_sentences = [\n",
        "    \"<sos> how old are you ? <eos>\",\n",
        "    \"<sos> i like to play tennis . <eos>\",\n",
        "    \"<sos> i hope it snows at christmas . <eos>\",\n",
        "    \"<sos> would you like to go to the movie theater . <eos>\",\n",
        "    \"<sos> the weather is beautiful today . <eos>\"\n",
        "]\n",
        "\n",
        "# 比較実行\n",
        "compare_translations(test_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 詳細な性能評価\n",
        "\n",
        "### 評価指標\n",
        "\n",
        "1. **BLEU Score**: 翻訳品質の自動評価\n",
        "2. **Translation Speed**: 推論速度（sentences/second）\n",
        "3. **Memory Usage**: GPU/CPUメモリ使用量\n",
        "4. **Model Size**: パラメータ数とモデルサイズ\n",
        "\n",
        "### 期待される結果\n",
        "\n",
        "**LSTM Encoder-Decoder**\n",
        "- ✅ **速度**: 高速（軽量アーキテクチャ）\n",
        "- ⚠️ **品質**: 限定的（語彙サイズとコンテキスト）\n",
        "- ✅ **メモリ**: 低使用量\n",
        "- ✅ **学習**: 高速収束\n",
        "\n",
        "**BERT-based (MarianMT)**\n",
        "- ⚠️ **速度**: 低速（大規模Transformer）\n",
        "- ✅ **品質**: 高品質（事前学習済み）\n",
        "- ⚠️ **メモリ**: 高使用量\n",
        "- ✅ **学習**: 事前学習済みで高性能"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed Performance Evaluation\n",
        "import psutil\n",
        "import gc\n",
        "from collections import defaultdict\n",
        "\n",
        "def calculate_bleu_score(predictions, references):\n",
        "    \"\"\"簡易BLEU計算（実際のプロジェクトではsacrebleuを推奨）\"\"\"\n",
        "    try:\n",
        "        from nltk.translate.bleu_score import sentence_bleu\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        import nltk\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        \n",
        "        total_score = 0\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            pred_tokens = word_tokenize(pred.lower())\n",
        "            ref_tokens = [word_tokenize(ref.lower())]\n",
        "            score = sentence_bleu(ref_tokens, pred_tokens)\n",
        "            total_score += score\n",
        "        \n",
        "        return total_score / len(predictions)\n",
        "    except ImportError:\n",
        "        print(\"NLTK not available, using simple word overlap metric\")\n",
        "        total_score = 0\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            pred_words = set(pred.lower().split())\n",
        "            ref_words = set(ref.lower().split())\n",
        "            if len(ref_words) > 0:\n",
        "                overlap = len(pred_words & ref_words) / len(ref_words)\n",
        "                total_score += overlap\n",
        "        return total_score / len(predictions)\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"現在のメモリ使用量を取得\"\"\"\n",
        "    process = psutil.Process()\n",
        "    cpu_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
        "    \n",
        "    gpu_memory = 0\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n",
        "    \n",
        "    return {\"CPU\": cpu_memory, \"GPU\": gpu_memory}\n",
        "\n",
        "def comprehensive_evaluation():\n",
        "    \"\"\"包括的な性能評価\"\"\"\n",
        "    print(\"🔍 COMPREHENSIVE PERFORMANCE EVALUATION\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # テストデータの準備\n",
        "    test_data = val_data[:50]  # 50サンプルで評価\n",
        "    eng_sentences = [f\"<sos> {item['english']} <eos>\" for item in test_data]\n",
        "    eng_clean = [item['english'] for item in test_data]\n",
        "    ita_references = [item['italian'] for item in test_data]\n",
        "    \n",
        "    # メモリ使用量（開始時）\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    start_memory = get_memory_usage()\n",
        "    \n",
        "    print(f\"📊 Evaluating on {len(test_data)} samples...\")\n",
        "    print(f\"💾 Initial memory: CPU {start_memory['CPU']:.1f}MB, GPU {start_memory['GPU']:.1f}MB\")\n",
        "    \n",
        "    # === LSTM評価 ===\n",
        "    print(\"\\n🤖 LSTM Evaluation...\")\n",
        "    lstm_start_time = time.time()\n",
        "    lstm_predictions = []\n",
        "    \n",
        "    for eng_sentence in eng_sentences:\n",
        "        pred = translate_sentence(eng_sentence)\n",
        "        # <sos>, <eos>を除去\n",
        "        clean_pred = pred.replace(\"<sos>\", \"\").replace(\"<eos>\", \"\").strip()\n",
        "        lstm_predictions.append(clean_pred)\n",
        "    \n",
        "    lstm_total_time = time.time() - lstm_start_time\n",
        "    lstm_memory = get_memory_usage()\n",
        "    \n",
        "    # === BERT評価 ===\n",
        "    print(\"\\n🧠 BERT Evaluation...\")\n",
        "    bert_start_time = time.time()\n",
        "    bert_predictions = []\n",
        "    \n",
        "    for eng_sentence in eng_clean:\n",
        "        pred = translate_with_bert(eng_sentence, bert_model, bert_tokenizer)\n",
        "        bert_predictions.append(pred)\n",
        "    \n",
        "    bert_total_time = time.time() - bert_start_time\n",
        "    bert_memory = get_memory_usage()\n",
        "    \n",
        "    # === 結果計算 ===\n",
        "    lstm_bleu = calculate_bleu_score(lstm_predictions, ita_references)\n",
        "    bert_bleu = calculate_bleu_score(bert_predictions, ita_references)\n",
        "    \n",
        "    # === 結果表示 ===\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"📈 FINAL RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    results = {\n",
        "        \"LSTM\": {\n",
        "            \"BLEU Score\": f\"{lstm_bleu:.4f}\",\n",
        "            \"Total Time\": f\"{lstm_total_time:.2f}s\",\n",
        "            \"Speed\": f\"{len(test_data)/lstm_total_time:.1f} sent/s\",\n",
        "            \"Memory (CPU)\": f\"{lstm_memory['CPU']:.1f}MB\",\n",
        "            \"Memory (GPU)\": f\"{lstm_memory['GPU']:.1f}MB\",\n",
        "        },\n",
        "        \"BERT\": {\n",
        "            \"BLEU Score\": f\"{bert_bleu:.4f}\",\n",
        "            \"Total Time\": f\"{bert_total_time:.2f}s\", \n",
        "            \"Speed\": f\"{len(test_data)/bert_total_time:.1f} sent/s\",\n",
        "            \"Memory (CPU)\": f\"{bert_memory['CPU']:.1f}MB\",\n",
        "            \"Memory (GPU)\": f\"{bert_memory['GPU']:.1f}MB\",\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    for model_name, metrics in results.items():\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"  {metric:12}: {value}\")\n",
        "    \n",
        "    # 勝者判定\n",
        "    print(f\"\\n🏆 WINNER:\")\n",
        "    print(f\"  Quality (BLEU):  {'BERT' if bert_bleu > lstm_bleu else 'LSTM'}\")\n",
        "    print(f\"  Speed:           {'LSTM' if len(test_data)/lstm_total_time > len(test_data)/bert_total_time else 'BERT'}\")\n",
        "    print(f\"  Efficiency:      {'LSTM' if lstm_memory['GPU'] < bert_memory['GPU'] else 'BERT'}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# 評価実行\n",
        "if 'bert_model' in locals():\n",
        "    evaluation_results = comprehensive_evaluation()\n",
        "else:\n",
        "    print(\"❌ BERT model not loaded. Please run the previous cells first.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
