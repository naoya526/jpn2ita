import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.utils.data import DataLoader

# I tried to implement a BERT model from scratch using PyTorch.

# 1. ã¾ãšåŸºæœ¬çš„ãªãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰
class ShowMultiHeadAttention(nn.Module):
    """
    ãƒ’ãƒ³ãƒˆ: 
    - Query, Key, Value ã®3ã¤ã®ç·šå½¢å¤‰æ›ãŒå¿…è¦
    - ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆç©ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: softmax(QK^T / sqrt(d_k))V
    - è¤‡æ•°ã®ãƒ˜ãƒƒãƒ‰ã‚’ä¸¦åˆ—å®Ÿè¡Œå¾Œã€concatenate
    - æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›ã§å‡ºåŠ›æ¬¡å…ƒã‚’èª¿æ•´
    """
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__() # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads  # å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒ
        self.dropout = torch.nn.Dropout(dropout)
        
        # Q, K, V ã®ç·šå½¢å¤‰æ›ï¼ˆä¿®æ­£ï¼štorch.nn.linear â†’ torch.nn.Linearï¼‰
        self.query = torch.nn.Linear(d_model, d_model)
        self.key = torch.nn.Linear(d_model, d_model)
        self.value = torch.nn.Linear(d_model, d_model)
        
        # æœ€çµ‚çš„ãªå‡ºåŠ›å¤‰æ›
        self.out_proj = torch.nn.Linear(d_model, d_model)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: Q, K, V ã‚’ç·šå½¢å¤‰æ›ã§ç”Ÿæˆ
        query = self.query(x)  # (batch, seq_len, d_model)
        key = self.key(x)      # (batch, seq_len, d_model)
        value = self.value(x)  # (batch, seq_len, d_model)
        
        print(f"Before View: Q={query.shape}, K={key.shape}, V={value.shape}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: Multi-Headç”¨ã«æ¬¡å…ƒã‚’å¤‰å½¢
        # ç†ç”±1: d_model ã‚’ num_heads å€‹ã®ãƒ˜ãƒƒãƒ‰ã«åˆ†å‰²
        # (batch, seq_len, d_model) â†’ (batch, seq_len, num_heads, head_dim)
        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim)
        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim)  # ä¿®æ­£: query.shape â†’ batch_size
        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        print(f"After View: Q={query.shape}, K={key.shape}, V={value.shape}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: æ¬¡å…ƒã®é †åºã‚’å¤‰æ›´ï¼ˆã“ã‚ŒãŒè³ªå•ã®ãƒã‚¤ãƒ³ãƒˆï¼ï¼‰
        # ç†ç”±2: ãƒãƒƒãƒä¸¦åˆ—å‡¦ç†ã®ãŸã‚ (batch, num_heads, seq_len, head_dim)
        query = query.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, head_dim)
        key = key.permute(0, 2, 1, 3)
        value = value.permute(0, 2, 1, 3)
        
        print(f"After Permute: Q={query.shape}, K={key.shape}, V={value.shape}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: Scaled Dot-Product Attention
        # scores = Q @ K^T / sqrt(d_k)
        # (batch, num_heads, seq_len, head_dim) @ (batch, num_heads, head_dim, seq_len)
        # â†’ (batch, num_heads, seq_len, seq_len)
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)
        print(f"scores: {scores.shape}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: ãƒã‚¹ã‚¯å‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # ã‚¹ãƒ†ãƒƒãƒ—6: Softmax + Dropout
        weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)
        weights = self.dropout(weights)
        
        # ã‚¹ãƒ†ãƒƒãƒ—7: Value ã¨ã®ç©
        # (batch, num_heads, seq_len, seq_len) @ (batch, num_heads, seq_len, head_dim)
        # â†’ (batch, num_heads, seq_len, head_dim)
        context = torch.matmul(weights, value)
        print(f"context: {context.shape}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—8: ãƒ˜ãƒƒãƒ‰ã‚’çµåˆã—ã¦å…ƒã®å½¢çŠ¶ã«æˆ»ã™
        # (batch, num_heads, seq_len, head_dim) â†’ (batch, seq_len, num_heads, head_dim)
        context = context.permute(0, 2, 1, 3)
        # â†’ (batch, seq_len, d_model)
        context = context.contiguous().view(batch_size, seq_len, self.num_heads * self.head_dim)
        print(f"Result: {context.shape}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—9: æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›
        return self.out_proj(context)  # ä¿®æ­£: output_linear â†’ out_proj
class MultiHeadAttention(nn.Module):
    """
    ãƒ’ãƒ³ãƒˆ: 
    - Query, Key, Value ã®3ã¤ã®ç·šå½¢å¤‰æ›ãŒå¿…è¦
    - ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆç©ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: softmax(QK^T / sqrt(d_k))V
    - è¤‡æ•°ã®ãƒ˜ãƒƒãƒ‰ã‚’ä¸¦åˆ—å®Ÿè¡Œå¾Œã€concatenate
    - æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›ã§å‡ºåŠ›æ¬¡å…ƒã‚’èª¿æ•´
    """
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__() # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads  # å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒ
        self.dropout = torch.nn.Dropout(dropout)
        
        # Q, K, V ã®ç·šå½¢å¤‰æ›ï¼ˆä¿®æ­£ï¼štorch.nn.linear â†’ torch.nn.Linearï¼‰
        self.query = torch.nn.Linear(d_model, d_model)
        self.key = torch.nn.Linear(d_model, d_model)
        self.value = torch.nn.Linear(d_model, d_model)
        
        # æœ€çµ‚çš„ãªå‡ºåŠ›å¤‰æ›
        self.out_proj = torch.nn.Linear(d_model, d_model)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: Q, K, V ã‚’ç·šå½¢å¤‰æ›ã§ç”Ÿæˆ
        query = self.query(x)  # (batch, seq_len, d_model)
        key = self.key(x)      # (batch, seq_len, d_model)
        value = self.value(x)  # (batch, seq_len, d_model)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: Multi-Headç”¨ã«æ¬¡å…ƒã‚’å¤‰å½¢
        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim)
        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim)  # ä¿®æ­£: query.shape â†’ batch_size
        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: æ¬¡å…ƒã®é †åºã‚’å¤‰æ›´ï¼ˆã“ã‚ŒãŒè³ªå•ã®ãƒã‚¤ãƒ³ãƒˆï¼ï¼‰
        query = query.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, head_dim)
        key = key.permute(0, 2, 1, 3)
        value = value.permute(0, 2, 1, 3)
                
        # ã‚¹ãƒ†ãƒƒãƒ—4: Scaled Dot-Product Attention
        # scores = Q @ K^T / sqrt(d_k)
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: ãƒã‚¹ã‚¯å‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # ã‚¹ãƒ†ãƒƒãƒ—6: Softmax + Dropout
        weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)
        weights = self.dropout(weights)
        # ã‚¹ãƒ†ãƒƒãƒ—7: Value ã¨ã®ç©
        context = torch.matmul(weights, value)    
        # ã‚¹ãƒ†ãƒƒãƒ—8: ãƒ˜ãƒƒãƒ‰ã‚’çµåˆã—ã¦å…ƒã®å½¢çŠ¶ã«æˆ»ã™
        context = context.permute(0, 2, 1, 3)
        # â†’ (batch, seq_len, d_model)
        context = context.contiguous().view(batch_size, seq_len, self.num_heads * self.head_dim)
    
        # ã‚¹ãƒ†ãƒƒãƒ—9: æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›
        return self.out_proj(context)  # ä¿®æ­£: output_linear â†’ out_proj
    
class PositionwiseFeedForward(nn.Module):
    """
    ãƒ’ãƒ³ãƒˆ:
    - 2å±¤ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    - ä¸­é–“å±¤ã§ã¯æ¬¡å…ƒã‚’æ‹¡å¼µï¼ˆé€šå¸¸4å€ï¼‰
    - GELUæ´»æ€§åŒ–é–¢æ•°ã‚’ä½¿ç”¨
    - ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚‚å¿˜ã‚Œãšã«
    """
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)  # å…¥åŠ›æ¬¡å…ƒ â†’ ä¸­é–“æ¬¡å…ƒ
        self.linear2 = nn.Linear(d_ff, d_model)  # ä¸­é–“æ¬¡å…ƒ
        self.dropout = nn.Dropout(dropout)
        self.gelu = nn.GELU()

    def forward(self, x):
        # ğŸ” FeedForward ã®æ­£ã—ã„é †åºã¨æ´»æ€§åŒ–ã«ã¤ã„ã¦
        
        # 1. ç¬¬1å±¤: d_model â†’ d_ff (æ¬¡å…ƒæ‹¡å¼µ)
        x = self.linear1(x)          # ç·šå½¢å¤‰æ›
        x = self.gelu(x)             # æ´»æ€§åŒ–é–¢æ•° (ä¸­é–“å±¤ã®ã¿)
        x = self.dropout(x)          # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        
        # 2. ç¬¬2å±¤: d_ff â†’ d_model (æ¬¡å…ƒå¾©å…ƒ)
        x = self.linear2(x)          # ç·šå½¢å¤‰æ›
        x = self.dropout(x)          # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        
        
        return x  # å‡ºåŠ›æ¬¡å…ƒã¯å…ƒã®d_modelã«æˆ»ã™
        
        # ğŸ’¡ ãªãœæœ€å¾Œã«æ´»æ€§åŒ–ã—ãªã„ã®ã‹ï¼Ÿ
        # 1. Residual Connection: x + FFN(x) ã§åŠ ç®—ã™ã‚‹ãŸã‚
        # 2. è¡¨ç¾ã®æŸ”è»Ÿæ€§: è² ã®å€¤ã‚‚é‡è¦ãªæƒ…å ±
        # 3. Transformerè¨­è¨ˆ: æœ€çµ‚çš„ã«ã¯LayerNormãŒæ­£è¦åŒ–
        

class TransformerBlock(nn.Module):
    """
    ãƒ’ãƒ³ãƒˆ:
    - Multi-Head Attention + Residual Connection + Layer Norm
    - Feed Forward + Residual Connection + Layer Norm
    - é †åºã«æ³¨æ„: Pre-LN vs Post-LN
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        # TODO: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã€FFNã€æ­£è¦åŒ–å±¤ã‚’çµ„ã¿åˆã‚ã›
        pass
    
    def forward(self, x, mask=None):
        # TODO: Transformer blockã®å‡¦ç†ã‚’å®Ÿè£…
        pass

class BertEmbeddings(nn.Module):
    """
    ãƒ’ãƒ³ãƒˆ:
    - Token Embeddings (èªå½™ã‚µã‚¤ã‚º Ã— d_model)
    - Position Embeddings (æœ€å¤§ç³»åˆ—é•· Ã— d_model) 
    - Segment Embeddings (2 Ã— d_model, NSPã‚¿ã‚¹ã‚¯ç”¨)
    - 3ã¤ã‚’è¶³ã—åˆã‚ã›ã¦LayerNormã¨Dropout
    """
    def __init__(self, vocab_size, d_model, max_seq_len=512, dropout=0.1):
        super().__init__()
        # TODO: 3ç¨®é¡ã®åŸ‹ã‚è¾¼ã¿ã‚’å®Ÿè£…
        pass
    
    def forward(self, input_ids, token_type_ids=None):
        # TODO: åŸ‹ã‚è¾¼ã¿ã®è¨ˆç®—ã‚’å®Ÿè£…
        pass

class Bert(nn.Module):
    """
    BERTå®Ÿè£…ã®æœ€çµ‚å½¢
    
    å­¦ç¿’ã®ãƒ’ãƒ³ãƒˆ:
    1. è«–æ–‡ã‚’èª­ã‚“ã§å…¨ä½“åƒã‚’ç†è§£
    2. å°ã•ãªéƒ¨å“ã‹ã‚‰å®Ÿè£…ï¼ˆAttention â†’ FFN â†’ Block â†’ Full Modelï¼‰
    3. å„å±¤ã§ print(tensor.shape) ã—ã¦ã‚µã‚¤ã‚ºã‚’ç¢ºèª
    4. ç°¡å˜ãªãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
    5. äº‹å‰å­¦ç¿’ã¯è¨ˆç®—é‡ãŒå¤§ãã„ã®ã§ã€å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é–‹å§‹
    
    é‡è¦ãªæ¦‚å¿µ:
    - Bidirectional: å·¦å³ä¸¡æ–¹å‘ã®æ–‡è„ˆã‚’è¦‹ã‚‹
    - Masked Language Model: ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¹ã‚¯ã—ãŸå˜èªã‚’äºˆæ¸¬
    - Next Sentence Prediction: 2ã¤ã®æ–‡ãŒé€£ç¶šã™ã‚‹ã‹ã‚’äºˆæ¸¬
    - Attention Weights: ã©ã®å˜èªã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã®å¯è¦–åŒ–
    """
    
    def __init__(self, vocab_size, d_model=768, num_layers=12, num_heads=12, d_ff=3072, max_seq_len=512, dropout=0.1):
        super().__init__()
        # TODO: BERTã®å…¨ä½“æ§‹é€ ã‚’å®Ÿè£…
        pass
    
    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        # TODO: BERTå…¨ä½“ã®forward passã‚’å®Ÿè£…
        pass

# main
def main():
    x = torch.randn(2, 10, 512)  # (batch, seq, d_model)
    attn = ShowMultiHeadAttention(512, 8)
    out = attn(x)
    assert out.shape == x.shape
    return 0

main()

# å®Ÿè£…ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ:
"""
ğŸ” æ®µéšçš„ãªãƒ†ã‚¹ãƒˆæ–¹æ³•:

1. MultiHeadAttentionå˜ä½“ãƒ†ã‚¹ãƒˆ:
   x = torch.randn(2, 10, 512)  # (batch, seq, d_model)
   attn = MultiHeadAttention(512, 8)
   out = attn(x)
   assert out.shape == x.shape

2. TransformerBlockå˜ä½“ãƒ†ã‚¹ãƒˆ:
   block = TransformerBlock(512, 8, 2048)
   out = block(x)
   assert out.shape == x.shape

3. BERTå…¨ä½“ãƒ†ã‚¹ãƒˆ:
   bert = Bert(vocab_size=30000, d_model=512, num_layers=6, num_heads=8)
   input_ids = torch.randint(0, 30000, (2, 50))
   out = bert(input_ids)
   print(f"Output shape: {out.shape}")

ğŸ“š å‚è€ƒã«ãªã‚‹å®Ÿè£…ã®ãƒ’ãƒ³ãƒˆ:
- "Attention Is All You Need" (TransformeråŸè«–æ–‡)
- "BERT: Pre-training of Deep Bidirectional Transformers" (BERTè«–æ–‡)
- PyTorchã® nn.TransformerEncoder ã®å®Ÿè£…ã‚’å‚è€ƒã«ï¼ˆãŸã ã—ã‚³ãƒ”ãƒ¼ã—ãªã„ï¼‰

ğŸ¯ å®Ÿè£…å¾Œã®èª²é¡Œ:
1. å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§MLMã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’
2. Attention weightsã®å¯è¦–åŒ–
3. [CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®åˆ†é¡æ€§èƒ½è©•ä¾¡
"""

# ==========================================
# ğŸ› ï¸ å®Ÿè£…ã®é“ã—ã‚‹ã¹ - è©³ç´°ã‚¬ã‚¤ãƒ‰
# ==========================================

"""
ğŸ“– STEP 1: Multi-Head Attention ã‚’ç†è§£ãƒ»å®Ÿè£…

æ ¸å¿ƒçš„ãªè³ªå•:
Q1. ãªãœã€ŒMultiã€Head ãªã®ã‹ï¼Ÿ
â†’ ç•°ãªã‚‹ç¨®é¡ã®é–¢ä¿‚æ€§ï¼ˆæ§‹æ–‡çš„ã€æ„å‘³çš„ï¼‰ã‚’ä¸¦åˆ—ã§æ‰ãˆã‚‹ãŸã‚

Q2. Scaled Dot-Product Attention ã®å¼ã¯ï¼Ÿ
â†’ Attention(Q,K,V) = softmax(QK^T / âˆšd_k)V

Q3. Maskingã¯ãªãœå¿…è¦ï¼Ÿ
â†’ PADãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®attentionã‚’é˜²ããŸã‚ï¼ˆ-infã‚’ä»£å…¥ã—ã¦softmaxâ†’0ï¼‰

å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ:
- d_model ã‚’ num_heads ã§å‰²ã‚Šåˆ‡ã‚Œã‚‹å¿…è¦ãŒã‚ã‚‹
- reshapeæ“ä½œã§(batch, seq, num_heads, head_dim)ã«å¤‰å½¢
- Einstein notation (torch.einsum) ãŒä¾¿åˆ©
- attention_weightsã‚’ä¿å­˜ã™ã‚‹ã¨å¯è¦–åŒ–ã«ä¾¿åˆ©
"""

class AttentionVisualization:
    """
    Attentioné‡ã¿ã®å¯è¦–åŒ–ç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    
    ä½¿ç”¨ä¾‹:
    viz = AttentionVisualization()
    viz.plot_attention(attention_weights, tokens, head_idx=0)
    """
    @staticmethod
    def plot_attention(attention_weights, tokens, head_idx=0):
        # TODO: matplotlib/seabornã§heatmapæç”»
        # ãƒ’ãƒ³ãƒˆ: attention_weights.shape = (batch, num_heads, seq_len, seq_len)
        pass

"""
ğŸ“– STEP 2: Position Embeddings ã®é‡è¦æ€§

æ ¸å¿ƒçš„ãªè³ªå•:
Q1. ãªãœä½ç½®æƒ…å ±ãŒå¿…è¦ï¼Ÿ
â†’ Transformerã¯é †åºã‚’è€ƒæ…®ã—ãªã„ã€‚ã€ŒI love youã€ã¨ã€ŒYou love Iã€ãŒåŒã˜ã«ãªã£ã¦ã—ã¾ã†

Q2. å­¦ç¿’å¯èƒ½ vs å›ºå®šå¼ ã®ã©ã¡ã‚‰ã‚’é¸ã¶ï¼Ÿ
â†’ BERTã¯å­¦ç¿’å¯èƒ½ã€‚GPTã¯å›ºå®šå¼ï¼ˆsin/cosï¼‰ã€‚ã©ã¡ã‚‰ã§ã‚‚å®Ÿè£…å¯èƒ½

Q3. çµ¶å¯¾ä½ç½® vs ç›¸å¯¾ä½ç½®ï¼Ÿ
â†’ BERTã¯çµ¶å¯¾ä½ç½®ã€‚æœ€è¿‘ã®ç ”ç©¶ã§ã¯ç›¸å¯¾ä½ç½®ã‚‚æ³¨ç›®

å®Ÿè£…ã®ã‚³ãƒ„:
- max_seq_lenã¾ã§ã®ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’äº‹å‰ã«ä½œæˆ
- position_ids = torch.arange(seq_len) ã§ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”Ÿæˆ
"""

"""
ğŸ“– STEP 3: Layer Normalization ã®é…ç½®

æ ¸å¿ƒçš„ãªè³ªå•:  
Q1. Pre-LN vs Post-LN ã®é•ã„ã¯ï¼Ÿ
â†’ Pre-LN: å­¦ç¿’ãŒå®‰å®šã€å‹¾é…æ¶ˆå¤±ãŒèµ·ãã«ãã„ï¼ˆæœ€è¿‘ã®ä¸»æµï¼‰
â†’ Post-LN: åŸè«–æ–‡é€šã‚Šã€ã—ã‹ã—æ·±ã„å±¤ã§ä¸å®‰å®š

Q2. BatchNorm vs LayerNormï¼Ÿ
â†’ LayerNorm: ç³»åˆ—é•·ãŒå¯å¤‰ã§ã‚‚å®‰å®š
â†’ BatchNorm: ãƒãƒƒãƒã‚µã‚¤ã‚ºã«ä¾å­˜

å®Ÿè£…ä¾‹:
# Pre-LN (æ¨å¥¨)
residual = x
x = self.layer_norm1(x)
x = self.attention(x)
x = x + residual

# Post-LN (åŸè«–æ–‡)
residual = x  
x = self.attention(x)
x = self.layer_norm1(x + residual)
"""

"""
ğŸ“– STEP 4: BERTã®äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯

MLM (Masked Language Model):
- 15%ã®å˜èªã‚’ãƒã‚¹ã‚¯: 80%â†’[MASK], 10%â†’ãƒ©ãƒ³ãƒ€ãƒ å˜èª, 10%â†’ãã®ã¾ã¾
- CrossEntropyLossã§ãƒã‚¹ã‚¯ã•ã‚ŒãŸä½ç½®ã®ã¿ã‚’äºˆæ¸¬

NSP (Next Sentence Prediction):  
- 50%ã¯é€£ç¶šã™ã‚‹æ–‡ã€50%ã¯ãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡
- [CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨ç¾ã§äºŒå€¤åˆ†é¡

å®Ÿè£…ã®ãƒ’ãƒ³ãƒˆ:
class BertForPreTraining(nn.Module):
    def __init__(self, bert_model):
        super().__init__()
        self.bert = bert_model
        self.mlm_head = nn.Linear(bert_model.d_model, bert_model.vocab_size)
        self.nsp_head = nn.Linear(bert_model.d_model, 2)
    
    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):
        # TODO: MLMã¨NSPã®æå¤±ã‚’è¨ˆç®—
        pass
"""

"""
ğŸ“– STEP 5: ãƒ‡ãƒãƒƒã‚°ã¨ãƒ†ã‚¹ãƒˆã®æˆ¦ç•¥

ğŸ› ã‚ˆãã‚ã‚‹ãƒã‚°ã¨å¯¾å‡¦æ³•:

1. Shape Mismatch:
   print(f"Expected: {expected_shape}, Got: {tensor.shape}")
   
2. Gradient Explosion/Vanishing:
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
   
3. NaN Loss:
   assert not torch.isnan(loss), f"NaN loss detected at step {step}"
   
4. Memory Error:
   torch.cuda.empty_cache()  # GPU memory cleanup
   
ğŸ§ª å˜ä½“ãƒ†ã‚¹ãƒˆã®ä¾‹:
def test_attention():
    batch_size, seq_len, d_model = 2, 10, 512
    x = torch.randn(batch_size, seq_len, d_model)
    
    attn = MultiHeadAttention(d_model, num_heads=8)
    out = attn(x)
    
    assert out.shape == x.shape, f"Shape mismatch: {out.shape} vs {x.shape}"
    assert not torch.isnan(out).any(), "NaN detected in output"
    assert not torch.isinf(out).any(), "Inf detected in output"
    
    print("âœ… MultiHeadAttention test passed!")

ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š:
def benchmark_model(model, input_shape, num_runs=100):
    import time
    
    dummy_input = torch.randn(*input_shape)
    model.eval()
    
    # Warmup
    for _ in range(10):
        with torch.no_grad():
            _ = model(dummy_input)
    
    # Benchmark
    start_time = time.time()
    for _ in range(num_runs):
        with torch.no_grad():
            _ = model(dummy_input)
    
    avg_time = (time.time() - start_time) / num_runs
    print(f"Average inference time: {avg_time*1000:.2f}ms")
"""

# ==========================================
# ğŸ¯ å®Ÿè£…å®Œäº†å¾Œã®ç™ºå±•èª²é¡Œ
# ==========================================

"""
1. ğŸ”¥ Advanced Techniques:
   - RoPE (Rotary Position Embedding)
   - Flash Attention (ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–)
   - LoRA (Low-Rank Adaptation)

2. ğŸ“ˆ æ€§èƒ½æ”¹å–„:
   - Mixed Precision Training (torch.cuda.amp)
   - Gradient Checkpointing (ãƒ¡ãƒ¢ãƒªç¯€ç´„)
   - DataParallel / DistributedDataParallel

3. ğŸ” åˆ†æãƒ»å¯è¦–åŒ–:
   - Attention Pattern ã®å¯è¦–åŒ–
   - Embedding Space ã®å¯è¦–åŒ– (t-SNE, UMAP)
   - Layer-wise Learning Rate Decay

4. ğŸš€ å®Ÿç”¨åŒ–:
   - ONNX Export (æ¨è«–æœ€é©åŒ–)
   - TensorRT Optimization
   - Quantization (8bit/16bit)
"""

# ğŸ¤” ãªãœ view + permute ãŒå¿…è¦ãªã®ã‹ï¼Ÿè©³ç´°è§£èª¬
"""
è³ªå•: ã€Œviewã¨permuteã§æ¬¡å…ƒã‚’å¤‰ãˆã¦ã„ã‚‹ã®ã¯ãªãœï¼Ÿã€

ğŸ’¡ ç­”ãˆ: **åŠ¹ç‡çš„ãªãƒãƒƒãƒä¸¦åˆ—å‡¦ç†ã®ãŸã‚**

ğŸ” å…·ä½“ä¾‹ã§ç†è§£ã—ã‚ˆã†:
d_model=512, num_heads=8, head_dim=64 ã¨ã™ã‚‹

1ï¸âƒ£ å…ƒã®å½¢çŠ¶:
   Q, K, V: (batch=2, seq_len=10, d_model=512)

2ï¸âƒ£ viewæ“ä½œã®ç†ç”±:
   512æ¬¡å…ƒã‚’8ã¤ã®ãƒ˜ãƒƒãƒ‰ã«åˆ†å‰²
   (2, 10, 512) â†’ (2, 10, 8, 64)
   â†‘ 512 = 8 Ã— 64 ã«åˆ†è§£

3ï¸âƒ£ permuteæ“ä½œã®ç†ç”±:
   (2, 10, 8, 64) â†’ (2, 8, 10, 64)
   â†‘ ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒã‚’å‰ã«æŒã£ã¦ãã‚‹

4ï¸âƒ£ ãªãœã“ã®é †åºï¼Ÿ
   è¡Œåˆ—ç©: (2, 8, 10, 64) @ (2, 8, 64, 10) â†’ (2, 8, 10, 10)
   â†‘ PyTorchã¯æœ€å¾Œã®2æ¬¡å…ƒã§è¡Œåˆ—ç©ã‚’è¨ˆç®—
   â†‘ 8ã¤ã®ãƒ˜ãƒƒãƒ‰ã‚’**ä¸¦åˆ—**ã§å‡¦ç†ã§ãã‚‹ï¼

âŒ ã‚‚ã—permuteã—ãªã‹ã£ãŸã‚‰:
   (2, 10, 8, 64) ã®å½¢çŠ¶ã§ã¯ã€ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒãŒæœ€å¾Œã‹ã‚‰2ç•ªç›®ã«ã‚ã‚Šã€
   è¡Œåˆ—ç©ãŒè¤‡é›‘ã«ãªã£ã¦ã—ã¾ã†

âœ… permuteã™ã‚‹ã“ã¨ã§:
   - 8ã¤ã®ãƒ˜ãƒƒãƒ‰ã‚’åŒæ™‚ã«å‡¦ç†
   - GPUã§ã®ä¸¦åˆ—è¨ˆç®—ãŒåŠ¹ç‡çš„
   - ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæœ€é©åŒ–
"""

def visualize_tensor_transformation():
    """
    ãƒ†ãƒ³ã‚½ãƒ«å¤‰å½¢ã®å¯è¦–åŒ–
    """
    print("ğŸ” Multi-Head Attention ã®ãƒ†ãƒ³ã‚½ãƒ«å¤‰å½¢ã‚’å¯è¦–åŒ–")
    print("=" * 60)
    
    # ä¾‹: å°ã•ãªã‚µã‚¤ã‚ºã§å®Ÿé¨“
    batch_size, seq_len, d_model = 2, 4, 8
    num_heads, head_dim = 2, 4  # d_model = num_heads * head_dim
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"å…¥åŠ› x: {x.shape}")
    
    # ç·šå½¢å¤‰æ›ï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚ã€æ’ç­‰å¤‰æ›ï¼‰
    query = x
    print(f"Q (ç·šå½¢å¤‰æ›å¾Œ): {query.shape}")
    
    # Step 1: viewæ“ä½œ
    query_reshaped = query.view(batch_size, seq_len, num_heads, head_dim)
    print(f"Q (viewå¾Œ): {query_reshaped.shape}")
    print("â†’ d_modelã‚’num_headså€‹ã®head_dimã«åˆ†å‰²")
    
    # Step 2: permuteæ“ä½œ  
    query_permuted = query_reshaped.permute(0, 2, 1, 3)
    print(f"Q (permuteå¾Œ): {query_permuted.shape}")
    print("â†’ ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒã‚’å‰ã«ç§»å‹•ï¼ˆä¸¦åˆ—å‡¦ç†ã®ãŸã‚ï¼‰")
    
    # å®Ÿéš›ã®è¨ˆç®—ä¾‹
    key_permuted = query_permuted  # ç°¡ç•¥åŒ–
    
    # è¡Œåˆ—ç©ã®å®Ÿè¡Œ
    scores = torch.matmul(query_permuted, key_permuted.transpose(-2, -1))
    print(f"scores: {scores.shape}")
    print("â†’ (batch, num_heads, seq_len, seq_len) ã®attention matrix")
    
    print("\nğŸ’¡ é‡è¦ãƒã‚¤ãƒ³ãƒˆ:")
    print("- permuteå¾Œã¯ (batch, num_heads, seq_len, head_dim)")
    print("- æœ€å¾Œã®2æ¬¡å…ƒ (seq_len, head_dim) ã§è¡Œåˆ—ç©")
    print("- num_headsåˆ†ã‚’ä¸¦åˆ—ã§è¨ˆç®—ï¼")

# å®Ÿè¡Œä¾‹ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„ï¼‰
# visualize_tensor_transformation()

# ğŸ“Š ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨è¨ˆç®—åŠ¹ç‡ã®æ¯”è¼ƒ
"""
ğŸš€ ãªãœåŠ¹ç‡çš„ãªã®ã‹ï¼Ÿ

1. **ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³**:
   - é€£ç¶šã—ãŸãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹
   - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡å‘ä¸Š

2. **ä¸¦åˆ—å‡¦ç†**:
   - 8ã¤ã®ãƒ˜ãƒƒãƒ‰ã‚’åŒæ™‚è¨ˆç®—
   - GPUã®è¤‡æ•°ã‚³ã‚¢ã‚’æ´»ç”¨

3. **è¡Œåˆ—ç©ã®æœ€é©åŒ–**:
   - BLAS (Basic Linear Algebra Subprograms) ã‚’æ´»ç”¨
   - ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–ã®æ©æµ

âŒ éåŠ¹ç‡ãªå®Ÿè£…ä¾‹:
for head_idx in range(num_heads):
    # å„ãƒ˜ãƒƒãƒ‰ã‚’é †æ¬¡å‡¦ç†ï¼ˆé…ã„ï¼ï¼‰
    q_head = query[:, :, head_idx*head_dim:(head_idx+1)*head_dim]
    # ... è¨ˆç®— ...

âœ… åŠ¹ç‡çš„ãªå®Ÿè£…ï¼ˆç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ï¼‰:
# å…¨ãƒ˜ãƒƒãƒ‰ã‚’ä¸¦åˆ—å‡¦ç†
scores = torch.matmul(query_all_heads, key_all_heads.transpose(-2, -1))
"""

# ğŸ¤” FeedForward ã®æ´»æ€§åŒ–é–¢æ•°ã«ã¤ã„ã¦è©³ç´°è§£èª¬
"""
è³ªå•: ã€ŒFeedForwardã¯æœ€å¾Œã¯æ´»æ€§åŒ–ã—ãªã„ã®ï¼Ÿã€

ğŸ’¡ ç­”ãˆ: **æœ€å¾Œã¯æ´»æ€§åŒ–ã—ã¾ã›ã‚“ï¼**

ğŸ“Š ç†ç”±ã®è©³ç´°:

1ï¸âƒ£ **Residual Connection ã®ãŸã‚**
   TransformerBlockã§ã¯: output = x + FFN(x)
   â†’ FFN(x)ãŒåˆ¶é™ã•ã‚Œã‚‹ã¨ã€Residualå­¦ç¿’ãŒé˜»å®³ã•ã‚Œã‚‹

2ï¸âƒ£ **è¡¨ç¾ã®æŸ”è»Ÿæ€§**
   - æ­£ã®å€¤ã ã‘ã§ãªãè² ã®å€¤ã‚‚é‡è¦ãªæƒ…å ±
   - ReLUã‚„GELUã¯è² ã®å€¤ã‚’åˆ¶é™ã—ã¦ã—ã¾ã†

3ï¸âƒ£ **Layer Normalization ãŒå¾Œå‡¦ç†**
   - FFN â†’ LayerNorm ã®é †åºã§æ­£è¦åŒ–ã•ã‚Œã‚‹
   - LayerNormãŒé©åˆ‡ã«å€¤ã‚’èª¿æ•´

4ï¸âƒ£ **æ¨™æº–çš„ãªTransformerè¨­è¨ˆ**
   - åŸè«–æ–‡ "Attention Is All You Need" ã§ã‚‚æœ€å¾Œã¯ç·šå½¢
   - BERT, GPTç­‰ã‚‚åŒæ§˜ã®è¨­è¨ˆ

ğŸ”¬ å®Ÿé¨“çš„ãªæ¯”è¼ƒ:
"""

def compare_ffn_activations():
    """
    æœ€å¾Œã®æ´»æ€§åŒ–ã‚ã‚Šãªã—ã®æ¯”è¼ƒå®Ÿé¨“
    """
    print("ğŸ§ª FFNæœ€çµ‚å±¤ã®æ´»æ€§åŒ–é–¢æ•°æ¯”è¼ƒå®Ÿé¨“")
    print("=" * 50)
    
    d_model, d_ff, batch_size, seq_len = 512, 2048, 2, 10
    x = torch.randn(batch_size, seq_len, d_model)
    
    # 1. æ¨™æº–çš„ãªFFNï¼ˆæœ€å¾Œã«æ´»æ€§åŒ–ãªã—ï¼‰
    class StandardFFN(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear1 = nn.Linear(d_model, d_ff)
            self.linear2 = nn.Linear(d_ff, d_model)
            self.gelu = nn.GELU()
            
        def forward(self, x):
            x = self.linear1(x)
            x = self.gelu(x)        # ä¸­é–“å±¤ã®ã¿æ´»æ€§åŒ–
            x = self.linear2(x)     # æœ€å¾Œã¯ç·šå½¢
            return x
    
    # 2. æœ€å¾Œã«ã‚‚æ´»æ€§åŒ–ã‚’å…¥ã‚ŒãŸFFNï¼ˆå®Ÿé¨“ç”¨ï¼‰
    class ActivatedFFN(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear1 = nn.Linear(d_model, d_ff)
            self.linear2 = nn.Linear(d_ff, d_model)
            self.gelu = nn.GELU()
            
        def forward(self, x):
            x = self.linear1(x)
            x = self.gelu(x)        # ä¸­é–“å±¤æ´»æ€§åŒ–
            x = self.linear2(x)
            x = self.gelu(x)        # æœ€å¾Œã«ã‚‚æ´»æ€§åŒ–ï¼ˆå®Ÿé¨“ï¼‰
            return x
    
    standard_ffn = StandardFFN()
    activated_ffn = ActivatedFFN()
    
    # å‡ºåŠ›ã®æ¯”è¼ƒ
    with torch.no_grad():
        standard_out = standard_ffn(x)
        activated_out = activated_ffn(x)
        
        print(f"å…¥åŠ›ã®ç¯„å›²: [{x.min():.3f}, {x.max():.3f}]")
        print(f"æ¨™æº–FFNå‡ºåŠ›: [{standard_out.min():.3f}, {standard_out.max():.3f}]")
        print(f"æ´»æ€§åŒ–FFNå‡ºåŠ›: [{activated_out.min():.3f}, {activated_out.max():.3f}]")
        
        # è² ã®å€¤ã®å‰²åˆ
        standard_neg_ratio = (standard_out < 0).float().mean()
        activated_neg_ratio = (activated_out < 0).float().mean()
        
        print(f"\nğŸ“Š è² ã®å€¤ã®å‰²åˆ:")
        print(f"æ¨™æº–FFN: {standard_neg_ratio:.1%}")
        print(f"æ´»æ€§åŒ–FFN: {activated_neg_ratio:.1%}")
        
        print(f"\nğŸ’¡ çµè«–:")
        print(f"- æ¨™æº–FFNã¯æ­£è² ä¸¡æ–¹ã®å€¤ã‚’å‡ºåŠ›ï¼ˆè¡¨ç¾åŠ›ãŒé«˜ã„ï¼‰")
        print(f"- æ´»æ€§åŒ–FFNã¯æ­£ã®å€¤ã®ã¿ï¼ˆè¡¨ç¾åŠ›ãŒåˆ¶é™ã•ã‚Œã‚‹ï¼‰")

# å®Ÿè¡Œä¾‹ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„ï¼‰
# compare_ffn_activations()

# ğŸ“š ä»–ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã®æ¯”è¼ƒ
"""
ğŸ” ä»–ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã®é•ã„:

1. **CNN (ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ)**
   - å„å±¤ã§ReLUæ´»æ€§åŒ–ãŒä¸€èˆ¬çš„
   - ç‰¹å¾´æŠ½å‡ºãŒç›®çš„

2. **RNN/LSTM**
   - éš ã‚ŒçŠ¶æ…‹ã§tanh/sigmoidã‚’ä½¿ç”¨
   - ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ã§åˆ¶å¾¡

3. **Transformer/BERT**
   - ä¸­é–“å±¤ã®ã¿GELUæ´»æ€§åŒ–
   - æœ€çµ‚å±¤ã¯ç·šå½¢ï¼ˆResidual + LayerNormï¼‰

4. **MLP (å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³)**
   - é€šå¸¸ã¯å„å±¤ã§æ´»æ€§åŒ–
   - åˆ†é¡å•é¡Œã§ã¯æœ€å¾Œã«Softmax

ğŸ¯ TransformerãŒç‰¹æ®Šãªç†ç”±:
- Residual Connection ã®å­˜åœ¨
- LayerNormalization ã®å¾Œå‡¦ç†
- Attentionæ©Ÿæ§‹ã¨ã®å”èª¿
"""
