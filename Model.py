import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.utils.data import DataLoader

# I tried to implement a BERT model from scratch using PyTorch.

# 1. ã¾ãšåŸºæœ¬çš„ãªãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰
class ShowMultiHeadAttention(nn.Module):
    """
    ãƒ’ãƒ³ãƒˆ: 
    - Query, Key, Value ã®3ã¤ã®ç·šå½¢å¤‰æ›ãŒå¿…è¦
    - ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆç©ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: softmax(QK^T / sqrt(d_k))V
    - è¤‡æ•°ã®ãƒ˜ãƒƒãƒ‰ã‚’ä¸¦åˆ—å®Ÿè¡Œå¾Œã€concatenate
    - æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›ã§å‡ºåŠ›æ¬¡å…ƒã‚’èª¿æ•´
    """
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__() # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads  # å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒ
        self.dropout = torch.nn.Dropout(dropout)
        
        # Q, K, V ã®ç·šå½¢å¤‰æ›ï¼ˆä¿®æ­£ï¼štorch.nn.linear â†’ torch.nn.Linearï¼‰
        self.query = torch.nn.Linear(d_model, d_model)
        self.key = torch.nn.Linear(d_model, d_model)
        self.value = torch.nn.Linear(d_model, d_model)
        
        # æœ€çµ‚çš„ãªå‡ºåŠ›å¤‰æ›
        self.out_proj = torch.nn.Linear(d_model, d_model)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: Q, K, V ã‚’ç·šå½¢å¤‰æ›ã§ç”Ÿæˆ
        query = self.query(x)  # (batch, seq_len, d_model)
        key = self.key(x)      # (batch, seq_len, d_model)
        value = self.value(x)  # (batch, seq_len, d_model)
        
        print(f"Before View: Q={query.shape}, K={key.shape}, V={value.shape}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: Multi-Headç”¨ã«æ¬¡å…ƒã‚’å¤‰å½¢
        # ç†ç”±1: d_model ã‚’ num_heads å€‹ã®ãƒ˜ãƒƒãƒ‰ã«åˆ†å‰²
        # (batch, seq_len, d_model) â†’ (batch, seq_len, num_heads, head_dim)
        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim)
        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim)  # ä¿®æ­£: query.shape â†’ batch_size
        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        print(f"After View: Q={query.shape}, K={key.shape}, V={value.shape}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: æ¬¡å…ƒã®é †åºã‚’å¤‰æ›´ï¼ˆã“ã‚ŒãŒè³ªå•ã®ãƒã‚¤ãƒ³ãƒˆï¼ï¼‰
        # ç†ç”±2: ãƒãƒƒãƒä¸¦åˆ—å‡¦ç†ã®ãŸã‚ (batch, num_heads, seq_len, head_dim)
        query = query.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, head_dim)
        key = key.permute(0, 2, 1, 3)
        value = value.permute(0, 2, 1, 3)
        
        print(f"After Permute: Q={query.shape}, K={key.shape}, V={value.shape}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: Scaled Dot-Product Attention
        # scores = Q @ K^T / sqrt(d_k)
        # (batch, num_heads, seq_len, head_dim) @ (batch, num_heads, head_dim, seq_len)
        # â†’ (batch, num_heads, seq_len, seq_len)
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)
        print(f"scores: {scores.shape}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: ãƒã‚¹ã‚¯å‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # ã‚¹ãƒ†ãƒƒãƒ—6: Softmax + Dropout
        weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)
        weights = self.dropout(weights)
        
        # ã‚¹ãƒ†ãƒƒãƒ—7: Value ã¨ã®ç©
        # (batch, num_heads, seq_len, seq_len) @ (batch, num_heads, seq_len, head_dim)
        # â†’ (batch, num_heads, seq_len, head_dim)
        context = torch.matmul(weights, value)
        print(f"context: {context.shape}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—8: ãƒ˜ãƒƒãƒ‰ã‚’çµåˆã—ã¦å…ƒã®å½¢çŠ¶ã«æˆ»ã™
        # (batch, num_heads, seq_len, head_dim) â†’ (batch, seq_len, num_heads, head_dim)
        context = context.permute(0, 2, 1, 3)
        # â†’ (batch, seq_len, d_model)
        context = context.contiguous().view(batch_size, seq_len, self.num_heads * self.head_dim)
        print(f"Result: {context.shape}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—9: æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›
        return self.out_proj(context)  # ä¿®æ­£: output_linear â†’ out_proj
class MultiHeadAttention(nn.Module):
    """
    ãƒ’ãƒ³ãƒˆ: 
    - Query, Key, Value ã®3ã¤ã®ç·šå½¢å¤‰æ›ãŒå¿…è¦
    - ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆç©ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: softmax(QK^T / sqrt(d_k))V
    - è¤‡æ•°ã®ãƒ˜ãƒƒãƒ‰ã‚’ä¸¦åˆ—å®Ÿè¡Œå¾Œã€concatenate
    - æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›ã§å‡ºåŠ›æ¬¡å…ƒã‚’èª¿æ•´
    """
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__() # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads  # å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒ
        self.dropout = torch.nn.Dropout(dropout)
        
        # Q, K, V ã®ç·šå½¢å¤‰æ›ï¼ˆä¿®æ­£ï¼štorch.nn.linear â†’ torch.nn.Linearï¼‰
        self.query = torch.nn.Linear(d_model, d_model)
        self.key = torch.nn.Linear(d_model, d_model)
        self.value = torch.nn.Linear(d_model, d_model)
        
        # æœ€çµ‚çš„ãªå‡ºåŠ›å¤‰æ›
        self.out_proj = torch.nn.Linear(d_model, d_model)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: Q, K, V ã‚’ç·šå½¢å¤‰æ›ã§ç”Ÿæˆ
        query = self.query(x)  # (batch, seq_len, d_model)
        key = self.key(x)      # (batch, seq_len, d_model)
        value = self.value(x)  # (batch, seq_len, d_model)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: Multi-Headç”¨ã«æ¬¡å…ƒã‚’å¤‰å½¢
        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim)
        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim)  # ä¿®æ­£: query.shape â†’ batch_size
        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: æ¬¡å…ƒã®é †åºã‚’å¤‰æ›´ï¼ˆã“ã‚ŒãŒè³ªå•ã®ãƒã‚¤ãƒ³ãƒˆï¼ï¼‰
        query = query.permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, head_dim)
        key = key.permute(0, 2, 1, 3)
        value = value.permute(0, 2, 1, 3)
                
        # ã‚¹ãƒ†ãƒƒãƒ—4: Scaled Dot-Product Attention
        # scores = Q @ K^T / sqrt(d_k)
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: ãƒã‚¹ã‚¯å‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # ã‚¹ãƒ†ãƒƒãƒ—6: Softmax + Dropout
        weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)
        weights = self.dropout(weights)
        # ã‚¹ãƒ†ãƒƒãƒ—7: Value ã¨ã®ç©
        context = torch.matmul(weights, value)    
        # ã‚¹ãƒ†ãƒƒãƒ—8: ãƒ˜ãƒƒãƒ‰ã‚’çµåˆã—ã¦å…ƒã®å½¢çŠ¶ã«æˆ»ã™
        context = context.permute(0, 2, 1, 3)
        # â†’ (batch, seq_len, d_model)
        context = context.contiguous().view(batch_size, seq_len, self.num_heads * self.head_dim)
    
        # ã‚¹ãƒ†ãƒƒãƒ—9: æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›
        return self.out_proj(context)  # ä¿®æ­£: output_linear â†’ out_proj
    
class PositionwiseFeedForward(nn.Module):
    """
    ãƒ’ãƒ³ãƒˆ:
    - 2å±¤ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    - ä¸­é–“å±¤ã§ã¯æ¬¡å…ƒã‚’æ‹¡å¼µï¼ˆé€šå¸¸4å€ï¼‰
    - GELUæ´»æ€§åŒ–é–¢æ•°ã‚’ä½¿ç”¨
    - ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚‚å¿˜ã‚Œãšã«
    """
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)  # å…¥åŠ›æ¬¡å…ƒ â†’ ä¸­é–“æ¬¡å…ƒ
        self.linear2 = nn.Linear(d_ff, d_model)  # ä¸­é–“æ¬¡å…ƒ
        self.dropout = nn.Dropout(dropout)
        self.gelu = nn.GELU()

    def forward(self, x):
        x = self.linear1(x)          
        x = self.gelu(x)             
        x = self.dropout(x)          
        x = self.linear2(x)          
        x = self.dropout(x)            
        return x  
      

class TransformerBlock(nn.Module):
    """
    ãƒ’ãƒ³ãƒˆ:
    - Multi-Head Attention + Residual Connection + Layer Norm
    - Feed Forward + Residual Connection + Layer Norm
    - é †åºã«æ³¨æ„: Pre-LN vs Post-LN
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model,num_heads)
        self.layer_norm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)
        self.layer_norm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)
        self.dropout = nn.Dropout(dropout)
        self.ffn = PositionwiseFeedForward(d_model,d_ff)
    def forward(self, x, mask=None):
        #Attention block
        #TODO implement transformer block
        residual = x
        #print("Took Residual...",x.shape)
        x = self.layer_norm1(x)
        #print("calculating layer norm...",x.shape)
        x = self.dropout(self.attention(x,mask))
        #print("calculating Attention...",x.shape)
        x = x + residual
        #print("calculating Residual Connection...",x.shape)
        #ffnn
        residual = x
        x = self.layer_norm2(x)
        #print("calculating layer norm...",x.shape)
        x = self.dropout(self.ffn(x))
        #print("calculating ffn...",x.shape)
        x = x + residual
        return x


class BertEmbeddings(nn.Module):
    """
    ãƒ’ãƒ³ãƒˆ:
    - Token Embeddings (èªå½™ã‚µã‚¤ã‚º Ã— d_model)
    - Position Embeddings (æœ€å¤§ç³»åˆ—é•· Ã— d_model) 
    - Segment Embeddings (2 Ã— d_model, NSPã‚¿ã‚¹ã‚¯ç”¨)
    - 3ã¤ã‚’è¶³ã—åˆã‚ã›ã¦LayerNormã¨Dropout
    """
    def __init__(self, vocab_size, d_model, max_seq_len=512, dropout=0.1):
        super().__init__()
        # TODO: 3ç¨®é¡ã®åŸ‹ã‚è¾¼ã¿ã‚’å®Ÿè£…
        self.d_model = d_model
        self.token = torch.nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.position = torch.nn.Embedding(max_seq_len, d_model)
        self.segment = torch.nn.Embedding(2, d_model)  # 2ã¤ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆ0ã¨1ï¼‰
        self.layer_norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        #Embedding: Lookup table that keep meaning vector of words
    def forward(self, input_ids, token_type_ids=None):
        # TODO: åŸ‹ã‚è¾¼ã¿ã®è¨ˆç®—ã‚’å®Ÿè£…
        batch_size, seq_len = input_ids.shape
        # Step 1: Token Embeddings
        token_embeddings = self.token(input_ids)
        # Step 2: Position Embeddings
        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)
        position_ids = position_ids.expand(batch_size, -1)  # ğŸ”§ ãƒãƒƒãƒæ¬¡å…ƒã‚’æ‹¡å¼µ
        position_embeddings = self.position(position_ids)
        # Step 3: Segment Embeddings
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)  # å…¨ã¦0ï¼ˆå˜ä¸€æ–‡ï¼‰
        segment_embeddings = self.segment(token_type_ids)  # (batch, seq_len, d_model)
        embeddings = token_embeddings + position_embeddings + segment_embeddings
        embeddings = self.dropout(self.layer_norm(embeddings))

        return embeddings
        
class Bert(nn.Module):
    """
    BERTå®Ÿè£…ã®æœ€çµ‚å½¢
    
    å­¦ç¿’ã®ãƒ’ãƒ³ãƒˆ:
    1. è«–æ–‡ã‚’èª­ã‚“ã§å…¨ä½“åƒã‚’ç†è§£
    2. å°ã•ãªéƒ¨å“ã‹ã‚‰å®Ÿè£…ï¼ˆAttention â†’ FFN â†’ Block â†’ Full Modelï¼‰
    3. å„å±¤ã§ print(tensor.shape) ã—ã¦ã‚µã‚¤ã‚ºã‚’ç¢ºèª
    4. ç°¡å˜ãªãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
    5. äº‹å‰å­¦ç¿’ã¯è¨ˆç®—é‡ãŒå¤§ãã„ã®ã§ã€å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é–‹å§‹
    
    é‡è¦ãªæ¦‚å¿µ:
    - Bidirectional: å·¦å³ä¸¡æ–¹å‘ã®æ–‡è„ˆã‚’è¦‹ã‚‹
    - Masked Language Model: ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¹ã‚¯ã—ãŸå˜èªã‚’äºˆæ¸¬
    - Next Sentence Prediction: 2ã¤ã®æ–‡ãŒé€£ç¶šã™ã‚‹ã‹ã‚’äºˆæ¸¬
    - Attention Weights: ã©ã®å˜èªã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã®å¯è¦–åŒ–
    """
    
    def __init__(self, vocab_size, d_model=768, num_layers=12, num_heads=12, d_ff=3072, max_seq_len=512, dropout=0.1):
        super().__init__()
        # TODO: BERTã®å…¨ä½“æ§‹é€ ã‚’å®Ÿè£…
        pass
    
    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        # TODO: BERTå…¨ä½“ã®forward passã‚’å®Ÿè£…
        pass

# main

def exp_Transformer():
    d_model = 512
    seq = 10
    batch_size = 2
    num_heads=2
    d_ff=10

    x = torch.randn(batch_size, seq, d_model)  # (batch, seq, d_model)
    print("start")
    func = TransformerBlock(d_model, num_heads, d_ff)
    #func = ShowMultiHeadAttention(d_model, num_heads)
    out = func(x)
    assert out.shape == x.shape
    return 0

def test_bert_embeddings():
    """
    BertEmbeddingsã®è»½é‡ãƒ†ã‚¹ãƒˆ
    """
    print("ğŸ§ª BertEmbeddings ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    vocab_size = 1000
    d_model = 128  # è»½é‡åŒ–ï¼ˆé€šå¸¸ã¯768ï¼‰
    max_seq_len = 64
    batch_size = 2
    seq_len = 8
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    embeddings = BertEmbeddings(
        vocab_size=vocab_size,
        d_model=d_model,
        max_seq_len=max_seq_len,
        dropout=0.1
    )
    
    print(f"ğŸ“‹ è¨­å®š:")
    print(f"  - vocab_size: {vocab_size}")
    print(f"  - d_model: {d_model}")
    print(f"  - max_seq_len: {max_seq_len}")
    print(f"  - batch_size: {batch_size}, seq_len: {seq_len}")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    token_type_ids = torch.zeros(batch_size, seq_len, dtype=torch.long)
    token_type_ids[:, seq_len//2:] = 1  # å¾ŒåŠã‚’æ–‡Bï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆ1ï¼‰ã«
    
    print(f"\nğŸ“¥ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿:")
    print(f"  input_ids: {input_ids.shape}")
    print(f"  token_type_ids: {token_type_ids.shape}")
    print(f"  input_ids[0]: {input_ids[0].tolist()}")
    print(f"  token_type_ids[0]: {token_type_ids[0].tolist()}")
    
    # Forward pass
    with torch.no_grad():
        output = embeddings(input_ids, token_type_ids)
    
    print(f"\nğŸ“¤ å‡ºåŠ›:")
    print(f"  output shape: {output.shape}")
    print(f"  output range: [{output.min():.3f}, {output.max():.3f}]")
    print(f"  output mean: {output.mean():.3f}")
    print(f"  output std: {output.std():.3f}")
    
    # å„åŸ‹ã‚è¾¼ã¿ã®å€‹åˆ¥ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ”¬ å€‹åˆ¥åŸ‹ã‚è¾¼ã¿ãƒ†ã‚¹ãƒˆ:")
    
    # Token Embeddings
    token_emb = embeddings.token(input_ids)
    print(f"  Token embeddings: {token_emb.shape}")
    
    # Position Embeddings
    position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)
    pos_emb = embeddings.position(position_ids)
    print(f"  Position embeddings: {pos_emb.shape}")
    
    # Segment Embeddings
    seg_emb = embeddings.segment(token_type_ids)
    print(f"  Segment embeddings: {seg_emb.shape}")
    
    # æ‰‹å‹•è¨ˆç®—ã¨ã®æ¯”è¼ƒ
    manual_sum = token_emb + pos_emb + seg_emb
    manual_output = embeddings.layer_norm(manual_sum)
    
    print(f"\nâœ… æ¤œè¨¼:")
    print(f"  æ‰‹å‹•è¨ˆç®—ã¨ã®å·®: {torch.abs(output - manual_output).max():.6f}")
    
    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ã®åŠ¹æœç¢ºèª
    print(f"\nğŸ¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ã®åŠ¹æœ:")
    seg_0_emb = embeddings.segment.weight[0]  # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ0ã®åŸ‹ã‚è¾¼ã¿
    seg_1_emb = embeddings.segment.weight[1]  # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ1ã®åŸ‹ã‚è¾¼ã¿
    similarity = F.cosine_similarity(seg_0_emb.unsqueeze(0), seg_1_emb.unsqueeze(0))
    print(f"  ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ0 vs 1 é¡ä¼¼åº¦: {similarity.item():.3f}")
    print(f"  (å­¦ç¿’å‰ãªã®ã§ãƒ©ãƒ³ãƒ€ãƒ å€¤)")
    
    print(f"\nğŸ‰ BertEmbeddings ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    
    return output

def test_edge_cases():
    """
    ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆ
    """
    print("\nğŸš¨ ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ")
    print("=" * 40)
    
    vocab_size = 100
    d_model = 64
    embeddings = BertEmbeddings(vocab_size, d_model, max_seq_len=32, dropout=0.0)
    
    # ãƒ†ã‚¹ãƒˆ1: æœ€å°ã‚µã‚¤ã‚º
    print("1ï¸âƒ£ æœ€å°ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆ (1x1)")
    input_ids = torch.tensor([[5]])
    output = embeddings(input_ids)
    print(f"   å‡ºåŠ›å½¢çŠ¶: {output.shape}")
    assert output.shape == (1, 1, d_model), "å½¢çŠ¶ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"
    
    # ãƒ†ã‚¹ãƒˆ2: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³
    print("2ï¸âƒ£ ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ã‚¹ãƒˆ")
    input_ids = torch.tensor([[0, 1, 2, 0, 0]])  # 0ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    output = embeddings(input_ids)
    print(f"   ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ä½ç½®ã®å‡ºåŠ›: {output[0, 0].sum():.3f}")
    
    # ãƒ†ã‚¹ãƒˆ3: token_type_idsæœªæŒ‡å®š
    print("3ï¸âƒ£ token_type_idsæœªæŒ‡å®šãƒ†ã‚¹ãƒˆ")
    input_ids = torch.tensor([[1, 2, 3, 4]])
    output1 = embeddings(input_ids)  # token_type_ids=None
    output2 = embeddings(input_ids, torch.zeros_like(input_ids))
    diff = torch.abs(output1 - output2).max()
    print(f"   å·®åˆ†: {diff:.6f}")
    assert diff < 1e-6, "token_type_ids=Noneã®å‡¦ç†ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"
    
    print("âœ… å…¨ã¦ã®ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆé€šéï¼")

def main():
    test_bert_embeddings()
    test_edge_cases()
    return 0

main()



def visualize_tensor_transformation():
    """
    ãƒ†ãƒ³ã‚½ãƒ«å¤‰å½¢ã®å¯è¦–åŒ–
    """
    print("ğŸ” Multi-Head Attention ã®ãƒ†ãƒ³ã‚½ãƒ«å¤‰å½¢ã‚’å¯è¦–åŒ–")
    print("=" * 60)
    
    # ä¾‹: å°ã•ãªã‚µã‚¤ã‚ºã§å®Ÿé¨“
    batch_size, seq_len, d_model = 2, 4, 8
    num_heads, head_dim = 2, 4  # d_model = num_heads * head_dim
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"å…¥åŠ› x: {x.shape}")
    
    # ç·šå½¢å¤‰æ›ï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚ã€æ’ç­‰å¤‰æ›ï¼‰
    query = x
    print(f"Q (ç·šå½¢å¤‰æ›å¾Œ): {query.shape}")
    
    # Step 1: viewæ“ä½œ
    query_reshaped = query.view(batch_size, seq_len, num_heads, head_dim)
    print(f"Q (viewå¾Œ): {query_reshaped.shape}")
    print("â†’ d_modelã‚’num_headså€‹ã®head_dimã«åˆ†å‰²")
    
    # Step 2: permuteæ“ä½œ  
    query_permuted = query_reshaped.permute(0, 2, 1, 3)
    print(f"Q (permuteå¾Œ): {query_permuted.shape}")
    print("â†’ ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒã‚’å‰ã«ç§»å‹•ï¼ˆä¸¦åˆ—å‡¦ç†ã®ãŸã‚ï¼‰")
    
    # å®Ÿéš›ã®è¨ˆç®—ä¾‹
    key_permuted = query_permuted  # ç°¡ç•¥åŒ–
    
    # è¡Œåˆ—ç©ã®å®Ÿè¡Œ
    scores = torch.matmul(query_permuted, key_permuted.transpose(-2, -1))
    print(f"scores: {scores.shape}")
    print("â†’ (batch, num_heads, seq_len, seq_len) ã®attention matrix")
    
    print("\nğŸ’¡ é‡è¦ãƒã‚¤ãƒ³ãƒˆ:")
    print("- permuteå¾Œã¯ (batch, num_heads, seq_len, head_dim)")
    print("- æœ€å¾Œã®2æ¬¡å…ƒ (seq_len, head_dim) ã§è¡Œåˆ—ç©")
    print("- num_headsåˆ†ã‚’ä¸¦åˆ—ã§è¨ˆç®—ï¼")

# å®Ÿè¡Œä¾‹ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„ï¼‰
# visualize_tensor_transformation()

def compare_ffn_activations():
    """
    æœ€å¾Œã®æ´»æ€§åŒ–ã‚ã‚Šãªã—ã®æ¯”è¼ƒå®Ÿé¨“
    """
    print("ğŸ§ª FFNæœ€çµ‚å±¤ã®æ´»æ€§åŒ–é–¢æ•°æ¯”è¼ƒå®Ÿé¨“")
    print("=" * 50)
    
    d_model, d_ff, batch_size, seq_len = 512, 2048, 2, 10
    x = torch.randn(batch_size, seq_len, d_model)
    
    # 1. æ¨™æº–çš„ãªFFNï¼ˆæœ€å¾Œã«æ´»æ€§åŒ–ãªã—ï¼‰
    class StandardFFN(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear1 = nn.Linear(d_model, d_ff)
            self.linear2 = nn.Linear(d_ff, d_model)
            self.gelu = nn.GELU()
            
        def forward(self, x):
            x = self.linear1(x)
            x = self.gelu(x)        # ä¸­é–“å±¤ã®ã¿æ´»æ€§åŒ–
            x = self.linear2(x)     # æœ€å¾Œã¯ç·šå½¢
            return x
    
    # 2. æœ€å¾Œã«ã‚‚æ´»æ€§åŒ–ã‚’å…¥ã‚ŒãŸFFNï¼ˆå®Ÿé¨“ç”¨ï¼‰
    class ActivatedFFN(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear1 = nn.Linear(d_model, d_ff)
            self.linear2 = nn.Linear(d_ff, d_model)
            self.gelu = nn.GELU()
            
        def forward(self, x):
            x = self.linear1(x)
            x = self.gelu(x)        # ä¸­é–“å±¤æ´»æ€§åŒ–
            x = self.linear2(x)
            x = self.gelu(x)        # æœ€å¾Œã«ã‚‚æ´»æ€§åŒ–ï¼ˆå®Ÿé¨“ï¼‰
            return x
    
    standard_ffn = StandardFFN()
    activated_ffn = ActivatedFFN()
    
    # å‡ºåŠ›ã®æ¯”è¼ƒ
    with torch.no_grad():
        standard_out = standard_ffn(x)
        activated_out = activated_ffn(x)
        
        print(f"å…¥åŠ›ã®ç¯„å›²: [{x.min():.3f}, {x.max():.3f}]")
        print(f"æ¨™æº–FFNå‡ºåŠ›: [{standard_out.min():.3f}, {standard_out.max():.3f}]")
        print(f"æ´»æ€§åŒ–FFNå‡ºåŠ›: [{activated_out.min():.3f}, {activated_out.max():.3f}]")
        
        # è² ã®å€¤ã®å‰²åˆ
        standard_neg_ratio = (standard_out < 0).float().mean()
        activated_neg_ratio = (activated_out < 0).float().mean()
        
        print(f"\nğŸ“Š è² ã®å€¤ã®å‰²åˆ:")
        print(f"æ¨™æº–FFN: {standard_neg_ratio:.1%}")
        print(f"æ´»æ€§åŒ–FFN: {activated_neg_ratio:.1%}")
        
        print(f"\nğŸ’¡ çµè«–:")
        print(f"- æ¨™æº–FFNã¯æ­£è² ä¸¡æ–¹ã®å€¤ã‚’å‡ºåŠ›ï¼ˆè¡¨ç¾åŠ›ãŒé«˜ã„ï¼‰")
        print(f"- æ´»æ€§åŒ–FFNã¯æ­£ã®å€¤ã®ã¿ï¼ˆè¡¨ç¾åŠ›ãŒåˆ¶é™ã•ã‚Œã‚‹ï¼‰")

# å®Ÿè¡Œä¾‹ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„ï¼‰
# compare_ffn_activations()

# ğŸ“š ä»–ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã®æ¯”è¼ƒ
"""
ğŸ” ä»–ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã®é•ã„:

1. **CNN (ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ)**
   - å„å±¤ã§ReLUæ´»æ€§åŒ–ãŒä¸€èˆ¬çš„
   - ç‰¹å¾´æŠ½å‡ºãŒç›®çš„

2. **RNN/LSTM**
   - éš ã‚ŒçŠ¶æ…‹ã§tanh/sigmoidã‚’ä½¿ç”¨
   - ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ã§åˆ¶å¾¡

3. **Transformer/BERT**
   - ä¸­é–“å±¤ã®ã¿GELUæ´»æ€§åŒ–
   - æœ€çµ‚å±¤ã¯ç·šå½¢ï¼ˆResidual + LayerNormï¼‰

4. **MLP (å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³)**
   - é€šå¸¸ã¯å„å±¤ã§æ´»æ€§åŒ–
   - åˆ†é¡å•é¡Œã§ã¯æœ€å¾Œã«Softmax

ğŸ¯ TransformerãŒç‰¹æ®Šãªç†ç”±:
- Residual Connection ã®å­˜åœ¨
- LayerNormalization ã®å¾Œå‡¦ç†
- Attentionæ©Ÿæ§‹ã¨ã®å”èª¿
"""
